{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon - Custom Language Model\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "For this project, I will build a simple custom language model that is able to learn from any text data provided, and return a transcript with confidence values from input posed in speech utterances. I will use Google's cloud-based services to preprocess the input audio data and transcribe into an initial guess. Then I will train a model to improve on Google cloud speech API's response.\n",
    "\n",
    "\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "In order to use Google's cloud-based services, you first need to create an account on the [Google Cloud Platform](https://cloud.google.com//).\n",
    "\n",
    "Then, for each service you want to use, you have to enable use of that service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-speech\n",
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:318: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#snimissingwarning.\n",
      "  SNIMissingWarning\n",
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarning.\n",
      "  InsecurePlatformWarning\n",
      "  Downloading google_cloud_speech-0.29.0-py2.py3-none-any.whl (42kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 3.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-gax<0.16dev,>=0.15.14 (from google-cloud-speech)\n",
      "  Downloading google-gax-0.15.15.tar.gz (109kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 4.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-core<0.28dev,>=0.27.0 (from google-cloud-speech)\n",
      "  Downloading google_cloud_core-0.27.1-py2.py3-none-any.whl (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 7.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting googleapis-common-protos[grpc]<2.0dev,>=1.5.2 (from google-cloud-speech)\n",
      "  Downloading googleapis-common-protos-1.5.2.tar.gz\n",
      "Collecting dill<0.3dev,>=0.2.5 (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "  Downloading dill-0.2.7.1.tar.gz (64kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 6.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting future<0.17dev,>=0.16.0 (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "  Downloading future-0.16.0.tar.gz (824kB)\n",
      "\u001b[K    100% |████████████████████████████████| 829kB 1.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio<1.6dev,>=1.0.2 (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "  Downloading grpcio-1.4.0-cp27-cp27mu-manylinux1_x86_64.whl (5.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.4MB 267kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2.0dev,>=1.0.0 (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "  Downloading google_auth-1.1.1-py2.py3-none-any.whl (60kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 10.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting ply==3.8 (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "  Downloading ply-3.8.tar.gz (157kB)\n",
      "\u001b[K    100% |████████████████████████████████| 163kB 8.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf<4.0dev,>=3.0.0 (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "  Downloading protobuf-3.4.0-cp27-cp27mu-manylinux1_x86_64.whl (6.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 6.2MB 215kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests<3.0dev,>=2.13.0 (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "  Downloading requests-2.18.4-py2.py3-none-any.whl (88kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 10.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting six>=1.10.0 (from google-cloud-core<0.28dev,>=0.27.0->google-cloud-speech)\n",
      "  Downloading six-1.11.0-py2.py3-none-any.whl\n",
      "Requirement already up-to-date: futures>=3.0.0; python_version < \"3.2\" in /usr/local/lib/python2.7/dist-packages (from google-cloud-core<0.28dev,>=0.27.0->google-cloud-speech)\n",
      "Collecting setuptools>=34.0.0 (from google-cloud-core<0.28dev,>=0.27.0->google-cloud-speech)\n",
      "  Downloading setuptools-36.5.0-py2.py3-none-any.whl (478kB)\n",
      "\u001b[K    100% |████████████████████████████████| 481kB 2.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: enum34>=1.0.4 in /usr/local/lib/python2.7/dist-packages (from grpcio<1.6dev,>=1.0.2->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Collecting cachetools>=2.0.0 (from google-auth<2.0dev,>=1.0.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "  Downloading cachetools-2.0.1-py2.py3-none-any.whl\n",
      "Collecting rsa>=3.1.4 (from google-auth<2.0dev,>=1.0.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "  Downloading rsa-3.4.2-py2.py3-none-any.whl (46kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 11.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.0.5 (from google-auth<2.0dev,>=1.0.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "  Downloading pyasn1_modules-0.1.4-py2.py3-none-any.whl (60kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 10.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1>=0.1.7 (from google-auth<2.0dev,>=1.0.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "  Downloading pyasn1-0.3.6-py2.py3-none-any.whl (63kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 11.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<1.23,>=1.21.1 (from requests<3.0dev,>=2.13.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "  Downloading urllib3-1.22-py2.py3-none-any.whl (132kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 9.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<2.7,>=2.5 (from requests<3.0dev,>=2.13.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "  Downloading idna-2.6-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 11.3MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests<3.0dev,>=2.13.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0dev,>=2.13.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "  Downloading certifi-2017.7.27.1-py2.py3-none-any.whl (349kB)\n",
      "\u001b[K    100% |████████████████████████████████| 358kB 3.1MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: google-gax, googleapis-common-protos, dill, future, ply\n",
      "  Running setup.py bdist_wheel for google-gax ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/paperspace/.cache/pip/wheels/47/2c/d6/1c98d54675550a39421ae92ffd0e3e72f1eaee7725bf738131\n",
      "  Running setup.py bdist_wheel for googleapis-common-protos ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/paperspace/.cache/pip/wheels/d8/68/48/4b94c07bfc9c37ec6de8a7d74d4cbf6f9e5f375dbe6b0116a4\n",
      "  Running setup.py bdist_wheel for dill ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/paperspace/.cache/pip/wheels/e5/88/fe/7e290ce5bb39d531eb9bee5cf254ba1c3e3c7ba3339ce67bee\n",
      "  Running setup.py bdist_wheel for future ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/paperspace/.cache/pip/wheels/c2/50/7c/0d83b4baac4f63ff7a765bd16390d2ab43c93587fac9d6017a\n",
      "  Running setup.py bdist_wheel for ply ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/paperspace/.cache/pip/wheels/6f/23/3c/7055bf8004cee3c7d06c46318014e0374539094c4e36246e9a\n",
      "Successfully built google-gax googleapis-common-protos dill future ply\n",
      "Installing collected packages: dill, future, six, setuptools, protobuf, grpcio, googleapis-common-protos, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, ply, urllib3, idna, certifi, requests, google-gax, google-cloud-core, google-cloud-speech\n",
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pip/commands/install.py\", line 342, in run\n",
      "    prefix=options.prefix_path,\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pip/req/req_set.py\", line 784, in install\n",
      "    **kwargs\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pip/req/req_install.py\", line 851, in install\n",
      "    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pip/req/req_install.py\", line 1064, in move_wheel_files\n",
      "    isolated=self.isolated,\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pip/wheel.py\", line 345, in move_wheel_files\n",
      "    clobber(source, lib_dir, True)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pip/wheel.py\", line 316, in clobber\n",
      "    ensure_dir(destdir)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pip/utils/__init__.py\", line 83, in ensure_dir\n",
      "    os.makedirs(path)\n",
      "  File \"/usr/lib/python2.7/os.py\", line 157, in makedirs\n",
      "    mkdir(name, mode)\n",
      "OSError: [Errno 13] Permission denied: '/usr/local/lib/python2.7/dist-packages/dill'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarning.\r\n",
      "  InsecurePlatformWarning\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-cloud-speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Google Cloud SDK: https://cloud.google.com/sdk/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Google Cloud SDK!\n",
      "WARNING: You appear to be running this script as root. This may cause \n",
      "the installation to be inaccessible to users other than the root user.\n",
      "\n",
      "To help improve the quality of this product, we collect anonymized usage data\n",
      "and anonymized stacktraces when crashes are encountered; additional information\n",
      "is available at <https://cloud.google.com/sdk/usage-statistics>. You may choose\n",
      "to opt out of this collection now (by choosing 'N' at the below prompt), or at\n",
      "any time in the future by running the following command:\n",
      "\n",
      "    gcloud config set disable_usage_reporting true\n",
      "\n",
      "\n",
      "Your current Cloud SDK version is: 170.0.1\n",
      "The latest available version is: 172.0.1\n",
      "\n",
      "+----------------------------------------------------------------------------------------------------------------+\n",
      "|                                                   Components                                                   |\n",
      "+------------------+------------------------------------------------------+--------------------------+-----------+\n",
      "|      Status      |                         Name                         |            ID            |    Size   |\n",
      "+------------------+------------------------------------------------------+--------------------------+-----------+\n",
      "| Update Available | BigQuery Command Line Tool                           | bq                       |   < 1 MiB |\n",
      "| Update Available | Cloud SDK Core Libraries                             | core                     |   6.7 MiB |\n",
      "| Update Available | Cloud Storage Command Line Tool                      | gsutil                   |   3.0 MiB |\n",
      "| Not Installed    | App Engine Go Extensions                             | app-engine-go            |  98.0 MiB |\n",
      "| Not Installed    | Cloud Bigtable Command Line Tool                     | cbt                      |   4.1 MiB |\n",
      "| Not Installed    | Cloud Bigtable Emulator                              | bigtable                 |   3.5 MiB |\n",
      "| Not Installed    | Cloud Datalab Command Line Tool                      | datalab                  |   < 1 MiB |\n",
      "| Not Installed    | Cloud Datastore Emulator                             | cloud-datastore-emulator |  15.4 MiB |\n",
      "| Not Installed    | Cloud Datastore Emulator (Legacy)                    | gcd-emulator             |  38.1 MiB |\n",
      "| Not Installed    | Cloud Pub/Sub Emulator                               | pubsub-emulator          |  33.2 MiB |\n",
      "| Not Installed    | Emulator Reverse Proxy                               | emulator-reverse-proxy   |  14.5 MiB |\n",
      "| Not Installed    | Google Container Local Builder                       | container-builder-local  |   3.7 MiB |\n",
      "| Not Installed    | Google Container Registry's Docker credential helper | docker-credential-gcr    |   2.2 MiB |\n",
      "| Not Installed    | gcloud Alpha Commands                                | alpha                    |   < 1 MiB |\n",
      "| Not Installed    | gcloud Beta Commands                                 | beta                     |   < 1 MiB |\n",
      "| Not Installed    | gcloud app Java Extensions                           | app-engine-java          | 130.9 MiB |\n",
      "| Not Installed    | gcloud app PHP Extensions                            | app-engine-php           |           |\n",
      "| Not Installed    | gcloud app Python Extensions                         | app-engine-python        |   6.3 MiB |\n",
      "| Not Installed    | kubectl                                              | kubectl                  |  16.0 MiB |\n",
      "+------------------+------------------------------------------------------+--------------------------+-----------+\n",
      "To install or remove components at your current SDK version [170.0.1], run:\n",
      "  $ gcloud components install COMPONENT_ID\n",
      "  $ gcloud components remove COMPONENT_ID\n",
      "\n",
      "To update your SDK installation to the latest version [172.0.1], run:\n",
      "  $ gcloud components update\n",
      "\n",
      "==> Source [/src/lexicon/google-cloud-sdk/completion.bash.inc] in your profile to enable shell command completion for gcloud.\n",
      "==> Source [/src/lexicon/google-cloud-sdk/path.bash.inc] in your profile to add the Google Cloud SDK command line tools to your $PATH.\n",
      "\n",
      "For more information on how to get started, please visit:\n",
      "  https://cloud.google.com/sdk/docs/quickstarts\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!CLOUDSDK_CORE_DISABLE_PROMPTS=1 ./google-cloud-sdk/install.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate with Google Cloud API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 34: google-cloud-sdk/completion.bash.inc: Syntax error: \"(\" unexpected (expecting \"}\")\r\n"
     ]
    }
   ],
   "source": [
    "!. google-cloud-sdk/completion.bash.inc && \\\n",
    ". google-cloud-sdk/path.bash.inc && \\\n",
    "gcloud auth activate-service-account lexicon-bot@exemplary-oath-179301.iam.gserviceaccount.com --key-file=Lexicon-e94eff39fad7.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']='/home/paperspace/lexicon/Lexicon-e94eff39fad7.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test out Cloud Spech API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'google.cloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ff4f235ec561>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Imports the Google Cloud client library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspeech\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeech\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menums\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeech\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'google.cloud'"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import speech\n",
    "from google.cloud.speech import enums\n",
    "from google.cloud.speech import types\n",
    "\n",
    "# Instantiates a client\n",
    "client = speech.SpeechClient()\n",
    "\n",
    "# The name of the dev-test audio file to transcribe\n",
    "dev_file_name_0 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0000.flac')\n",
    "gt0 = 'GO DO YOU HEAR'\n",
    "\n",
    "dev_file_name_1 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0001.flac')\n",
    "gt1 = 'BUT IN LESS THAN FIVE MINUTES THE STAIRCASE GROANED BENEATH AN EXTRAORDINARY WEIGHT'\n",
    "\n",
    "# The name of the test audio file to transcribe\n",
    "dev_file_name_2 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0002.flac')\n",
    "gt2 = 'AT THIS MOMENT THE WHOLE SOUL OF THE OLD MAN SEEMED CENTRED IN HIS EYES WHICH BECAME BLOODSHOT THE VEINS OF THE THROAT SWELLED HIS CHEEKS AND TEMPLES BECAME PURPLE AS THOUGH HE WAS STRUCK WITH EPILEPSY NOTHING WAS WANTING TO COMPLETE THIS BUT THE UTTERANCE OF A CRY'\n",
    "\n",
    "dev_file_name_3 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0003.flac')\n",
    "gt3 = 'AND THE CRY ISSUED FROM HIS PORES IF WE MAY THUS SPEAK A CRY FRIGHTFUL IN ITS SILENCE'\n",
    "\n",
    "dev_file_name_4 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0004.flac')\n",
    "gt4 = \"D'AVRIGNY RUSHED TOWARDS THE OLD MAN AND MADE HIM INHALE A POWERFUL RESTORATIVE\"\n",
    "\n",
    "\n",
    "test_file_name_1 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'RNN-Tutorial-master',\n",
    "    'data',\n",
    "    'raw',\n",
    "    'librivox',\n",
    "    'LibriSpeech',\n",
    "    'test-clean-wav',\n",
    "    '4507-16021-0019.wav')\n",
    "\n",
    "\n",
    "audio_files = {dev_file_name_0:gt0, dev_file_name_1:gt1, dev_file_name_2:gt2, dev_file_name_3:gt3, dev_file_name_4:gt4}\n",
    "\n",
    "\n",
    "# Loads the audio into memory\n",
    "with io.open(dev_file_name_2, 'rb') as audio_file:\n",
    "    content = audio_file.read()\n",
    "    audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "config = types.RecognitionConfig(\n",
    "    encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
    "    sample_rate_hertz=16000,\n",
    "    language_code='en-US',\n",
    "    max_alternatives=10,\n",
    "    profanity_filter=False,\n",
    "    enable_word_time_offsets=True)\n",
    "\n",
    "# Detects speech and words in the audio file\n",
    "operation = client.long_running_recognize(config, audio)\n",
    "\n",
    "print('Waiting for operation to complete...')\n",
    "result = operation.result(timeout=90)\n",
    "\n",
    "alternatives = result.results[0].alternatives\n",
    "for alternative in alternatives:\n",
    "    print('Transcript: {}'.format(alternative.transcript))\n",
    "    print('Confidence Score: {}'.format(alternative.confidence))\n",
    "\n",
    "    for word_info in alternative.words:\n",
    "        word = word_info.word\n",
    "        start_time = word_info.start_time\n",
    "        end_time = word_info.end_time\n",
    "        start = start_time.seconds + start_time.nanos * 1e-9\n",
    "        end = end_time.seconds + end_time.nanos * 1e-9\n",
    "        delta = end - start\n",
    "        \n",
    "        print('Word: {}, start_time (s): {}, end_time (s): {}, total_time (s): {}'.format(\n",
    "            word,\n",
    "            start,\n",
    "            end,\n",
    "            delta))\n",
    "        \n",
    "        #TODO: Do we need to figure out how to assign words to alternatives?\n",
    "            # If same amounts, assign words to index of parsed word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Download the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "\n",
    "librispeech_dataset_folder_path = 'LibriSpeech'\n",
    "tar_gz_path = 'dev-clean.tar.gz'\n",
    "\n",
    "books_path = 'original-books.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(books_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Librispeech Book Texts') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://www.openslr.org/resources/12/original-books.tar.gz',\n",
    "            books_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(librispeech_dataset_folder_path+'/books'):\n",
    "    with tarfile.open(books_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Librispeech dev-clean.tar.gz') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://www.openslr.org/resources/12/dev-clean.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(librispeech_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Preprocess Dataset - Download Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.2.4.tar.gz (1.2MB)\n",
      "\u001b[K    100% |################################| 1.2MB 700kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: six in /root/miniconda3/envs/carnd-term1/lib/python3.5/site-packages (from nltk)\n",
      "Building wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/79/8b/2a/b2da7fce57a1fd9b20b08fa8800c83b6fde62af9e880722e29\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk #NLP Toolkit\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gensim Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41 text files in the directory: /src/lexicon/LibriSpeech/books/utf-8/**/*.txt*\n"
     ]
    }
   ],
   "source": [
    "# Prepare a plain text corpus from which we train a languague model\n",
    "import glob\n",
    "\n",
    "# Gather all text files from directory\n",
    "WORKING_DIRECTORY = os.path.join(os.getcwd(),'LibriSpeech/')\n",
    "\n",
    "# TRAINING_DIRECTORY = os.path.abspath(os.path.join(os.sep,'Volumes',\"My\\ Passport\\ for\\ Mac\",'lexicon','LibriSpeech'))\n",
    "dev_path = \"{}{}{}{}\".format(WORKING_DIRECTORY, 'dev-clean/', '**/', '*.txt*')\n",
    "train_path = \"{}{}{}{}{}\".format(WORKING_DIRECTORY, 'books/', 'utf-8/', '**/', '*.txt*')\n",
    "\n",
    "text_paths = sorted(glob.glob(train_path, recursive=True))\n",
    "print('Found',len(text_paths),'text files in the directory:', train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset - Tokenize Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "# reading the file in unicode format using codecs library    \n",
    "stoplist = set(stopwords.words('english'))\n",
    "# Strip punctuation\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation) \n",
    "        \n",
    "corpus_raw = u\"\"\n",
    "for book_filename in text_paths:\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        lines = book_file.read()\n",
    "        corpus_raw += lines\n",
    "\n",
    "               \n",
    "# Tokenize\n",
    "tokenized_words = nltk.tokenize.word_tokenize(corpus_raw)\n",
    "\n",
    "## Clean the tokens ##\n",
    "# Remove stop words\n",
    "tokenized_words = [word for word in tokenized_words if word not in stoplist]\n",
    "\n",
    "# Remove single-character tokens (mostly punctuation)\n",
    "tokenized_words = [word for word in tokenized_words if len(word) > 1]\n",
    "\n",
    "# Remove numbers\n",
    "tokenized_words = [word for word in tokenized_words if not word.isnumeric()]\n",
    "\n",
    "# Lowercase all words (default_stopwords are lowercase too)\n",
    "tokenized_words = [word.lower() for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset - Extract N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.probability import LidstoneProbDist\n",
    "from nltk.collocations import *\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "bigram_model = nltk.bigrams(tokenized_words)\n",
    "bigram_model = sorted(bigram_model, key=lambda item: item[1], reverse=True)  \n",
    "# print(bigram_model)\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "np.save(\"lang_model.npy\",bigram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word|Freq:\n",
      "('of', 'the')|423\n",
      "('in', 'the')|264\n",
      "('to', 'the')|176\n",
      "('on', 'the')|125\n",
      "('and', 'the')|124\n",
      "('it', 'was')|112\n",
      "('at', 'the')|85\n",
      "('of', 'his')|84\n",
      "('it', 'is')|76\n",
      "('with', 'the')|75\n",
      "('to', 'be')|74\n",
      "('he', 'was')|70\n",
      "('for', 'the')|69\n",
      "('had', 'been')|66\n",
      "('he', 'had')|64\n",
      "('by', 'the')|62\n",
      "('all', 'the')|60\n",
      "('from', 'the')|59\n",
      "('that', 'the')|56\n",
      "('into', 'the')|52\n",
      "('that', 'he')|49\n",
      "('one', 'of')|47\n",
      "('in', 'his')|44\n",
      "('there', 'was')|44\n",
      "('out', 'of')|42\n",
      "('was', 'the')|38\n",
      "('did', 'not')|38\n",
      "('the', 'first')|37\n",
      "('of', 'her')|37\n",
      "('the', 'other')|37\n",
      "('the', 'old')|36\n",
      "('she', 'was')|36\n",
      "('is', 'the')|35\n",
      "('as', 'if')|34\n",
      "('have', 'been')|33\n",
      "('she', 'had')|33\n",
      "('over', 'the')|33\n",
      "('they', 'were')|33\n",
      "('and', 'he')|33\n",
      "('as', 'the')|32\n",
      "('as', 'he')|31\n",
      "('to', 'his')|30\n",
      "('but', 'the')|29\n",
      "('to', 'me')|29\n",
      "('and', 'his')|29\n",
      "('could', 'not')|29\n",
      "('and', 'to')|28\n",
      "('with', 'his')|28\n",
      "('the', 'same')|27\n",
      "('he', 'would')|26\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(bigram_model)\n",
    "\n",
    "# Output top 50 words\n",
    "print(\"Word|Freq:\")\n",
    "for word, frequency in fdist.most_common(50):\n",
    "    print(u'{}|{}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Listing the words that can follow after 'greater':\n",
      " dict_keys(['is', 'part', 'the', 'care', 'number', 'wants', 'quandary'])\n",
      "\n",
      "Listing 20 most frequent words to come after 'greater':\n",
      " [('is', 1), ('part', 1), ('the', 1), ('care', 1), ('number', 1), ('wants', 1), ('quandary', 1)]\n"
     ]
    }
   ],
   "source": [
    "cfreq_2gram = nltk.ConditionalFreqDist(bigram_model)\n",
    "# print('Conditional Frequency Conditions:\\n', cfreq_2gram)\n",
    "print()\n",
    "\n",
    "# First access the FreqDist associated with \"one\", then the keys in that FreqDist\n",
    "print(\"Listing the words that can follow after 'greater':\\n\", cfreq_2gram[\"greater\"].keys())\n",
    "print()\n",
    "\n",
    "# Determine Most common in conditional frequency\n",
    "print(\"Listing 20 most frequent words to come after 'greater':\\n\", cfreq_2gram[\"greater\"].most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Language Model using KenLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
      "  Downloading https://github.com/kpu/kenlm/archive/master.zip (518kB)\n",
      "\u001b[K    100% |################################| 522kB 1.5MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: kenlm\n",
      "  Running setup.py install for kenlm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed kenlm-0.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade https://github.com/kpu/kenlm/archive/master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEMO - Evaluate Sentences Using Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each word in the evaluation list:\n",
    "# Select word and determine its frequency distribution\n",
    "# Grab probability of second word in the list\n",
    "# Continue this process until the sentence is scored\n",
    "\n",
    "# Add small epsilon value to avoid division by zero\n",
    "epsilon = 0.0000001\n",
    "\n",
    "# Loads the audio into memory\n",
    "for audio, ground_truth in audio_files.items():\n",
    "    with io.open(audio, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "        audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code='en-US',\n",
    "        max_alternatives=10,\n",
    "        profanity_filter=False,\n",
    "        enable_word_time_offsets=True)\n",
    "\n",
    "    # Detects speech and words in the audio file\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "\n",
    "    print('Waiting for operation to complete...')\n",
    "    result = operation.result(timeout=90)\n",
    "\n",
    "    alternatives = result.results[0].alternatives\n",
    "\n",
    "\n",
    "    print(\"API Results: \", alternatives)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    rerank_results = {}\n",
    "    for alternative in alternatives:\n",
    "        sent = alternative.transcript\n",
    "\n",
    "        words = nltk.tokenize.word_tokenize(sent)\n",
    "        probs = np.ones_like(words, dtype=np.float32)*epsilon\n",
    "        # print(words,'\\n',probs)\n",
    "        for word in words:\n",
    "            if words.index(word) < len(words)-1: \n",
    "                freq = cfreq_2gram[word].freq(words[words.index(word)+1])\n",
    "                probs[words.index(word)] = freq\n",
    "            # print(probs)\n",
    "\n",
    "        lexicon_score = np.sum(probs)\n",
    "        # print(word_score)\n",
    "\n",
    "        # Re-rank alternatives using a weighted average of the two scores\n",
    "        api_weight = 0.95\n",
    "        confidence_score = alternative.confidence*api_weight + lexicon_score*(1-api_weight)\n",
    "        rerank_results[alternative.transcript] = confidence_score\n",
    "\n",
    "    print(\"RE-RANKED Results: \\n\", rerank_results)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    import operator\n",
    "    index, value = max(enumerate(list(rerank_results.values())), key=operator.itemgetter(1))\n",
    "    # Select Corresponding Transcript:\n",
    "    script=''\n",
    "    for trnscript, confidence in rerank_results.items():\n",
    "        if confidence == value:\n",
    "            script = trnscript\n",
    "\n",
    "    # Evaluate the differences between the Original and the Reranked transcript:\n",
    "    print(\"ORIGINAL Transcript: \\n'{0}' \\nwith a confidence_score of: {1}\".format(alternative.transcript, alternative.confidence))\n",
    "    print(\"RE-RANKED Transcript: \\n'{0}' \\nwith a confidence_score of: {1}\".format(script, value))\n",
    "    print(\"GROUND TRUTH TRANSCRIPT: \\n{0}\".format(ground_truth))\n",
    "    print()\n",
    "    ranked_differences = list(set(nltk.tokenize.word_tokenize(alternative.transcript.lower())) -\n",
    "                              set(nltk.tokenize.word_tokenize(script.lower())))\n",
    "    if len(ranked_differences) == 0:  \n",
    "        print(\"No reranking was performed. The transcripts match!\")\n",
    "    else:\n",
    "        print(\"The original transcript was RE-RANKED. The transcripts do not match!\")\n",
    "        print(\"Differences between original and re-ranked: \", ranked_differences)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    # Evaluate Differences between the Original and Ground Truth:\n",
    "    gt_orig_diff = list(set(nltk.tokenize.word_tokenize(alternative.transcript.lower())) -\n",
    "                              set(nltk.tokenize.word_tokenize(ground_truth.lower())))\n",
    "    if len(gt_orig_diff) == 0:  \n",
    "        print(\"The ORIGINAL transcript matches ground truth!\")\n",
    "    else:\n",
    "        print(\"The original transcript DOES NOT MATCH ground truth.\")\n",
    "        print(\"Differences between original and ground truth: \", gt_orig_diff)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    gt_rr_diff = list(set(nltk.tokenize.word_tokenize(script.lower())) -\n",
    "                              set(nltk.tokenize.word_tokenize(ground_truth.lower())))\n",
    "    if len(gt_rr_diff) == 0:  \n",
    "        print(\"The RE-RANKED transcript matches ground truth!\")\n",
    "    else:\n",
    "        print(\"The RE_RANKED transcript DOES NOT MATCH ground truth.\")\n",
    "        print(\"Differences between Reranked and ground truth: \", gt_rr_diff)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # Compute the Levenshtein Distance (a.k.a. Edit Distance)\n",
    "#     import nltk.metrics.distance as lev_dist\n",
    "    \n",
    "    # Google API Edit Distance\n",
    "    goog_edit_distance = nltk.edit_distance(alternative.transcript.lower(), ground_truth.lower())\n",
    "    \n",
    "    # Re-Ranked Edit Distance\n",
    "    rr_edit_distance = nltk.edit_distance(script.lower(), ground_truth.lower())\n",
    "\n",
    "    \n",
    "    print(\"ORIGINAL Edit Distance: \\n{0}\".format(goog_edit_distance))\n",
    "    print(\"RE-RANKED Edit Distance: \\n{0}\".format(rr_edit_distance))\n",
    "    print()\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 97 text files in the directory: /src/lexicon/LibriSpeech/dev-clean/**/*.txt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'types' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-e7740e7ed017>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0maudio_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecognitionAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     config = types.RecognitionConfig(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'types' is not defined"
     ]
    }
   ],
   "source": [
    "# Gather all samples, load into dictionary\n",
    "# Prepare a plain text corpus from which we train a languague model\n",
    "import glob\n",
    "import operator\n",
    "\n",
    "# Gather all text files from directory\n",
    "WORKING_DIRECTORY = os.path.join(os.getcwd(),'LibriSpeech/')\n",
    "\n",
    "# TRAINING_DIRECTORY = os.path.abspath(os.path.join(os.sep,'Volumes',\"My\\ Passport\\ for\\ Mac\",'lexicon','LibriSpeech'))\n",
    "dev_path = \"{}{}{}{}\".format(WORKING_DIRECTORY, 'dev-clean/', '**/', '*.txt')\n",
    "train_path = \"{}{}{}{}{}\".format(WORKING_DIRECTORY, 'books/', 'utf-8/', '**/', '*.txt*')\n",
    "\n",
    "text_paths = sorted(glob.glob(dev_path, recursive=True))\n",
    "print('Found',len(text_paths),'text files in the directory:', dev_path)\n",
    "\n",
    "transcripts = {}\n",
    "for document in text_paths:\n",
    "    with codecs.open(document, 'r', 'utf-8') as filep:\n",
    "        for i,line in enumerate(filep):\n",
    "            transcripts[line.split()[0]] = ' '.join(line.split()[1:])\n",
    "\n",
    "# Save Dictionary in Pickle File\n",
    "\n",
    "\n",
    "## Evaluate all samples found ##\n",
    "cloud_speech_api_accuracy = []\n",
    "custom_lang_model_accuracy = []\n",
    "epsilon = 0.000000001\n",
    "api_weight = 0.85\n",
    "steps = 0\n",
    "# Pull In Audio File\n",
    "for filename, gt_transcript in transcripts.items():\n",
    "    steps += 1\n",
    "    dirs = filename.split('-')\n",
    "    \n",
    "    audio_filepath = dev_file_name_0 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    dirs[0],\n",
    "    dirs[1],\n",
    "    \"{0}.flac\".format(filename))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Load the audio into memory\n",
    "    with io.open(audio_filepath, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "        audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code='en-US',\n",
    "        max_alternatives=10,\n",
    "        profanity_filter=False,\n",
    "        enable_word_time_offsets=True)\n",
    "\n",
    "    # Detects speech and words in the audio file\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "    result = operation.result(timeout=90)\n",
    "    alternatives = result.results[0].alternatives\n",
    "\n",
    "\n",
    "    # Evaluate API Results for Re-Ranking:\n",
    "    rerank_results = {}\n",
    "    for alternative in alternatives:\n",
    "        sent = alternative.transcript\n",
    "        \n",
    "        # Strip punctuation\n",
    "        translate_table = dict((ord(char), None) for char in string.punctuation)        \n",
    "        sent = sent.translate(translate_table) # remove punctuations\n",
    "\n",
    "        words = nltk.tokenize.word_tokenize(sent)\n",
    "        probs = np.ones_like(words, dtype=np.float32)*epsilon\n",
    "\n",
    "        for word in words:\n",
    "            if words.index(word) < len(words)-1: \n",
    "                freq = cfreq_2gram[word].freq(words[words.index(word)+1])\n",
    "                probs[words.index(word)] = freq\n",
    "\n",
    "        lexicon_score = np.sum(probs)\n",
    "\n",
    "        # Re-rank alternatives using a weighted average of the two scores\n",
    "        confidence_score = alternative.confidence*api_weight + lexicon_score*(1-api_weight)\n",
    "        rerank_results[alternative.transcript] = confidence_score\n",
    "\n",
    "\n",
    "    \n",
    "    index, value = max(enumerate(list(rerank_results.values())), key=operator.itemgetter(1))\n",
    "    # Select Corresponding Transcript:\n",
    "    script=''\n",
    "    for trnscript, confidence in rerank_results.items():\n",
    "        if confidence == value:\n",
    "            script = trnscript\n",
    "                \n",
    "    # Compute the Accuracy, based on the Levenshtein Distance (a.k.a. Edit Distance)\n",
    "    gcs_ed = nltk.edit_distance(alternative.transcript.lower(), gt_transcript.lower())\n",
    "    gcs_upper_bound = max(len(alternative.transcript),len(gt_transcript))\n",
    "    gcs_accuracy = (1.0 - gcs_ed/gcs_upper_bound)\n",
    "    \n",
    "    clm_ed = nltk.edit_distance(script.lower(), gt_transcript.lower())\n",
    "    clm_upper_bound = max(len(script),len(gt_transcript))\n",
    "    clm_accuracy = (1.0 - clm_ed/clm_upper_bound)\n",
    "    \n",
    "    cloud_speech_api_accuracy.append(gcs_accuracy)\n",
    "    custom_lang_model_accuracy.append(clm_accuracy)\n",
    "\n",
    "    if steps % 10 == 0:\n",
    "        print(\"{0} Transcripts Processed.\".format(steps))\n",
    "        print('Average API Accuracy:', np.mean(cloud_speech_api_accuracy))\n",
    "        print('Average Custom Model Accuracy:', np.mean(custom_lang_model_accuracy))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41 text files in the directory: /src/lexicon/LibriSpeech/books/utf-8/**/*.txt*\n",
      "Books:  ['/src/lexicon/LibriSpeech/books/utf-8/1004/1004.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/10123/10123.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/10359/10359.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/10360/10360.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/10378/10378.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/10390/10390.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/1193/1193.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/12441/12441-0.txt', '/src/lexicon/LibriSpeech/books/utf-8/1249/1249.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/1325/1325.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/1674/1674.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/2046/2046.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/2147/2147.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/2184/2184.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/2383/2383.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/2486/2486.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/2488/2488.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/2512/2512-0.txt', '/src/lexicon/LibriSpeech/books/utf-8/2515/2515.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/2678/2678.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/2679/2679.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/269/269-0.txt', '/src/lexicon/LibriSpeech/books/utf-8/282/282-0.txt', '/src/lexicon/LibriSpeech/books/utf-8/2891/2891.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/3053/3053.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/3169/3169.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/325/325.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/3300/3300.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/34757/34757-0.txt', '/src/lexicon/LibriSpeech/books/utf-8/3604/3604.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/3623/3623.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/3697/3697.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/37660/37660-0.txt', '/src/lexicon/LibriSpeech/books/utf-8/4028/4028.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/4042/4042.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/435/435.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/6456/6456.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/7098/7098.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/76/76.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/778/778.txt.utf-8', '/src/lexicon/LibriSpeech/books/utf-8/786/786-0.txt']\n"
     ]
    }
   ],
   "source": [
    "# Prepare a plain text corpus from which we train a languague model\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Gather all text files from directory\n",
    "WORKING_DIRECTORY = os.path.join(os.getcwd(),'LibriSpeech/')\n",
    "\n",
    "# TRAINING_DIRECTORY = os.path.abspath(os.path.join(os.sep,'Volumes',\"My\\ Passport\\ for\\ Mac\",'lexicon','LibriSpeech'))\n",
    "dev_path = \"{}{}{}{}\".format(WORKING_DIRECTORY, 'dev-clean/', '**/', '*.txt')\n",
    "train_path = \"{}{}{}{}{}\".format(WORKING_DIRECTORY, 'books/', 'utf-8/', '**/', '*.txt*')\n",
    "\n",
    "text_paths = sorted(glob.glob(train_path, recursive=True))\n",
    "print('Found',len(text_paths),'text files in the directory:', train_path)\n",
    "print('Books: ', text_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import tensorflow as tf\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "# reading the file in unicode format using codecs library    \n",
    "stoplist = set(stopwords.words('english'))\n",
    "# Strip punctuation\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation) \n",
    "        \n",
    "corpus_raw = u\"\"\n",
    "for book_filename in text_paths:\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        lines = book_file.read()\n",
    "        corpus_raw += lines \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 189419\n",
      "Number of speeches: 3\n",
      "Average number of sentences in each speech: 0.0\n",
      "Number of lines: 422872\n",
      "Average number of words in each line: 8.899073478499403\n",
      "\n",
      "The sentences 0 to 10:\n",
      "***The Project Gutenberg Etext of The Divine Comedy of Dante***\n",
      "Translanted by Henry Wadsworth Longfellow\n",
      "\n",
      "\n",
      "Copyright laws are changing all over the world, be sure to check\n",
      "the copyright laws for your country before posting these files!!\n",
      "\n",
      "Please take a look at the important information in this header.\n",
      "We encourage you to keep this file on your own disk, keeping an\n",
      "electronic path open for the next readers.  Do not remove this.\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in corpus_raw.split()})))\n",
    "speeches = corpus_raw.split('\\n\\n')\n",
    "print('Number of speeches: {}'.format(len(speeches)))\n",
    "sentence_count_speech = [speeches.count('\\n') for speech in speeches]\n",
    "print('Average number of sentences in each speech: {}'.format(np.average(sentence_count_speech)))\n",
    "\n",
    "sentences = [sentence for speech in speeches for sentence in speech.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(corpus_raw.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    vocabs = set(text)\n",
    "    int_to_vocab = dict(enumerate(vocabs, 1))\n",
    "    vocab_to_int = { v: k for k, v in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    return {\n",
    "        '.': '||period||',\n",
    "        ',': '||comma||',\n",
    "        '\"': '||quotation||',\n",
    "        ';': '||semi_colon||',\n",
    "        '!': '||exclamation||',\n",
    "        '?': '||question||',\n",
    "        '(': '||left_parentheses||',\n",
    "        ')': '||right_parentheses||',\n",
    "        '*': '||star||',\n",
    "        '--': '||dash||',\n",
    "        '\\n': '||return||'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import codecs\n",
    "\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "corp_file = open(os.path.join(os.getcwd(),\"saved_corp.txt\"), \"w\", encoding=\"utf-8\")\n",
    "\n",
    "corpus_raw = corpus_raw.encode('ascii', 'ignore')\n",
    "corpus_raw = corpus_raw.decode(\"utf-8\")\n",
    "\n",
    "corp_file.write(corpus_raw)\n",
    "corp_file.close\n",
    "\n",
    "corp_filename = os.path.join(os.getcwd(),\"saved_corp.txt\")\n",
    "\n",
    "helper.preprocess_and_save_data(corp_filename, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return (\n",
    "        tf.placeholder(tf.int32, shape=(None, None), name='input'),\n",
    "        tf.placeholder(tf.int32, shape=(None, None)),\n",
    "        tf.placeholder(tf.float32, name='keep_prob'),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([\n",
    "        tf.contrib.rnn.BasicLSTMCell(rnn_size),\n",
    "        tf.contrib.rnn.BasicLSTMCell(rnn_size)])\n",
    "\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    initial_state = tf.identity(initial_state, 'initial_state')\n",
    "\n",
    "    return cell, initial_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.embed_sequence(input_data, vocab_size, embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, 'final_state')\n",
    "\n",
    "    return outputs, final_state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    inputs = get_embed(input_data, vocab_size, rnn_size)\n",
    "    outputs, final_state = build_rnn(cell, inputs)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, None)\n",
    "\n",
    "    return logits, final_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: A Numpy array where each item is a tuple of (batch of input, batch of target).\n",
    "    \"\"\"\n",
    "    n_batches = int(len(int_text) / (batch_size * seq_length))\n",
    "\n",
    "    # Drop the last few characters to make only full batches\n",
    "    xdata = np.array(int_text[: n_batches * batch_size * seq_length])\n",
    "    ydata = np.array(int_text[1: n_batches * batch_size * seq_length+1])\n",
    "\n",
    "    x_batches = np.split(xdata.reshape(batch_size, -1), n_batches, 1)\n",
    "    y_batches = np.split(ydata.reshape(batch_size, -1), n_batches, 1)\n",
    "\n",
    "    return np.array(list(zip(x_batches, y_batches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 10\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 512\n",
    "# Sequence Length\n",
    "seq_length = 9\n",
    "# Learning Rate\n",
    "learning_rate = 0.002\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 100\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)+1\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/4163   train_loss = 11.356\n",
      "Epoch   0 Batch  100/4163   train_loss = 7.034\n",
      "Epoch   0 Batch  200/4163   train_loss = 6.989\n",
      "Epoch   0 Batch  300/4163   train_loss = 6.881\n",
      "Epoch   0 Batch  400/4163   train_loss = 6.997\n",
      "Epoch   0 Batch  500/4163   train_loss = 6.815\n",
      "Epoch   0 Batch  600/4163   train_loss = 6.929\n",
      "Epoch   0 Batch  700/4163   train_loss = 6.431\n",
      "Epoch   0 Batch  800/4163   train_loss = 6.017\n",
      "Epoch   0 Batch  900/4163   train_loss = 5.947\n",
      "Epoch   0 Batch 1000/4163   train_loss = 5.915\n",
      "Epoch   0 Batch 1100/4163   train_loss = 5.736\n",
      "Epoch   0 Batch 1200/4163   train_loss = 5.973\n",
      "Epoch   0 Batch 1300/4163   train_loss = 5.649\n",
      "Epoch   0 Batch 1400/4163   train_loss = 5.741\n",
      "Epoch   0 Batch 1500/4163   train_loss = 5.608\n",
      "Epoch   0 Batch 1600/4163   train_loss = 5.563\n",
      "Epoch   0 Batch 1700/4163   train_loss = 5.545\n",
      "Epoch   0 Batch 1800/4163   train_loss = 5.593\n",
      "Epoch   0 Batch 1900/4163   train_loss = 5.637\n",
      "Epoch   0 Batch 2000/4163   train_loss = 5.621\n",
      "Epoch   0 Batch 2100/4163   train_loss = 5.588\n",
      "Epoch   0 Batch 2200/4163   train_loss = 5.407\n",
      "Epoch   0 Batch 2300/4163   train_loss = 5.428\n",
      "Epoch   0 Batch 2400/4163   train_loss = 5.292\n",
      "Epoch   0 Batch 2500/4163   train_loss = 5.762\n",
      "Epoch   0 Batch 2600/4163   train_loss = 5.367\n",
      "Epoch   0 Batch 2700/4163   train_loss = 5.297\n",
      "Epoch   0 Batch 2800/4163   train_loss = 5.327\n",
      "Epoch   0 Batch 2900/4163   train_loss = 5.404\n",
      "Epoch   0 Batch 3000/4163   train_loss = 5.400\n",
      "Epoch   0 Batch 3100/4163   train_loss = 5.306\n",
      "Epoch   0 Batch 3200/4163   train_loss = 5.272\n",
      "Epoch   0 Batch 3300/4163   train_loss = 5.151\n",
      "Epoch   0 Batch 3400/4163   train_loss = 5.165\n",
      "Epoch   0 Batch 3500/4163   train_loss = 5.383\n",
      "Epoch   0 Batch 3600/4163   train_loss = 5.278\n",
      "Epoch   0 Batch 3700/4163   train_loss = 5.322\n",
      "Epoch   0 Batch 3800/4163   train_loss = 5.205\n",
      "Epoch   0 Batch 3900/4163   train_loss = 5.342\n",
      "Epoch   0 Batch 4000/4163   train_loss = 5.327\n",
      "Epoch   0 Batch 4100/4163   train_loss = 5.149\n",
      "Epoch   1 Batch   37/4163   train_loss = 5.115\n",
      "Epoch   1 Batch  137/4163   train_loss = 5.168\n",
      "Epoch   1 Batch  237/4163   train_loss = 5.186\n",
      "Epoch   1 Batch  337/4163   train_loss = 5.177\n",
      "Epoch   1 Batch  437/4163   train_loss = 5.178\n",
      "Epoch   1 Batch  537/4163   train_loss = 5.066\n",
      "Epoch   1 Batch  637/4163   train_loss = 4.939\n",
      "Epoch   1 Batch  737/4163   train_loss = 5.143\n",
      "Epoch   1 Batch  837/4163   train_loss = 5.216\n",
      "Epoch   1 Batch  937/4163   train_loss = 5.189\n",
      "Epoch   1 Batch 1037/4163   train_loss = 5.016\n",
      "Epoch   1 Batch 1137/4163   train_loss = 4.835\n",
      "Epoch   1 Batch 1237/4163   train_loss = 5.037\n",
      "Epoch   1 Batch 1337/4163   train_loss = 5.072\n",
      "Epoch   1 Batch 1437/4163   train_loss = 4.940\n",
      "Epoch   1 Batch 1537/4163   train_loss = 4.938\n",
      "Epoch   1 Batch 1637/4163   train_loss = 4.750\n",
      "Epoch   1 Batch 1737/4163   train_loss = 4.804\n",
      "Epoch   1 Batch 1837/4163   train_loss = 4.959\n",
      "Epoch   1 Batch 1937/4163   train_loss = 4.858\n",
      "Epoch   1 Batch 2037/4163   train_loss = 4.991\n",
      "Epoch   1 Batch 2137/4163   train_loss = 4.971\n",
      "Epoch   1 Batch 2237/4163   train_loss = 5.031\n",
      "Epoch   1 Batch 2337/4163   train_loss = 4.965\n",
      "Epoch   1 Batch 2437/4163   train_loss = 4.865\n",
      "Epoch   1 Batch 2537/4163   train_loss = 5.004\n",
      "Epoch   1 Batch 2637/4163   train_loss = 4.992\n",
      "Epoch   1 Batch 2937/4163   train_loss = 4.910\n",
      "Epoch   1 Batch 3037/4163   train_loss = 4.975\n",
      "Epoch   1 Batch 3137/4163   train_loss = 4.982\n",
      "Epoch   1 Batch 3237/4163   train_loss = 4.870\n",
      "Epoch   1 Batch 3337/4163   train_loss = 4.911\n",
      "Epoch   1 Batch 3437/4163   train_loss = 4.751\n",
      "Epoch   1 Batch 3537/4163   train_loss = 4.775\n",
      "Epoch   1 Batch 3637/4163   train_loss = 4.883\n",
      "Epoch   1 Batch 3737/4163   train_loss = 4.652\n",
      "Epoch   1 Batch 3837/4163   train_loss = 4.921\n",
      "Epoch   1 Batch 3937/4163   train_loss = 5.001\n",
      "Epoch   1 Batch 4037/4163   train_loss = 4.601\n",
      "Epoch   1 Batch 4137/4163   train_loss = 4.836\n",
      "Epoch   2 Batch   74/4163   train_loss = 4.911\n",
      "Epoch   2 Batch  174/4163   train_loss = 4.794\n",
      "Epoch   2 Batch  274/4163   train_loss = 4.894\n",
      "Epoch   2 Batch  374/4163   train_loss = 4.833\n",
      "Epoch   2 Batch  474/4163   train_loss = 4.809\n",
      "Epoch   2 Batch  574/4163   train_loss = 4.907\n",
      "Epoch   2 Batch  674/4163   train_loss = 4.791\n",
      "Epoch   2 Batch  774/4163   train_loss = 4.868\n",
      "Epoch   2 Batch  874/4163   train_loss = 4.879\n",
      "Epoch   2 Batch  974/4163   train_loss = 4.690\n",
      "Epoch   2 Batch 1074/4163   train_loss = 4.740\n",
      "Epoch   2 Batch 1174/4163   train_loss = 4.646\n",
      "Epoch   2 Batch 1274/4163   train_loss = 4.575\n",
      "Epoch   2 Batch 1374/4163   train_loss = 4.678\n",
      "Epoch   2 Batch 1474/4163   train_loss = 4.677\n",
      "Epoch   2 Batch 1574/4163   train_loss = 4.827\n",
      "Epoch   2 Batch 1674/4163   train_loss = 4.722\n",
      "Epoch   2 Batch 1774/4163   train_loss = 4.602\n",
      "Epoch   2 Batch 1874/4163   train_loss = 4.634\n",
      "Epoch   2 Batch 1974/4163   train_loss = 4.824\n",
      "Epoch   2 Batch 2074/4163   train_loss = 4.624\n",
      "Epoch   2 Batch 2174/4163   train_loss = 4.675\n",
      "Epoch   2 Batch 2274/4163   train_loss = 4.617\n",
      "Epoch   2 Batch 2374/4163   train_loss = 4.602\n",
      "Epoch   2 Batch 2474/4163   train_loss = 4.575\n",
      "Epoch   2 Batch 2574/4163   train_loss = 4.655\n",
      "Epoch   2 Batch 2674/4163   train_loss = 4.774\n",
      "Epoch   2 Batch 2774/4163   train_loss = 4.666\n",
      "Epoch   2 Batch 2874/4163   train_loss = 4.822\n",
      "Epoch   2 Batch 2974/4163   train_loss = 4.733\n",
      "Epoch   2 Batch 3074/4163   train_loss = 4.713\n",
      "Epoch   2 Batch 3174/4163   train_loss = 4.681\n",
      "Epoch   2 Batch 3274/4163   train_loss = 4.676\n",
      "Epoch   2 Batch 3374/4163   train_loss = 4.602\n",
      "Epoch   2 Batch 3474/4163   train_loss = 4.523\n",
      "Epoch   2 Batch 3574/4163   train_loss = 4.546\n",
      "Epoch   2 Batch 3674/4163   train_loss = 4.573\n",
      "Epoch   2 Batch 3774/4163   train_loss = 4.560\n",
      "Epoch   2 Batch 3874/4163   train_loss = 4.542\n",
      "Epoch   2 Batch 3974/4163   train_loss = 4.661\n",
      "Epoch   2 Batch 4074/4163   train_loss = 4.491\n",
      "Epoch   3 Batch   11/4163   train_loss = 4.724\n",
      "Epoch   3 Batch  111/4163   train_loss = 4.460\n",
      "Epoch   3 Batch  211/4163   train_loss = 4.444\n",
      "Epoch   3 Batch  311/4163   train_loss = 4.463\n",
      "Epoch   3 Batch  411/4163   train_loss = 4.398\n",
      "Epoch   3 Batch  511/4163   train_loss = 4.471\n",
      "Epoch   3 Batch  611/4163   train_loss = 4.479\n",
      "Epoch   3 Batch  711/4163   train_loss = 4.430\n",
      "Epoch   3 Batch  811/4163   train_loss = 4.614\n",
      "Epoch   3 Batch  911/4163   train_loss = 4.645\n",
      "Epoch   3 Batch 1011/4163   train_loss = 4.582\n",
      "Epoch   3 Batch 1111/4163   train_loss = 4.664\n",
      "Epoch   3 Batch 1211/4163   train_loss = 4.367\n",
      "Epoch   3 Batch 1311/4163   train_loss = 4.418\n",
      "Epoch   3 Batch 1411/4163   train_loss = 4.391\n",
      "Epoch   3 Batch 1511/4163   train_loss = 4.319\n",
      "Epoch   3 Batch 1611/4163   train_loss = 4.303\n",
      "Epoch   3 Batch 1711/4163   train_loss = 4.424\n",
      "Epoch   3 Batch 1811/4163   train_loss = 4.388\n",
      "Epoch   3 Batch 1911/4163   train_loss = 4.448\n",
      "Epoch   3 Batch 2011/4163   train_loss = 4.458\n",
      "Epoch   3 Batch 2111/4163   train_loss = 4.366\n",
      "Epoch   3 Batch 2211/4163   train_loss = 4.419\n",
      "Epoch   3 Batch 2311/4163   train_loss = 4.535\n",
      "Epoch   3 Batch 2411/4163   train_loss = 4.423\n",
      "Epoch   3 Batch 2511/4163   train_loss = 4.569\n",
      "Epoch   3 Batch 2611/4163   train_loss = 4.407\n",
      "Epoch   3 Batch 2711/4163   train_loss = 4.506\n",
      "Epoch   3 Batch 2811/4163   train_loss = 4.425\n",
      "Epoch   3 Batch 2911/4163   train_loss = 4.420\n",
      "Epoch   3 Batch 3011/4163   train_loss = 4.434\n",
      "Epoch   3 Batch 3111/4163   train_loss = 4.436\n",
      "Epoch   3 Batch 3211/4163   train_loss = 4.372\n",
      "Epoch   3 Batch 3311/4163   train_loss = 4.514\n",
      "Epoch   3 Batch 3411/4163   train_loss = 4.521\n",
      "Epoch   3 Batch 3511/4163   train_loss = 4.385\n",
      "Epoch   3 Batch 3611/4163   train_loss = 4.332\n",
      "Epoch   3 Batch 3711/4163   train_loss = 4.341\n",
      "Epoch   3 Batch 3811/4163   train_loss = 4.354\n",
      "Epoch   3 Batch 3911/4163   train_loss = 4.442\n",
      "Epoch   3 Batch 4011/4163   train_loss = 4.323\n",
      "Epoch   3 Batch 4111/4163   train_loss = 4.377\n",
      "Epoch   4 Batch   48/4163   train_loss = 4.275\n",
      "Epoch   4 Batch  148/4163   train_loss = 4.372\n",
      "Epoch   4 Batch  248/4163   train_loss = 4.304\n",
      "Epoch   4 Batch  348/4163   train_loss = 4.220\n",
      "Epoch   4 Batch  448/4163   train_loss = 4.320\n",
      "Epoch   4 Batch  548/4163   train_loss = 4.262\n",
      "Epoch   4 Batch  648/4163   train_loss = 4.362\n",
      "Epoch   4 Batch  748/4163   train_loss = 4.323\n",
      "Epoch   4 Batch  848/4163   train_loss = 4.352\n",
      "Epoch   4 Batch  948/4163   train_loss = 4.298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4 Batch 1048/4163   train_loss = 4.461\n",
      "Epoch   4 Batch 1148/4163   train_loss = 4.275\n",
      "Epoch   4 Batch 1248/4163   train_loss = 4.331\n",
      "Epoch   4 Batch 1348/4163   train_loss = 4.208\n",
      "Epoch   4 Batch 1448/4163   train_loss = 4.238\n",
      "Epoch   4 Batch 1548/4163   train_loss = 4.097\n",
      "Epoch   4 Batch 1648/4163   train_loss = 4.157\n",
      "Epoch   4 Batch 1748/4163   train_loss = 4.119\n",
      "Epoch   4 Batch 1848/4163   train_loss = 4.217\n",
      "Epoch   4 Batch 1948/4163   train_loss = 4.122\n",
      "Epoch   4 Batch 2048/4163   train_loss = 4.241\n",
      "Epoch   4 Batch 2148/4163   train_loss = 4.301\n",
      "Epoch   4 Batch 2248/4163   train_loss = 4.248\n",
      "Epoch   4 Batch 2348/4163   train_loss = 4.182\n",
      "Epoch   4 Batch 2448/4163   train_loss = 4.265\n",
      "Epoch   4 Batch 2548/4163   train_loss = 4.365\n",
      "Epoch   4 Batch 2648/4163   train_loss = 4.245\n",
      "Epoch   4 Batch 2748/4163   train_loss = 4.353\n",
      "Epoch   4 Batch 2848/4163   train_loss = 4.196\n",
      "Epoch   4 Batch 2948/4163   train_loss = 4.255\n",
      "Epoch   4 Batch 3048/4163   train_loss = 4.240\n",
      "Epoch   4 Batch 3348/4163   train_loss = 4.167\n",
      "Epoch   4 Batch 3448/4163   train_loss = 4.316\n",
      "Epoch   4 Batch 3548/4163   train_loss = 4.315\n",
      "Epoch   4 Batch 3648/4163   train_loss = 4.279\n",
      "Epoch   4 Batch 3748/4163   train_loss = 4.084\n",
      "Epoch   4 Batch 3848/4163   train_loss = 4.227\n",
      "Epoch   4 Batch 3948/4163   train_loss = 4.084\n",
      "Epoch   4 Batch 4048/4163   train_loss = 4.146\n",
      "Epoch   4 Batch 4148/4163   train_loss = 4.348\n",
      "Epoch   5 Batch   85/4163   train_loss = 4.034\n",
      "Epoch   5 Batch  185/4163   train_loss = 4.043\n",
      "Epoch   5 Batch  285/4163   train_loss = 4.075\n",
      "Epoch   5 Batch  385/4163   train_loss = 4.167\n",
      "Epoch   5 Batch  485/4163   train_loss = 4.143\n",
      "Epoch   5 Batch  585/4163   train_loss = 3.927\n",
      "Epoch   5 Batch  685/4163   train_loss = 4.112\n",
      "Epoch   5 Batch  785/4163   train_loss = 4.232\n",
      "Epoch   5 Batch  885/4163   train_loss = 4.284\n",
      "Epoch   5 Batch  985/4163   train_loss = 4.386\n",
      "Epoch   5 Batch 1085/4163   train_loss = 4.177\n",
      "Epoch   5 Batch 1185/4163   train_loss = 4.150\n",
      "Epoch   5 Batch 1285/4163   train_loss = 4.067\n",
      "Epoch   5 Batch 1385/4163   train_loss = 4.182\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return map(\n",
    "        loaded_graph.get_tensor_by_name,\n",
    "        ['input:0', 'initial_state:0', 'final_state:0', 'probs:0']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return int_to_vocab[probabilities.argmax()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_length = 20\n",
    "prime_word = 'Better'\n",
    "\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word]\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        script = script.replace(' ' + token.lower(), key)\n",
    "    script = script.replace('\\n ', '\\n')\n",
    "    script = script.replace('( ', '(')\n",
    "        \n",
    "    print(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all samples, load into dictionary\n",
    "# Prepare a plain text corpus from which we train a languague model\n",
    "import glob\n",
    "import operator\n",
    "\n",
    "# Gather all text files from directory\n",
    "WORKING_DIRECTORY = os.path.join(os.getcwd(),'LibriSpeech/')\n",
    "\n",
    "# TRAINING_DIRECTORY = os.path.abspath(os.path.join(os.sep,'Volumes',\"My\\ Passport\\ for\\ Mac\",'lexicon','LibriSpeech'))\n",
    "dev_path = \"{}{}{}{}\".format(WORKING_DIRECTORY, 'dev-clean/', '**/', '*.txt')\n",
    "train_path = \"{}{}{}{}{}\".format(WORKING_DIRECTORY, 'books/', 'utf-8/', '**/', '*.txt*')\n",
    "\n",
    "text_paths = sorted(glob.glob(dev_path, recursive=True))\n",
    "print('Found',len(text_paths),'text files in the directory:', dev_path)\n",
    "\n",
    "transcripts = {}\n",
    "for document in text_paths:\n",
    "    with codecs.open(document, 'r', 'utf-8') as filep:\n",
    "        for i,line in enumerate(filep):\n",
    "            transcripts[line.split()[0]] = ' '.join(line.split()[1:])\n",
    "\n",
    "# Save Dictionary in Pickle File\n",
    "\n",
    "\n",
    "## Evaluate all samples found ##\n",
    "cloud_speech_api_accuracy = []\n",
    "custom_lang_model_accuracy = []\n",
    "epsilon = 0.000000001\n",
    "api_weight = 0.85\n",
    "steps = 0\n",
    "# Pull In Audio File\n",
    "for filename, gt_transcript in transcripts.items():\n",
    "    steps += 1\n",
    "    dirs = filename.split('-')\n",
    "    \n",
    "    audio_filepath = dev_file_name_0 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    dirs[0],\n",
    "    dirs[1],\n",
    "    \"{0}.flac\".format(filename))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Load the audio into memory\n",
    "    with io.open(audio_filepath, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "        audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code='en-US',\n",
    "        max_alternatives=10,\n",
    "        profanity_filter=False,\n",
    "        enable_word_time_offsets=True)\n",
    "\n",
    "    # Detects speech and words in the audio file\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "    result = operation.result(timeout=90)\n",
    "    alternatives = result.results[0].alternatives\n",
    "\n",
    "\n",
    "    # Evaluate API Results for Re-Ranking:\n",
    "    rerank_results = {}\n",
    "    for alternative in alternatives:\n",
    "        sent = alternative.transcript\n",
    "        \n",
    "        # Strip punctuation\n",
    "        translate_table = dict((ord(char), None) for char in string.punctuation)        \n",
    "        sent = sent.translate(translate_table) # remove punctuations\n",
    "\n",
    "        words = nltk.tokenize.word_tokenize(sent)\n",
    "        \n",
    "        \n",
    "        # Run Words through Neural Network\n",
    "        gen_length = len(sent)\n",
    "        num_chunks = 3\n",
    "        sent_sections = list(chunks(sent, num_chunks))\n",
    "        potential_scripts = []\n",
    "        alternatives_gen_acc = []\n",
    "        prime_word = ''\n",
    "        \n",
    "        for section in sent_sections[:-1]:\n",
    "            prime_word += section\n",
    "        \n",
    "            loaded_graph = tf.Graph()\n",
    "            with tf.Session(graph=loaded_graph) as sess:\n",
    "                # Load saved model\n",
    "                loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "                loader.restore(sess, load_dir)\n",
    "\n",
    "                # Get Tensors from loaded model\n",
    "                input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "                # Sentences generation setup\n",
    "                gen_sentences = [prime_word]\n",
    "                prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "                # Generate sentences\n",
    "                for n in range(gen_length):\n",
    "                    # Dynamic Input\n",
    "                    dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "                    dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "                    # Get Prediction\n",
    "                    probabilities, prev_state = sess.run(\n",
    "                        [probs, final_state],\n",
    "                        {input_text: dyn_input, initial_state: prev_state})\n",
    "\n",
    "                    pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "                    gen_sentences.append(pred_word)\n",
    "\n",
    "                # Remove tokens\n",
    "                gen_script = ' '.join(gen_sentences)\n",
    "                for key, token in token_dict.items():\n",
    "                    ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "                    gen_script = gen_script.replace(' ' + token.lower(), key)\n",
    "                gen_script = gen_script.replace('\\n ', '\\n')\n",
    "                gen_script = gen_script.replace('( ', '(')\n",
    "\n",
    "                print()\n",
    "                print()\n",
    "                print(gen_script)\n",
    "                \n",
    "                # potential_scripts.append(gen_script)\n",
    "                \n",
    "                # Find Edit distance between word and potential script that was generated\n",
    "                alt_ed = nltk.edit_distance(sent.lower(), gen_script.lower())\n",
    "                alt_upper_bound = max(len(sent),len(gen_script))\n",
    "                alt_accuracy = (1.0 - alt_ed/alt_upper_bound)\n",
    "                alternatives_gen_acc.append(alt_accuracy)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Re-rank alternatives using a weighted average of the two scores\n",
    "        prediction_score = np.mean(alternatives_gen_acc)\n",
    "        rerank_results[alternative.transcript] = confidence_score\n",
    "\n",
    "\n",
    "    \n",
    "    index, value = max(enumerate(list(rerank_results.values())), key=operator.itemgetter(1))\n",
    "    # Select Corresponding Transcript:\n",
    "    script=''\n",
    "    for trnscript, confidence in rerank_results.items():\n",
    "        if confidence == value:\n",
    "            script = trnscript\n",
    "            \n",
    "    \n",
    "    \n",
    "                \n",
    "    # Compute the Accuracy, based on the Levenshtein Distance (a.k.a. Edit Distance)\n",
    "    gcs_ed = nltk.edit_distance(alternative.transcript.lower(), gt_transcript.lower())\n",
    "    gcs_upper_bound = max(len(alternative.transcript),len(gt_transcript))\n",
    "    gcs_accuracy = (1.0 - gcs_ed/gcs_upper_bound)\n",
    "    \n",
    "    clm_ed = nltk.edit_distance(script.lower(), gt_transcript.lower())\n",
    "    clm_upper_bound = max(len(script),len(gt_transcript))\n",
    "    clm_accuracy = (1.0 - clm_ed/clm_upper_bound)\n",
    "    \n",
    "    cloud_speech_api_accuracy.append(gcs_accuracy)\n",
    "    custom_lang_model_accuracy.append(clm_accuracy)\n",
    "\n",
    "    if steps % 10 == 0:\n",
    "        print(\"{0} Transcripts Processed.\".format(steps))\n",
    "        print('Average API Accuracy:', np.mean(cloud_speech_api_accuracy))\n",
    "        print('Average Custom Model Accuracy:', np.mean(custom_lang_model_accuracy))\n",
    "        print()\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
