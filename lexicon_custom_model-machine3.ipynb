{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon - Custom Language Model\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "For this project, I will build a simple custom language model that is able to learn from any text data provided, and return a transcript with confidence values from input posed in speech utterances. I will use Google's cloud-based services to preprocess the input audio data and transcribe into an initial guess. Then I will train a model to improve on Google cloud speech API's response.\n",
    "\n",
    "\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "In order to use Google's cloud-based services, you first need to create an account on the [Google Cloud Platform](https://cloud.google.com//).\n",
    "\n",
    "Then, for each service you want to use, you have to enable use of that service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Download the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "\n",
    "librispeech_dataset_folder_path = 'LibriSpeech'\n",
    "tedlium_dataset_folder_path = 'TEDLIUM_release1'\n",
    "tar_gz_path = 'dev-clean.tar.gz'\n",
    "ted_gz_path = 'TEDLIUM_release1.tar.gz'\n",
    "\n",
    "books_path = 'original-books.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(books_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Librispeech Book Texts') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://www.openslr.org/resources/12/original-books.tar.gz',\n",
    "            books_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(librispeech_dataset_folder_path+'/books'):\n",
    "    with tarfile.open(books_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Librispeech dev-clean.tar.gz') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://www.openslr.org/resources/12/dev-clean.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(librispeech_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "if not isfile(ted_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Tedlium TEDLIUM_release1.tar.gz') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://www.openslr.org/resources/7/TEDLIUM_release1.tar.gz',\n",
    "            ted_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(tedlium_dataset_folder_path):\n",
    "    with tarfile.open(ted_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Check the Version of TensorFlow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41 text files in the directories /src/LibriSpeech/books/utf-8/**/*.txt* and 774 stm files in directory: /src/TEDLIUM_release1/train/**/*.stm:\n"
     ]
    }
   ],
   "source": [
    "# Prepare a plain text corpus from which we train a languague model\n",
    "import glob\n",
    "import os\n",
    "import utils\n",
    "\n",
    "# Gather all text files from directory\n",
    "LIBRISPEECH_DIRECTORY = os.path.join(os.getcwd(),'LibriSpeech/')\n",
    "TEDLIUM_DIRECTORY = os.path.join(os.getcwd(),'TEDLIUM_release1/')\n",
    "\n",
    "# TRAINING_DIRECTORY = os.path.abspath(os.path.join(os.sep,'Volumes',\"My\\ Passport\\ for\\ Mac\",'lexicon','LibriSpeech'))\n",
    "dev_path = \"{}{}{}{}\".format(LIBRISPEECH_DIRECTORY, 'dev-clean/', '**/', '*.txt*')\n",
    "train_librispeech_path = \"{}{}{}{}{}\".format(LIBRISPEECH_DIRECTORY, 'books/', 'utf-8/', '**/', '*.txt*')\n",
    "TED_path = \"{}{}{}{}\".format(TEDLIUM_DIRECTORY,'train/','**/', '*.stm')\n",
    "\n",
    "text_paths = sorted(glob.glob(train_librispeech_path, recursive=True))\n",
    "stm_paths = sorted(glob.glob(TED_path, recursive=True))\n",
    "\n",
    "print('Found',len(text_paths),\"text files in the directories {0} and {1} stm files in directory: {2}:\".format(train_librispeech_path, len(stm_paths),TED_path ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "      \n",
    "corpus_raw = u\"\"\n",
    "stm_segments = []\n",
    "for book_filename in text_paths:\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        lines = book_file.read()\n",
    "        corpus_raw += lines\n",
    "for stm_filename in stm_paths[:100]: # Process STM file (Tedlium)\n",
    "        stm_segments.append(utils.parse_stm_file(stm_filename))\n",
    "        for segments in stm_segments:\n",
    "            for segment in segments:\n",
    "                corpus_raw += segment.transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 207061\n",
      "Number of speeches: 3\n",
      "Average number of sentences in each speech: 0.0\n",
      "Number of lines: 422872\n",
      "Average number of words in each line: 32.99537212206057\n",
      "\n",
      "The sentences 10 to 100:\n",
      "\n",
      "\n",
      "**Welcome To The World of Free Plain Vanilla Electronic Texts**\n",
      "\n",
      "**Etexts Readable By Both Humans and By Computers, Since 1971**\n",
      "\n",
      "*These Etexts Prepared By Hundreds of Volunteers and Donations*\n",
      "\n",
      "Information on contacting Project Gutenberg to get Etexts, and\n",
      "further information is included below.  We need your donations.\n",
      "\n",
      "\n",
      "The Divine Comedy of Dante by H. W. Longfellow\n",
      "\n",
      "August, 1997  [Etext #1004]\n",
      "\n",
      "\n",
      "***The Project Gutenberg Etext of The Divine Comedy of Dante***\n",
      "****This file should be named 0ddcl10.txtt or 0ddcl10.zip*****\n",
      "\n",
      "Corrected EDITIONS of our etexts get a new NUMBER, 0ddcl11.txt.\n",
      "VERSIONS based on separate sources get new LETTER, 0ddcl10a.txt.\n",
      "\n",
      "\n",
      "This etext was prepared by Dennis McCarthy, Atlanta, GA.\n",
      "\n",
      "\n",
      "We are now trying to release all our books one month in advance\n",
      "of the official release dates, for time for better editing.\n",
      "\n",
      "\n",
      "Please note:  neither this list nor its contents are final till\n",
      "midnight of the last day of the month of any such announcement.\n",
      "The official release date of all Project Gutenberg Etexts is at\n",
      "Midnight, Central Time, of the last day of the stated month.  A\n",
      "preliminary version may often be posted for suggestion, comment\n",
      "and editing by those who wish to do so.  To be sure you have an\n",
      "up to date first edition [xxxxx10x.xxx] please check file sizes\n",
      "in the first week of the next month.  Since our ftp program has\n",
      "a bug in it that scrambles the date [tried to fix and failed] a\n",
      "look at the file size will have to do, but we will try to see a\n",
      "new copy has at least one byte more or less.\n",
      "\n",
      "\n",
      "Information about Project Gutenberg (one page)\n",
      "\n",
      "We produce about two million dollars for each hour we work.  The\n",
      "fifty hours is one conservative estimate for how long it we take\n",
      "to get any etext selected, entered, proofread, edited, copyright\n",
      "searched and analyzed, the copyright letters written, etc.  This\n",
      "projected audience is one hundred million readers.  If our value\n",
      "per text is nominally estimated at one dollar then we produce $2\n",
      "million dollars per hour this year as we release thirty-two text\n",
      "files per month, or 384 more Etexts in 1997 for a total of 1000+\n",
      "If these reach just 10% of the computerized population, then the\n",
      "total should reach over 100 billion Etexts given away.\n",
      "\n",
      "The Goal of Project Gutenberg is to Give Away One Trillion Etext\n",
      "Files by the December 31, 2001.  [10,000 x 100,000,000=Trillion]\n",
      "This is ten thousand titles each to one hundred million readers,\n",
      "which is only 10% of the present number of computer users.  2001\n",
      "should have at least twice as many computer users as that, so it\n",
      "will require us reaching less than 5% of the users in 2001.\n",
      "\n",
      "\n",
      "We need your donations more than ever!\n",
      "\n",
      "\n",
      "All donations should be made to \"Project Gutenberg/CMU\": and are\n",
      "tax deductible to the extent allowable by law.  (CMU = Carnegie-\n",
      "Mellon University).\n",
      "\n",
      "For these and other matters, please mail to:\n",
      "\n",
      "Project Gutenberg\n",
      "P. O. Box  2782\n",
      "Champaign, IL 61825\n",
      "\n",
      "When all other email fails try our Executive Director:\n",
      "Michael S. Hart <hart@pobox.com>\n",
      "\n",
      "We would prefer to send you this information by email\n",
      "(Internet, Bitnet, Compuserve, ATTMAIL or MCImail).\n",
      "\n",
      "******\n",
      "If you have an FTP program (or emulator), please\n",
      "FTP directly to the Project Gutenberg archives:\n",
      "[Mac users, do NOT point and click. . .type]\n",
      "\n",
      "ftp uiarchive.cso.uiuc.edu\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (10, 100)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in corpus_raw.split()})))\n",
    "speeches = corpus_raw.split('\\n\\n')\n",
    "print('Number of speeches: {}'.format(len(speeches)))\n",
    "sentence_count_speech = [speeches.count('\\n') for speech in speeches]\n",
    "print('Average number of sentences in each speech: {}'.format(np.mean(sentence_count_speech)))\n",
    "\n",
    "sentences = [sentence for speech in speeches for sentence in speech.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(corpus_raw.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    vocabs = set(text)\n",
    "    int_to_vocab = dict(enumerate(vocabs, 1))\n",
    "    vocab_to_int = { v: k for k, v in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    return {\n",
    "        '.': '||period||',\n",
    "        ',': '||comma||',\n",
    "        '\"': '||quotation||',\n",
    "        ';': '||semi_colon||',\n",
    "        '!': '||exclamation||',\n",
    "        '?': '||question||',\n",
    "        '(': '||left_parentheses||',\n",
    "        ')': '||right_parentheses||',\n",
    "        '*': '||star||',\n",
    "        '--': '||dash||',\n",
    "        '\\n': '||return||'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import codecs\n",
    "\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "corp_file = open(os.path.join(os.getcwd(),\"saved_corp.txt\"), \"w\", encoding=\"utf-8\")\n",
    "\n",
    "corpus_raw = corpus_raw.encode('ascii', 'ignore')\n",
    "corpus_raw = corpus_raw.decode(\"utf-8\")\n",
    "\n",
    "corp_file.write(corpus_raw)\n",
    "corp_file.close\n",
    "\n",
    "corp_filename = os.path.join(os.getcwd(),\"saved_corp.txt\")\n",
    "\n",
    "helper.preprocess_and_save_data(corp_filename, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return (\n",
    "        tf.placeholder(tf.int32, shape=(None, None), name='input'),\n",
    "        tf.placeholder(tf.int32, shape=(None, None)),\n",
    "        tf.placeholder(tf.float32, name='keep_prob'),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([\n",
    "        tf.contrib.rnn.BasicLSTMCell(rnn_size),\n",
    "        tf.contrib.rnn.BasicLSTMCell(rnn_size)])\n",
    "\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    initial_state = tf.identity(initial_state, 'initial_state')\n",
    "\n",
    "    return cell, initial_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.embed_sequence(input_data, vocab_size, embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, 'final_state')\n",
    "\n",
    "    return outputs, final_state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    inputs = get_embed(input_data, vocab_size, rnn_size)\n",
    "    outputs, final_state = build_rnn(cell, inputs)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, None)\n",
    "\n",
    "    return logits, final_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: A Numpy array where each item is a tuple of (batch of input, batch of target).\n",
    "    \"\"\"\n",
    "    n_batches = int(len(int_text) / (batch_size * seq_length))\n",
    "\n",
    "    # Drop the last few characters to make only full batches\n",
    "    xdata = np.array(int_text[: n_batches * batch_size * seq_length])\n",
    "    ydata = np.array(int_text[1: n_batches * batch_size * seq_length+1])\n",
    "\n",
    "    x_batches = np.split(xdata.reshape(batch_size, -1), n_batches, 1)\n",
    "    y_batches = np.split(ydata.reshape(batch_size, -1), n_batches, 1)\n",
    "\n",
    "    return np.array(list(zip(x_batches, y_batches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 2\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "# RNN Size\n",
    "rnn_size = 256\n",
    "# Sequence Length\n",
    "seq_length = 9\n",
    "# Learning Rate\n",
    "learning_rate = 0.002\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 100\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)+1\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/8927   train_loss = 11.580\n",
      "Epoch   0 Batch  100/8927   train_loss = 6.572\n",
      "Epoch   0 Batch  200/8927   train_loss = 6.527\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return map(\n",
    "        loaded_graph.get_tensor_by_name,\n",
    "        ['input:0', 'initial_state:0', 'final_state:0', 'probs:0']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return int_to_vocab[probabilities.argmax()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_length = 20\n",
    "prime_word = 'are'\n",
    "\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word]\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        script = script.replace(' ' + token.lower(), key)\n",
    "    script = script.replace('\\n ', '\\n')\n",
    "    script = script.replace('( ', '(')\n",
    "        \n",
    "    print(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all samples, load into dictionary\n",
    "# Prepare a plain text corpus from which we train a languague model\n",
    "import glob\n",
    "import operator\n",
    "\n",
    "# Prepare a plain text corpus from which we train a languague model\n",
    "import glob\n",
    "\n",
    "# Gather all text files from directory\n",
    "LIBRISPEECH_DIRECTORY = os.path.join(os.getcwd(),'LibriSpeech/')\n",
    "TEDLIUM_DIRECTORY = os.path.join(os.getcwd(),'TEDLIUM/')\n",
    "\n",
    "# TRAINING_DIRECTORY = os.path.abspath(os.path.join(os.sep,'Volumes',\"My\\ Passport\\ for\\ Mac\",'lexicon','LibriSpeech'))\n",
    "dev_path = \"{}{}{}{}\".format(LIBRISPEECH_DIRECTORY, 'dev-clean/', '**/', '*.txt*')\n",
    "train_librispeech_path = \"{}{}{}{}{}\".format(LIBRISPEECH_DIRECTORY, 'books/', 'utf-8/', '**/', '*.txt*')\n",
    "TED_path = \"{}{}{}\".format(TEDLIUM_DIRECTORY, '**/', '*.txt*')\n",
    "\n",
    "text_paths = sorted(glob.glob(train_librispeech_path, recursive=True))\n",
    "text_paths.append(sorted(glob.glob(TED_path, recursive=True)))\n",
    "\n",
    "print('Found',len(text_paths),'text files in the directory:', train_path)\n",
    "\n",
    "transcripts = {}\n",
    "for document in text_paths:\n",
    "    with codecs.open(document, 'r', 'utf-8') as filep:\n",
    "        for i,line in enumerate(filep):\n",
    "            transcripts[line.split()[0]] = ' '.join(line.split()[1:])\n",
    "\n",
    "# Save Dictionary in Pickle File\n",
    "\n",
    "\n",
    "## Evaluate all samples found ##\n",
    "cloud_speech_api_accuracy = []\n",
    "custom_lang_model_accuracy = []\n",
    "epsilon = 0.000000001\n",
    "api_weight = 0.85\n",
    "steps = 0\n",
    "# Pull In Audio File\n",
    "for filename, gt_transcript in transcripts.items():\n",
    "    steps += 1\n",
    "    dirs = filename.split('-')\n",
    "    \n",
    "    audio_filepath = dev_file_name_0 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    dirs[0],\n",
    "    dirs[1],\n",
    "    \"{0}.flac\".format(filename))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Load the audio into memory\n",
    "    with io.open(audio_filepath, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "        audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code='en-US',\n",
    "        max_alternatives=10,\n",
    "        profanity_filter=False,\n",
    "        enable_word_time_offsets=True)\n",
    "\n",
    "    # Detects speech and words in the audio file\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "    result = operation.result(timeout=90)\n",
    "    alternatives = result.results[0].alternatives\n",
    "\n",
    "\n",
    "    # Evaluate API Results for Re-Ranking:\n",
    "    rerank_results = {}\n",
    "    for alternative in alternatives:\n",
    "        sent = alternative.transcript\n",
    "        \n",
    "        # Strip punctuation\n",
    "        translate_table = dict((ord(char), None) for char in string.punctuation)        \n",
    "        sent = sent.translate(translate_table) # remove punctuations\n",
    "\n",
    "        words = nltk.tokenize.word_tokenize(sent)\n",
    "        \n",
    "        \n",
    "        # Run Words through Neural Network\n",
    "        gen_length = len(sent)\n",
    "        num_chunks = 3\n",
    "        sent_sections = list(chunks(sent, num_chunks))\n",
    "        potential_scripts = []\n",
    "        alternatives_gen_acc = []\n",
    "        prime_word = ''\n",
    "        \n",
    "        for section in sent_sections[:-1]:\n",
    "            prime_word += section\n",
    "        \n",
    "            loaded_graph = tf.Graph()\n",
    "            with tf.Session(graph=loaded_graph) as sess:\n",
    "                # Load saved model\n",
    "                loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "                loader.restore(sess, load_dir)\n",
    "\n",
    "                # Get Tensors from loaded model\n",
    "                input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "                # Sentences generation setup\n",
    "                gen_sentences = [prime_word]\n",
    "                prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "                # Generate sentences\n",
    "                for n in range(gen_length):\n",
    "                    # Dynamic Input\n",
    "                    dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "                    dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "                    # Get Prediction\n",
    "                    probabilities, prev_state = sess.run(\n",
    "                        [probs, final_state],\n",
    "                        {input_text: dyn_input, initial_state: prev_state})\n",
    "\n",
    "                    pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "                    gen_sentences.append(pred_word)\n",
    "\n",
    "                # Remove tokens\n",
    "                gen_script = ' '.join(gen_sentences)\n",
    "                for key, token in token_dict.items():\n",
    "                    ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "                    gen_script = gen_script.replace(' ' + token.lower(), key)\n",
    "                gen_script = gen_script.replace('\\n ', '\\n')\n",
    "                gen_script = gen_script.replace('( ', '(')\n",
    "\n",
    "                print()\n",
    "                print()\n",
    "                print(gen_script)\n",
    "                \n",
    "                # potential_scripts.append(gen_script)\n",
    "                \n",
    "                # Find Edit distance between word and potential script that was generated\n",
    "                alt_ed = nltk.edit_distance(sent.lower(), gen_script.lower())\n",
    "                alt_upper_bound = max(len(sent),len(gen_script))\n",
    "                alt_accuracy = (1.0 - alt_ed/alt_upper_bound)\n",
    "                alternatives_gen_acc.append(alt_accuracy)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Re-rank alternatives using a weighted average of the two scores\n",
    "        prediction_score = np.mean(alternatives_gen_acc)\n",
    "        rerank_results[alternative.transcript] = confidence_score\n",
    "\n",
    "\n",
    "    \n",
    "    index, value = max(enumerate(list(rerank_results.values())), key=operator.itemgetter(1))\n",
    "    # Select Corresponding Transcript:\n",
    "    script=''\n",
    "    for trnscript, confidence in rerank_results.items():\n",
    "        if confidence == value:\n",
    "            script = trnscript\n",
    "            \n",
    "    \n",
    "    \n",
    "                \n",
    "    # Compute the Accuracy, based on the Levenshtein Distance (a.k.a. Edit Distance)\n",
    "    gcs_ed = nltk.edit_distance(alternative.transcript.lower(), gt_transcript.lower())\n",
    "    gcs_upper_bound = max(len(alternative.transcript),len(gt_transcript))\n",
    "    gcs_accuracy = (1.0 - gcs_ed/gcs_upper_bound)\n",
    "    \n",
    "    clm_ed = nltk.edit_distance(script.lower(), gt_transcript.lower())\n",
    "    clm_upper_bound = max(len(script),len(gt_transcript))\n",
    "    clm_accuracy = (1.0 - clm_ed/clm_upper_bound)\n",
    "    \n",
    "    cloud_speech_api_accuracy.append(gcs_accuracy)\n",
    "    custom_lang_model_accuracy.append(clm_accuracy)\n",
    "\n",
    "    if steps % 10 == 0:\n",
    "        print(\"{0} Transcripts Processed.\".format(steps))\n",
    "        print('Average API Accuracy:', np.mean(cloud_speech_api_accuracy))\n",
    "        print('Average Custom Model Accuracy:', np.mean(custom_lang_model_accuracy))\n",
    "        print()\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Google Cloud SDK: https://cloud.google.com/sdk/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CLOUDSDK_CORE_DISABLE_PROMPTS=1 ./google-cloud-sdk/install.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate with Google Cloud API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!. google-cloud-sdk/completion.bash.inc && \\\n",
    ". google-cloud-sdk/path.bash.inc && \\\n",
    "gcloud auth activate-service-account lexicon-bot@exemplary-oath-179301.iam.gserviceaccount.com --key-file=Lexicon-e94eff39fad7.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']='/home/paperspace/lexicon/Lexicon-e94eff39fad7.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test out Cloud Spech API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import speech\n",
    "from google.cloud.speech import enums\n",
    "from google.cloud.speech import types\n",
    "\n",
    "# Instantiates a client\n",
    "client = speech.SpeechClient()\n",
    "\n",
    "# The name of the dev-test audio file to transcribe\n",
    "dev_file_name_0 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0000.flac')\n",
    "gt0 = 'GO DO YOU HEAR'\n",
    "\n",
    "dev_file_name_1 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0001.flac')\n",
    "gt1 = 'BUT IN LESS THAN FIVE MINUTES THE STAIRCASE GROANED BENEATH AN EXTRAORDINARY WEIGHT'\n",
    "\n",
    "# The name of the test audio file to transcribe\n",
    "dev_file_name_2 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0002.flac')\n",
    "gt2 = 'AT THIS MOMENT THE WHOLE SOUL OF THE OLD MAN SEEMED CENTRED IN HIS EYES WHICH BECAME BLOODSHOT THE VEINS OF THE THROAT SWELLED HIS CHEEKS AND TEMPLES BECAME PURPLE AS THOUGH HE WAS STRUCK WITH EPILEPSY NOTHING WAS WANTING TO COMPLETE THIS BUT THE UTTERANCE OF A CRY'\n",
    "\n",
    "dev_file_name_3 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0003.flac')\n",
    "gt3 = 'AND THE CRY ISSUED FROM HIS PORES IF WE MAY THUS SPEAK A CRY FRIGHTFUL IN ITS SILENCE'\n",
    "\n",
    "dev_file_name_4 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0004.flac')\n",
    "gt4 = \"D'AVRIGNY RUSHED TOWARDS THE OLD MAN AND MADE HIM INHALE A POWERFUL RESTORATIVE\"\n",
    "\n",
    "\n",
    "test_file_name_1 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'RNN-Tutorial-master',\n",
    "    'data',\n",
    "    'raw',\n",
    "    'librivox',\n",
    "    'LibriSpeech',\n",
    "    'test-clean-wav',\n",
    "    '4507-16021-0019.wav')\n",
    "\n",
    "\n",
    "audio_files = {dev_file_name_0:gt0, dev_file_name_1:gt1, dev_file_name_2:gt2, dev_file_name_3:gt3, dev_file_name_4:gt4}\n",
    "\n",
    "\n",
    "# Loads the audio into memory\n",
    "with io.open(dev_file_name_2, 'rb') as audio_file:\n",
    "    content = audio_file.read()\n",
    "    audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "config = types.RecognitionConfig(\n",
    "    encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
    "    sample_rate_hertz=16000,\n",
    "    language_code='en-US',\n",
    "    max_alternatives=10,\n",
    "    profanity_filter=False,\n",
    "    enable_word_time_offsets=True)\n",
    "\n",
    "# Detects speech and words in the audio file\n",
    "operation = client.long_running_recognize(config, audio)\n",
    "\n",
    "print('Waiting for operation to complete...')\n",
    "result = operation.result(timeout=90)\n",
    "\n",
    "alternatives = result.results[0].alternatives\n",
    "for alternative in alternatives:\n",
    "    print('Transcript: {}'.format(alternative.transcript))\n",
    "    print('Confidence Score: {}'.format(alternative.confidence))\n",
    "\n",
    "    for word_info in alternative.words:\n",
    "        word = word_info.word\n",
    "        start_time = word_info.start_time\n",
    "        end_time = word_info.end_time\n",
    "        start = start_time.seconds + start_time.nanos * 1e-9\n",
    "        end = end_time.seconds + end_time.nanos * 1e-9\n",
    "        delta = end - start\n",
    "        \n",
    "        print('Word: {}, start_time (s): {}, end_time (s): {}, total_time (s): {}'.format(\n",
    "            word,\n",
    "            start,\n",
    "            end,\n",
    "            delta))\n",
    "        \n",
    "        #TODO: Do we need to figure out how to assign words to alternatives?\n",
    "            # If same amounts, assign words to index of parsed word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Preprocess Dataset - Download Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk #NLP Toolkit\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gensim Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a plain text corpus from which we train a languague model\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Gather all text files from directory\n",
    "LIBRISPEECH_DIRECTORY = os.path.join(os.getcwd(),'LibriSpeech/')\n",
    "TEDLIUM_DIRECTORY = os.path.join(os.getcwd(),'TEDLIUM_release1/')\n",
    "\n",
    "# TRAINING_DIRECTORY = os.path.abspath(os.path.join(os.sep,'Volumes',\"My\\ Passport\\ for\\ Mac\",'lexicon','LibriSpeech'))\n",
    "dev_path = \"{}{}{}{}\".format(LIBRISPEECH_DIRECTORY, 'dev-clean/', '**/', '*.txt*')\n",
    "train_librispeech_path = \"{}{}{}{}{}\".format(LIBRISPEECH_DIRECTORY, 'books/', 'utf-8/', '**/', '*.txt*')\n",
    "TED_path = \"{}{}{}{}\".format(TEDLIUM_DIRECTORY,'train/','**/', '*.stm')\n",
    "\n",
    "text_paths = sorted(glob.glob(train_librispeech_path, recursive=True))\n",
    "stm_paths = sorted(glob.glob(TED_path, recursive=True))\n",
    "\n",
    "print('Found',len(text_paths),\"text files in the directories {0} and {1} stm files in directory: {2}:\".format(train_librispeech_path, len(stm_paths),TED_path ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset - Tokenize Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import utils\n",
    "\n",
    "# reading the file in unicode format using codecs library    \n",
    "stoplist = set(stopwords.words('english'))\n",
    "# Strip punctuation\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation) \n",
    "        \n",
    "corpus_raw = u\"\"\n",
    "stm_segments = []\n",
    "for book_filename in text_paths:\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        lines = book_file.read()\n",
    "        corpus_raw += lines\n",
    "for stm_filename in stm_paths: # Process STM file (Tedlium)\n",
    "        stm_segments.append(utils.parse_stm_file(stm_filename))\n",
    "        for segments in stm_segments:\n",
    "            for segment in segments:\n",
    "                corpus_raw += segment.transcript\n",
    "        \n",
    "\n",
    "# Collect STM Segments\n",
    "\n",
    "               \n",
    "# Tokenize\n",
    "tokenized_words = nltk.tokenize.word_tokenize(corpus_raw)\n",
    "\n",
    "## Clean the tokens ##\n",
    "# Remove stop words\n",
    "tokenized_words = [word for word in tokenized_words if word not in stoplist]\n",
    "\n",
    "# Remove single-character tokens (mostly punctuation)\n",
    "tokenized_words = [word for word in tokenized_words if len(word) > 1]\n",
    "\n",
    "# Remove numbers\n",
    "tokenized_words = [word for word in tokenized_words if not word.isnumeric()]\n",
    "\n",
    "# Lowercase all words (default_stopwords are lowercase too)\n",
    "tokenized_words = [word.lower() for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset - Extract N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.probability import LidstoneProbDist\n",
    "from nltk.collocations import *\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "bigram_model = nltk.bigrams(tokenized_words)\n",
    "bigram_model = sorted(bigram_model, key=lambda item: item[1], reverse=True)  \n",
    "# print(bigram_model)\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "np.save(\"lang_model.npy\",bigram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(bigram_model)\n",
    "\n",
    "# Output top 50 words\n",
    "print(\"Word|Freq:\")\n",
    "for word, frequency in fdist.most_common(50):\n",
    "    print(u'{}|{}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfreq_2gram = nltk.ConditionalFreqDist(bigram_model)\n",
    "# print('Conditional Frequency Conditions:\\n', cfreq_2gram)\n",
    "print()\n",
    "\n",
    "# First access the FreqDist associated with \"one\", then the keys in that FreqDist\n",
    "print(\"Listing the words that can follow after 'greater':\\n\", cfreq_2gram[\"greater\"].keys())\n",
    "print()\n",
    "\n",
    "# Determine Most common in conditional frequency\n",
    "print(\"Listing 20 most frequent words to come after 'greater':\\n\", cfreq_2gram[\"greater\"].most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEMO - Evaluate Sentences Using Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each word in the evaluation list:\n",
    "# Select word and determine its frequency distribution\n",
    "# Grab probability of second word in the list\n",
    "# Continue this process until the sentence is scored\n",
    "\n",
    "# Add small epsilon value to avoid division by zero\n",
    "epsilon = 0.0000001\n",
    "\n",
    "# Loads the audio into memory\n",
    "for audio, ground_truth in audio_files.items():\n",
    "    with io.open(audio, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "        audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code='en-US',\n",
    "        max_alternatives=10,\n",
    "        profanity_filter=False,\n",
    "        enable_word_time_offsets=True)\n",
    "\n",
    "    # Detects speech and words in the audio file\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "\n",
    "    print('Waiting for operation to complete...')\n",
    "    result = operation.result(timeout=90)\n",
    "\n",
    "    alternatives = result.results[0].alternatives\n",
    "\n",
    "\n",
    "    print(\"API Results: \", alternatives)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    rerank_results = {}\n",
    "    for alternative in alternatives:\n",
    "        sent = alternative.transcript\n",
    "\n",
    "        words = nltk.tokenize.word_tokenize(sent)\n",
    "        probs = np.ones_like(words, dtype=np.float32)*epsilon\n",
    "        # print(words,'\\n',probs)\n",
    "        for word in words:\n",
    "            if words.index(word) < len(words)-1: \n",
    "                freq = cfreq_2gram[word].freq(words[words.index(word)+1])\n",
    "                probs[words.index(word)] = freq\n",
    "            # print(probs)\n",
    "\n",
    "        lexicon_score = np.sum(probs)\n",
    "        # print(word_score)\n",
    "\n",
    "        # Re-rank alternatives using a weighted average of the two scores\n",
    "        api_weight = 0.95\n",
    "        confidence_score = alternative.confidence*api_weight + lexicon_score*(1-api_weight)\n",
    "        rerank_results[alternative.transcript] = confidence_score\n",
    "\n",
    "    print(\"RE-RANKED Results: \\n\", rerank_results)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    import operator\n",
    "    index, value = max(enumerate(list(rerank_results.values())), key=operator.itemgetter(1))\n",
    "    # Select Corresponding Transcript:\n",
    "    script=''\n",
    "    for trnscript, confidence in rerank_results.items():\n",
    "        if confidence == value:\n",
    "            script = trnscript\n",
    "\n",
    "    # Evaluate the differences between the Original and the Reranked transcript:\n",
    "    print(\"ORIGINAL Transcript: \\n'{0}' \\nwith a confidence_score of: {1}\".format(alternative.transcript, alternative.confidence))\n",
    "    print(\"RE-RANKED Transcript: \\n'{0}' \\nwith a confidence_score of: {1}\".format(script, value))\n",
    "    print(\"GROUND TRUTH TRANSCRIPT: \\n{0}\".format(ground_truth))\n",
    "    print()\n",
    "    ranked_differences = list(set(nltk.tokenize.word_tokenize(alternative.transcript.lower())) -\n",
    "                              set(nltk.tokenize.word_tokenize(script.lower())))\n",
    "    if len(ranked_differences) == 0:  \n",
    "        print(\"No reranking was performed. The transcripts match!\")\n",
    "    else:\n",
    "        print(\"The original transcript was RE-RANKED. The transcripts do not match!\")\n",
    "        print(\"Differences between original and re-ranked: \", ranked_differences)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    # Evaluate Differences between the Original and Ground Truth:\n",
    "    gt_orig_diff = list(set(nltk.tokenize.word_tokenize(alternative.transcript.lower())) -\n",
    "                              set(nltk.tokenize.word_tokenize(ground_truth.lower())))\n",
    "    if len(gt_orig_diff) == 0:  \n",
    "        print(\"The ORIGINAL transcript matches ground truth!\")\n",
    "    else:\n",
    "        print(\"The original transcript DOES NOT MATCH ground truth.\")\n",
    "        print(\"Differences between original and ground truth: \", gt_orig_diff)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    gt_rr_diff = list(set(nltk.tokenize.word_tokenize(script.lower())) -\n",
    "                              set(nltk.tokenize.word_tokenize(ground_truth.lower())))\n",
    "    if len(gt_rr_diff) == 0:  \n",
    "        print(\"The RE-RANKED transcript matches ground truth!\")\n",
    "    else:\n",
    "        print(\"The RE_RANKED transcript DOES NOT MATCH ground truth.\")\n",
    "        print(\"Differences between Reranked and ground truth: \", gt_rr_diff)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # Compute the Levenshtein Distance (a.k.a. Edit Distance)\n",
    "#     import nltk.metrics.distance as lev_dist\n",
    "    \n",
    "    # Google API Edit Distance\n",
    "    goog_edit_distance = nltk.edit_distance(alternative.transcript.lower(), ground_truth.lower())\n",
    "    \n",
    "    # Re-Ranked Edit Distance\n",
    "    rr_edit_distance = nltk.edit_distance(script.lower(), ground_truth.lower())\n",
    "\n",
    "    \n",
    "    print(\"ORIGINAL Edit Distance: \\n{0}\".format(goog_edit_distance))\n",
    "    print(\"RE-RANKED Edit Distance: \\n{0}\".format(rr_edit_distance))\n",
    "    print()\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Gather all samples, load into dictionary\n",
    "# Prepare a plain text corpus from which we train a languague model\n",
    "import glob\n",
    "import operator\n",
    "\n",
    "# Gather all text files from directory\n",
    "WORKING_DIRECTORY = os.path.join(os.getcwd(),'LibriSpeech/')\n",
    "\n",
    "# TRAINING_DIRECTORY = os.path.abspath(os.path.join(os.sep,'Volumes',\"My\\ Passport\\ for\\ Mac\",'lexicon','LibriSpeech'))\n",
    "dev_path = \"{}{}{}{}\".format(WORKING_DIRECTORY, 'dev-clean/', '**/', '*.txt')\n",
    "train_path = \"{}{}{}{}{}\".format(WORKING_DIRECTORY, 'books/', 'utf-8/', '**/', '*.txt*')\n",
    "\n",
    "text_paths = sorted(glob.glob(dev_path, recursive=True))\n",
    "print('Found',len(text_paths),'text files in the directory:', dev_path)\n",
    "\n",
    "transcripts = {}\n",
    "for document in text_paths:\n",
    "    with codecs.open(document, 'r', 'utf-8') as filep:\n",
    "        for i,line in enumerate(filep):\n",
    "            transcripts[line.split()[0]] = ' '.join(line.split()[1:])\n",
    "\n",
    "# Save Dictionary in Pickle File\n",
    "\n",
    "\n",
    "## Evaluate all samples found ##\n",
    "cloud_speech_api_accuracy = []\n",
    "custom_lang_model_accuracy = []\n",
    "epsilon = 0.000000001\n",
    "api_weight = 0.85\n",
    "steps = 0\n",
    "# Pull In Audio File\n",
    "for filename, gt_transcript in transcripts.items():\n",
    "    steps += 1\n",
    "    dirs = filename.split('-')\n",
    "    \n",
    "    audio_filepath = dev_file_name_0 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    dirs[0],\n",
    "    dirs[1],\n",
    "    \"{0}.flac\".format(filename))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Load the audio into memory\n",
    "    with io.open(audio_filepath, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "        audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code='en-US',\n",
    "        max_alternatives=10,\n",
    "        profanity_filter=False,\n",
    "        enable_word_time_offsets=True)\n",
    "\n",
    "    # Detects speech and words in the audio file\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "    result = operation.result(timeout=90)\n",
    "    alternatives = result.results[0].alternatives\n",
    "\n",
    "\n",
    "    # Evaluate API Results for Re-Ranking:\n",
    "    rerank_results = {}\n",
    "    for alternative in alternatives:\n",
    "        sent = alternative.transcript\n",
    "        \n",
    "        # Strip punctuation\n",
    "        translate_table = dict((ord(char), None) for char in string.punctuation)        \n",
    "        sent = sent.translate(translate_table) # remove punctuations\n",
    "\n",
    "        words = nltk.tokenize.word_tokenize(sent)\n",
    "        probs = np.ones_like(words, dtype=np.float32)*epsilon\n",
    "\n",
    "        for word in words:\n",
    "            if words.index(word) < len(words)-1: \n",
    "                freq = cfreq_2gram[word].freq(words[words.index(word)+1])\n",
    "                probs[words.index(word)] = freq\n",
    "\n",
    "        lexicon_score = np.sum(probs)\n",
    "\n",
    "        # Re-rank alternatives using a weighted average of the two scores\n",
    "        confidence_score = alternative.confidence*api_weight + lexicon_score*(1-api_weight)\n",
    "        rerank_results[alternative.transcript] = confidence_score\n",
    "\n",
    "\n",
    "    \n",
    "    index, value = max(enumerate(list(rerank_results.values())), key=operator.itemgetter(1))\n",
    "    # Select Corresponding Transcript:\n",
    "    script=''\n",
    "    for trnscript, confidence in rerank_results.items():\n",
    "        if confidence == value:\n",
    "            script = trnscript\n",
    "                \n",
    "    # Compute the Accuracy, based on the Levenshtein Distance (a.k.a. Edit Distance)\n",
    "    gcs_ed = nltk.edit_distance(alternative.transcript.lower(), gt_transcript.lower())\n",
    "    gcs_upper_bound = max(len(alternative.transcript),len(gt_transcript))\n",
    "    gcs_accuracy = (1.0 - gcs_ed/gcs_upper_bound)\n",
    "    \n",
    "    clm_ed = nltk.edit_distance(script.lower(), gt_transcript.lower())\n",
    "    clm_upper_bound = max(len(script),len(gt_transcript))\n",
    "    clm_accuracy = (1.0 - clm_ed/clm_upper_bound)\n",
    "    \n",
    "    cloud_speech_api_accuracy.append(gcs_accuracy)\n",
    "    custom_lang_model_accuracy.append(clm_accuracy)\n",
    "\n",
    "    if steps % 10 == 0:\n",
    "        print(\"{0} Transcripts Processed.\".format(steps))\n",
    "        print('Average API Accuracy:', np.mean(cloud_speech_api_accuracy))\n",
    "        print('Average Custom Model Accuracy:', np.mean(custom_lang_model_accuracy))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
