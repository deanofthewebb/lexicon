{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon - Orchestrator\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "For this project, I will build a simple custom ochestrator that processes data objects from the \"Lexicon\" class.\n",
    "    - These objects are custom datasets that are modeled after the Ted Talk speakers. \n",
    "    - Each Lexicon has a corpus and some helper methods aimed at training and prediction\n",
    "    - Lexicon class will also have a preprocessing and caching function.\n",
    "    - Each object will have two methods of prediction, n-gram language model and a recurrent neural network model\n",
    "    - Each object has a custom reporting function that reports the results of training\n",
    "    - Each object will be able to learn from any text data provided, and return a transcript with confidence values from input posed in speech utterances. \n",
    "        - I will use Google's cloud-based services to preprocess the input audio data and transcribe into an initial guess. Then I will train a model to improve on Google cloud speech API's response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf; print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Use to reload modules\n",
    "from importlib import reload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']=os.path.join(os.getcwd(),'Lexicon-e94eff39fad7.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "\n",
    "librispeech_dataset_folder_path = 'LibriSpeech'\n",
    "tedlium_dataset_folder_path = 'TEDLIUM_release1'\n",
    "tar_gz_path = 'dev-clean.tar.gz'\n",
    "ted_gz_path = 'TEDLIUM_release1.tar.gz'\n",
    "\n",
    "books_path = 'original-books.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(books_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Librispeech Book Texts') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://www.openslr.org/resources/12/original-books.tar.gz',\n",
    "            books_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(librispeech_dataset_folder_path+'/books'):\n",
    "    with tarfile.open(books_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Librispeech dev-clean.tar.gz') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://www.openslr.org/resources/12/dev-clean.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(librispeech_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "if not isfile(ted_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Tedlium TEDLIUM_release1.tar.gz') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://www.openslr.org/resources/7/TEDLIUM_release1.tar.gz',\n",
    "            ted_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(tedlium_dataset_folder_path):\n",
    "    with tarfile.open(ted_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import speech\n",
    "from google.cloud.speech import enums\n",
    "from google.cloud.speech import types\n",
    "\n",
    "# Instantiates a client\n",
    "client = speech.SpeechClient()\n",
    "\n",
    "# The name of the dev-test audio file to transcribe\n",
    "dev_file_name_0 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0000.flac')\n",
    "gt0 = 'GO DO YOU HEAR'\n",
    "\n",
    "dev_file_name_1 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0001.flac')\n",
    "gt1 = 'BUT IN LESS THAN FIVE MINUTES THE STAIRCASE GROANED BENEATH AN EXTRAORDINARY WEIGHT'\n",
    "\n",
    "# The name of the test audio file to transcribe\n",
    "dev_file_name_2 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0002.flac')\n",
    "gt2 = 'AT THIS MOMENT THE WHOLE SOUL OF THE OLD MAN SEEMED CENTRED IN HIS EYES WHICH BECAME BLOODSHOT THE VEINS OF THE THROAT SWELLED HIS CHEEKS AND TEMPLES BECAME PURPLE AS THOUGH HE WAS STRUCK WITH EPILEPSY NOTHING WAS WANTING TO COMPLETE THIS BUT THE UTTERANCE OF A CRY'\n",
    "\n",
    "dev_file_name_3 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0003.flac')\n",
    "gt3 = 'AND THE CRY ISSUED FROM HIS PORES IF WE MAY THUS SPEAK A CRY FRIGHTFUL IN ITS SILENCE'\n",
    "\n",
    "dev_file_name_4 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0004.flac')\n",
    "gt4 = \"D'AVRIGNY RUSHED TOWARDS THE OLD MAN AND MADE HIM INHALE A POWERFUL RESTORATIVE\"\n",
    "\n",
    "\n",
    "test_file_name_1 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'RNN-Tutorial-master',\n",
    "    'data',\n",
    "    'raw',\n",
    "    'librivox',\n",
    "    'LibriSpeech',\n",
    "    'test-clean-wav',\n",
    "    '4507-16021-0019.wav')\n",
    "\n",
    "\n",
    "audio_files = {dev_file_name_0:gt0, dev_file_name_1:gt1, dev_file_name_2:gt2, dev_file_name_3:gt3, dev_file_name_4:gt4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 41 text files in the directories /Volumes/My Passport for Mac/lexicon/LibriSpeech/books/utf-8/**/*.txt*\n",
      "97 segmented text files in the /Volumes/My Passport for Mac/lexicon/LibriSpeech/dev-clean/**/*.txt* directory and \n",
      "774 stm files in directory: /Volumes/My Passport for Mac/lexicon/TEDLIUM_release1/train/**/*.stm:\n"
     ]
    }
   ],
   "source": [
    "# Prepare a plain text corpus from which we train a languague model\n",
    "import glob\n",
    "import os\n",
    "import utils\n",
    "import nltk\n",
    "\n",
    "# Gather all text files from directory\n",
    "LIBRISPEECH_DIRECTORY = os.path.join(os.getcwd(),'LibriSpeech/')\n",
    "TEDLIUM_DIRECTORY = os.path.join(os.getcwd(),'TEDLIUM_release1/')\n",
    "\n",
    "# TRAINING_DIRECTORY = os.path.abspath(os.path.join(os.sep,'Volumes',\"My\\ Passport\\ for\\ Mac\",'lexicon','LibriSpeech'))\n",
    "dev_librispeech_path = \"{}{}{}{}\".format(LIBRISPEECH_DIRECTORY, 'dev-clean/', '**/', '*.txt*')\n",
    "train_librispeech_path = \"{}{}{}{}{}\".format(LIBRISPEECH_DIRECTORY, 'books/', 'utf-8/', '**/', '*.txt*')\n",
    "TED_path = \"{}{}{}{}\".format(TEDLIUM_DIRECTORY,'train/','**/', '*.stm')\n",
    "\n",
    "text_paths = sorted(glob.glob(train_librispeech_path, recursive=True))\n",
    "segmented_text_paths = sorted(glob.glob(dev_librispeech_path, recursive=True))\n",
    "stm_paths = sorted(glob.glob(TED_path, recursive=True))\n",
    "\n",
    "print('Found:',len(text_paths),\"text files in the directories {0}\\n{1} segmented text files in the {2} directory and \\n{3} stm files in directory: {4}:\".format(train_librispeech_path, \n",
    "        len(segmented_text_paths), dev_librispeech_path, len(stm_paths),TED_path ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Text Corpuses for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "from lexicon import Lexicon\n",
    "from speech import Speech\n",
    "      \n",
    "librispeech_corpus = u\"\"\n",
    "stm_segments = []\n",
    "lexicons = {} # {speaker_id: lexicon_object}\n",
    "speeches = {} # {speech_id: speech_object}\n",
    "segmented_librispeeches = {}\n",
    "\n",
    "for book_filename in text_paths[:15]: # 1 Book\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        lines = book_file.read()\n",
    "        librispeech_corpus += lines\n",
    "for stm_filename in stm_paths: # Process STM files (Tedlium)\n",
    "        stm_segments.append(utils.parse_stm_file(stm_filename))\n",
    "        \n",
    "\n",
    "# Train on 3 speakers\n",
    "for segments in stm_segments[15:18]: \n",
    "    for segment in segments:\n",
    "        segment_key = \"{0}_{1}_{2}\".format(segment.speaker_id.strip(), str(segment.start_time).replace('.','_'),\n",
    "                                          str(segment.stop_time).replace('.','_'))\n",
    "        if segment.speaker_id not in speeches.keys():\n",
    "            source_file = os.path.join(os.getcwd(), 'TEDLIUM_release1',\n",
    "                                       'train','sph', '{}.sph'.format(segment.filename))\n",
    "            speech = Speech(speaker_id=segment.speaker_id,\n",
    "                                           speech_id = segment_key,\n",
    "                                           source_file=source_file,\n",
    "                                           ground_truth = ' '.join(segment.transcript.split()[1:]),\n",
    "                                           start = segment.start_time,\n",
    "                                           stop = segment.stop_time,\n",
    "                                           audio_type = 'LINEAR16')\n",
    "        else:\n",
    "            speech = speeches[segment.speaker_id.strip()]\n",
    "            print('Already found speech in list at location: ', speech)\n",
    "        \n",
    "        speeches[segment_key] = speech\n",
    "\n",
    "        if segment.speaker_id not in lexicons.keys():\n",
    "            lexicon = Lexicon(base_corpus=librispeech_corpus, name=segment.speaker_id)\n",
    "            lexicons[segment.speaker_id.strip()] = lexicon\n",
    "        else:\n",
    "            lexicon = lexicons[segment.speaker_id.strip()]\n",
    "        \n",
    "        # Add Speech to Lexicon\n",
    "        if speech not in lexicon.speeches:\n",
    "            lexicon.add_speech(speech)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GCS Transcripts using GCS Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 103396\n",
      "Number of sentences: 49048\n",
      "Average number of words in a sentence: 25.53141820257707\n",
      "\n",
      "Transcript sentences 0 to 10:\n",
      "                        *the bargain\n",
      "Ye be submitted through your free assent\n",
      "To stand in this case at my judgement\n",
      " It was fifty feet long and\n",
      "half as wide\n",
      " For, as\n",
      "witnesseth Saint Matthew, chap\n",
      "  When I had led him for some time two Japanese with a string\n",
      "of pack-horses loaded with deer-hides met me, and not only put the\n",
      "saddle on again, but held the stirrup while I remounted, and bowed\n",
      "politely when I went away\n",
      "\n",
      "\n",
      "These reflections bring to mind a discussion on this point, between the\n",
      "writer and a slaveholding friend in Kentucky, on Christmas morning,\n",
      "1846\n",
      "  I have never yet heard a baby\n",
      "cry, and I have never seen a child troublesome or disobedient\n",
      "\n",
      "iii\n",
      " And\n",
      "of the hinder part of their buttocks it is full horrible to see, for\n",
      "certes, in that part of their body where they purge their stinking\n",
      "ordure, that foul part shew they to the people proudly in despite\n",
      "of honesty [decency], which honesty Jesus Christ and his friends\n",
      "observed to shew in his life\n",
      " Blife: quickly, eagerly; for \"blive\" or \"belive\n",
      "\n",
      "\n",
      "25\n",
      "\n",
      "Ground Truth sentences 0 to 10:\n",
      "                        *the bargain\n",
      "Ye be submitted through your free assent\n",
      "To stand in this case at my judgement\n",
      " It was fifty feet long and\n",
      "half as wide\n",
      " For, as\n",
      "witnesseth Saint Matthew, chap\n",
      "  When I had led him for some time two Japanese with a string\n",
      "of pack-horses loaded with deer-hides met me, and not only put the\n",
      "saddle on again, but held the stirrup while I remounted, and bowed\n",
      "politely when I went away\n",
      "\n",
      "\n",
      "These reflections bring to mind a discussion on this point, between the\n",
      "writer and a slaveholding friend in Kentucky, on Christmas morning,\n",
      "1846\n",
      "  I have never yet heard a baby\n",
      "cry, and I have never seen a child troublesome or disobedient\n",
      "\n",
      "iii\n",
      " And\n",
      "of the hinder part of their buttocks it is full horrible to see, for\n",
      "certes, in that part of their body where they purge their stinking\n",
      "ordure, that foul part shew they to the people proudly in despite\n",
      "of honesty [decency], which honesty Jesus Christ and his friends\n",
      "observed to shew in his life\n",
      " Blife: quickly, eagerly; for \"blive\" or \"belive\n",
      "\n",
      "\n",
      "25\n",
      "\n",
      "Dataset Stats\n",
      "Roughly the number of unique words: 103278\n",
      "Number of sentences: 48981\n",
      "Average number of words in a sentence: 25.539311161470774\n",
      "\n",
      "Transcript sentences 0 to 10:\n",
      "\"\n",
      "\"Ah,\" thought this friar, \"that shall go with me\n",
      "\n",
      "\n",
      "Implicit and prompt obedience is required from infancy; and from a\n",
      "very early age the children are utilised by being made to fetch and\n",
      "carry and go on messages\n",
      " Pickering passed me, so near\n",
      "that I might have put out my hand and touched him,\n",
      "and in a moment I heard the carriage drive off rapidly\n",
      "toward the village\n",
      "\"  Carlton turned to another column, and read the\n",
      "following:\n",
      "\n",
      "\"Some advantages of a peculiar character are connected with this\n",
      "institution, which it may be proper to point out\n",
      "\n",
      "And when that drunken was all in the crock*                 *pitcher<18>\n",
      "To bedde went the daughter right anon,\n",
      "To bedde went Alein, and also John\n",
      "\n",
      "\n",
      "\n",
      "Of her first master, she can give no account, as she must have been a\n",
      "mere infant when he died; and she, with her parents and some ten or\n",
      "twelve other fellow human chattels, became the legal property of his\n",
      "son, Charles Ardinburgh\n",
      " This may either\n",
      "loosen the corpse from the soft mud or ooze in which it is imbedded,\n",
      "thus permitting it to rise when other agencies have already prepared\n",
      "it for so doing; or it may overcome the tenacity of some putrescent\n",
      "portions of the cellular tissue; allowing the cavities to distend under\n",
      "the influence of the gas\n",
      " 1\n",
      "\n",
      "Colonel Torrens himself wrote much of the political economy of his\n",
      "paper; and had at this time made an attack upon some opinion of Ricardo\n",
      "and my father, to which, at my father's instigation, I attempted an\n",
      "answer, and Coulson, out of consideration for my father and goodwill to\n",
      "me, inserted it\n",
      " 65-88\n",
      "\n",
      "Ground Truth sentences 0 to 10:\n",
      "\"\n",
      "\"Ah,\" thought this friar, \"that shall go with me\n",
      "\n",
      "\n",
      "Implicit and prompt obedience is required from infancy; and from a\n",
      "very early age the children are utilised by being made to fetch and\n",
      "carry and go on messages\n",
      " Pickering passed me, so near\n",
      "that I might have put out my hand and touched him,\n",
      "and in a moment I heard the carriage drive off rapidly\n",
      "toward the village\n",
      "\"  Carlton turned to another column, and read the\n",
      "following:\n",
      "\n",
      "\"Some advantages of a peculiar character are connected with this\n",
      "institution, which it may be proper to point out\n",
      "\n",
      "And when that drunken was all in the crock*                 *pitcher<18>\n",
      "To bedde went the daughter right anon,\n",
      "To bedde went Alein, and also John\n",
      "\n",
      "\n",
      "\n",
      "Of her first master, she can give no account, as she must have been a\n",
      "mere infant when he died; and she, with her parents and some ten or\n",
      "twelve other fellow human chattels, became the legal property of his\n",
      "son, Charles Ardinburgh\n",
      " This may either\n",
      "loosen the corpse from the soft mud or ooze in which it is imbedded,\n",
      "thus permitting it to rise when other agencies have already prepared\n",
      "it for so doing; or it may overcome the tenacity of some putrescent\n",
      "portions of the cellular tissue; allowing the cavities to distend under\n",
      "the influence of the gas\n",
      " 1\n",
      "\n",
      "Colonel Torrens himself wrote much of the political economy of his\n",
      "paper; and had at this time made an attack upon some opinion of Ricardo\n",
      "and my father, to which, at my father's instigation, I attempted an\n",
      "answer, and Coulson, out of consideration for my father and goodwill to\n",
      "me, inserted it\n",
      " 65-88\n",
      "\n",
      "Dataset Stats\n",
      "Roughly the number of unique words: 103295\n",
      "Number of sentences: 48988\n",
      "Average number of words in a sentence: 25.537662284641137\n",
      "\n",
      "Transcript sentences 0 to 10:\n",
      "\n",
      "\n",
      "“It was a fine storm; I got a great day out of it,” I\n",
      "said\n",
      "                       *reward\n",
      "Madame, rue upon my paine's smart,\n",
      "For with a word ye may me slay or save\n",
      "  The second generation of city poor too often have\n",
      "no holiday clothes and consider their relations a \"bad lot\n",
      "\n",
      "\n",
      "Balls being no part of Sojourner's mission, she was not desirous\n",
      "of attending; but her hostess could be satisfied with nothing\n",
      "short of a taste of it, and she was forced to go with her, or\n",
      "relinquish their company at once, in which move there might be\n",
      "more exposure than in accompanying her\n",
      "--It will throw light on his sane character to give a literal copy\n",
      "of the note:\n",
      "\n",
      "                                  \"FORDHAM, April 20, 1849\n",
      "\n",
      "\n",
      "\"My DEAR WILLIS--The poem which I inclose, and which I am so vain as to\n",
      "hope you will like, in some respects, has been just published in a paper\n",
      "for which sheer necessity compels me to write, now and then\n",
      "\n",
      "As I lounged across the platform with Stoddard, Pickering\n",
      "came out into the vestibule of his car, followed by\n",
      "two ladies and an elderly gentleman\n",
      " Moreover, this knot is one\n",
      "which few besides sailors can tie, and is peculiar to the Maltese\n",
      " Jones?\"\n",
      "\"No, no,\" returned he, \"I think that's all nonsense; my Negroes\n",
      "do their own preaching\n",
      " federal laws and your state's laws\n",
      "S\n",
      "\n",
      "Ground Truth sentences 0 to 10:\n",
      "\n",
      "\n",
      "“It was a fine storm; I got a great day out of it,” I\n",
      "said\n",
      "                       *reward\n",
      "Madame, rue upon my paine's smart,\n",
      "For with a word ye may me slay or save\n",
      "  The second generation of city poor too often have\n",
      "no holiday clothes and consider their relations a \"bad lot\n",
      "\n",
      "\n",
      "Balls being no part of Sojourner's mission, she was not desirous\n",
      "of attending; but her hostess could be satisfied with nothing\n",
      "short of a taste of it, and she was forced to go with her, or\n",
      "relinquish their company at once, in which move there might be\n",
      "more exposure than in accompanying her\n",
      "--It will throw light on his sane character to give a literal copy\n",
      "of the note:\n",
      "\n",
      "                                  \"FORDHAM, April 20, 1849\n",
      "\n",
      "\n",
      "\"My DEAR WILLIS--The poem which I inclose, and which I am so vain as to\n",
      "hope you will like, in some respects, has been just published in a paper\n",
      "for which sheer necessity compels me to write, now and then\n",
      "\n",
      "As I lounged across the platform with Stoddard, Pickering\n",
      "came out into the vestibule of his car, followed by\n",
      "two ladies and an elderly gentleman\n",
      " Moreover, this knot is one\n",
      "which few besides sailors can tie, and is peculiar to the Maltese\n",
      " Jones?\"\n",
      "\"No, no,\" returned he, \"I think that's all nonsense; my Negroes\n",
      "do their own preaching\n",
      " federal laws and your state's laws\n",
      "S\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "view_sentence_range = (0, 10)\n",
    "\n",
    "for speaker_id, lexicon in lexicons.items():\n",
    "    print('Dataset Stats')\n",
    "    print('Roughly the number of unique words: {}'.format(lexicon.vocab_size))\n",
    "    \n",
    "    word_counts = [len(sentence.split()) for sentence in lexicon.corpus_sentences]\n",
    "    print('Number of sentences: {}'.format(len(lexicon.corpus_sentences)))\n",
    "    print('Average number of words in a sentence: {}'.format(np.average(word_counts)))\n",
    "\n",
    "    print()\n",
    "    print('Transcript sentences {} to {}:'.format(*view_sentence_range))\n",
    "    print('\\n'.join(lexicon.training_set[0][view_sentence_range[0]:view_sentence_range[1]]))\n",
    "    print()\n",
    "    print('Ground Truth sentences {} to {}:'.format(*view_sentence_range))\n",
    "    print('\\n'.join(lexicon.training_set[1][view_sentence_range[0]:view_sentence_range[1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset - Tokenize Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "# reading the file in unicode format using codecs library    \n",
    "stoplist = set(stopwords.words('english'))\n",
    "# Strip punctuation\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation) \n",
    "        \n",
    "corpus_raw = u\"\"\n",
    "for book_filename in text_paths:\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        lines = book_file.read()\n",
    "        corpus_raw += lines.translate(translate_table) # remove punctuations \n",
    "\n",
    "               \n",
    "# Tokenize\n",
    "tokenized_words = nltk.tokenize.word_tokenize(corpus_raw)\n",
    "\n",
    "## Clean the tokens ##\n",
    "# Remove stop words\n",
    "tokenized_words = [word for word in tokenized_words if word not in stoplist]\n",
    "\n",
    "# Remove single-character tokens (mostly punctuation)\n",
    "tokenized_words = [word for word in tokenized_words if len(word) > 1]\n",
    "\n",
    "# Remove numbers\n",
    "tokenized_words = [word for word in tokenized_words if not word.isnumeric()]\n",
    "\n",
    "# Lowercase all words (default_stopwords are lowercase too)\n",
    "tokenized_words = [word.lower() for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset - Extract N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.collocations import *\n",
    "from nltk.probability import FreqDist\n",
    "import nltk\n",
    "\n",
    "# extracting the bi-grams and sorting them according to their frequencies\n",
    "finder = BigramCollocationFinder.from_words(tokenized_words)\n",
    "# finder.apply_freq_filter(3)\n",
    "\n",
    "bigram_model = nltk.bigrams(tokenized_words)\n",
    "bigram_model = sorted(bigram_model, key=lambda item: item[1], reverse=True)  \n",
    "# print(bigram_model)\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "np.save(\"lang_model.npy\",bigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(bigram_model)\n",
    "\n",
    "# Output top 50 words\n",
    "print(\"Word|Freq:\")\n",
    "for word, frequency in fdist.most_common(50):\n",
    "    print(u'{}|{}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfreq_2gram = nltk.ConditionalFreqDist(bigram_model)\n",
    "# print('Conditional Frequency Conditions:\\n', cfreq_2gram)\n",
    "print()\n",
    "\n",
    "# First access the FreqDist associated with \"one\", then the keys in that FreqDist\n",
    "print(\"Listing the words that can follow after 'greater':\\n\", cfreq_2gram[\"greater\"].keys())\n",
    "print()\n",
    "\n",
    "# Determine Most common in conditional frequency\n",
    "print(\"Listing 20 most frequent words to come after 'greater':\\n\", cfreq_2gram[\"greater\"].most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each word in the evaluation list:\n",
    "# Select word and determine its frequency distribution\n",
    "# Grab probability of second word in the list\n",
    "# Continue this process until the sentence is scored\n",
    "\n",
    "# Add small epsilon value to avoid division by zero\n",
    "epsilon = 0.0000001\n",
    "\n",
    "# Loads the audio into memory\n",
    "for audio, ground_truth in audio_files.items():\n",
    "    with io.open(audio, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "        audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code='en-US',\n",
    "        max_alternatives=10,\n",
    "        profanity_filter=False,\n",
    "        enable_word_time_offsets=True)\n",
    "\n",
    "    # Detects speech and words in the audio file\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "\n",
    "    print('Waiting for operation to complete...')\n",
    "    result = operation.result(timeout=90)\n",
    "\n",
    "    alternatives = result.results[0].alternatives\n",
    "\n",
    "\n",
    "    #print(\"API Results: \", alternatives)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    rerank_results = {}\n",
    "    for alternative in alternatives:\n",
    "        sent = alternative.transcript\n",
    "\n",
    "        words = nltk.tokenize.word_tokenize(sent)\n",
    "        probs = np.ones_like(words, dtype=np.float32)*epsilon\n",
    "        # print(words,'\\n',probs)\n",
    "        for word in words:\n",
    "            if words.index(word) < len(words)-1: \n",
    "                freq = cfreq_2gram[word].freq(words[words.index(word)+1])\n",
    "                probs[words.index(word)] = freq\n",
    "            # print(probs)\n",
    "\n",
    "        lexicon_score = np.sum(probs)\n",
    "        # print(word_score)\n",
    "\n",
    "        # Re-rank alternatives using a weighted average of the two scores\n",
    "        api_weight = 0.90\n",
    "        confidence_score = alternative.confidence*api_weight + lexicon_score*(1-api_weight)\n",
    "        rerank_results[alternative.transcript] = confidence_score\n",
    "\n",
    "    print(\"RE-RANKED Results: \\n\", rerank_results)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    import operator\n",
    "    index, value = max(enumerate(list(rerank_results.values())), key=operator.itemgetter(1))\n",
    "    # Select Corresponding Transcript:\n",
    "    script=''\n",
    "    for trnscript, confidence in rerank_results.items():\n",
    "        if confidence == value:\n",
    "            script = trnscript\n",
    "\n",
    "    # Evaluate the differences between the Original and the Reranked transcript:\n",
    "    print(\"ORIGINAL Transcript: \\n'{0}' \\nwith a confidence_score of: {1}\".format(alternative.transcript, alternative.confidence))\n",
    "    \n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(\"RE-RANKED Transcript: \\n'{0}' \\nwith a confidence_score of: {1}\".format(script, value))\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(\"GROUND TRUTH TRANSCRIPT: \\n{0}\".format(ground_truth))\n",
    "    print()\n",
    "    ranked_differences = list(set(nltk.tokenize.word_tokenize(alternative.transcript.lower())) -\n",
    "                              set(nltk.tokenize.word_tokenize(script.lower())))\n",
    "    if len(ranked_differences) == 0:  \n",
    "        print(\"No reranking was performed. The transcripts match!\")\n",
    "    else:\n",
    "        print(\"The original transcript was RE-RANKED. The transcripts do not match!\")\n",
    "        print(\"Differences between original and re-ranked: \", ranked_differences)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    # Evaluate Differences between the Original and Ground Truth:\n",
    "    gt_orig_diff = list(set(nltk.tokenize.word_tokenize(alternative.transcript.lower())) -\n",
    "                              set(nltk.tokenize.word_tokenize(ground_truth.lower())))\n",
    "    if len(gt_orig_diff) == 0:  \n",
    "        print(\"The ORIGINAL transcript matches ground truth!\")\n",
    "    else:\n",
    "        print(\"The original transcript DOES NOT MATCH ground truth.\")\n",
    "        print(\"Differences between original and ground truth: \", gt_orig_diff)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    gt_rr_diff = list(set(nltk.tokenize.word_tokenize(script.lower())) -\n",
    "                              set(nltk.tokenize.word_tokenize(ground_truth.lower())))\n",
    "    if len(gt_rr_diff) == 0:  \n",
    "        print(\"The RE-RANKED transcript matches ground truth!\")\n",
    "    else:\n",
    "        print(\"The RE_RANKED transcript DOES NOT MATCH ground truth.\")\n",
    "        print(\"Differences between Reranked and ground truth: \", gt_rr_diff)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # Compute the Levenshtein Distance (a.k.a. Edit Distance)\n",
    "#     import nltk.metrics.distance as lev_dist\n",
    "    \n",
    "    # Google API Edit Distance\n",
    "    goog_edit_distance = nltk.edit_distance(alternative.transcript.lower(), ground_truth.lower())\n",
    "    \n",
    "    # Re-Ranked Edit Distance\n",
    "    rr_edit_distance = nltk.edit_distance(script.lower(), ground_truth.lower())\n",
    "\n",
    "    \n",
    "    print(\"ORIGINAL Edit Distance: \\n{0}\".format(goog_edit_distance))\n",
    "    print(\"RE-RANKED Edit Distance: \\n{0}\".format(rr_edit_distance))\n",
    "    print()\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use other TED speeches for building test set\n",
    "test_speeches = {}\n",
    "for segments in stm_segments[:2]:\n",
    "    for segment in segments:\n",
    "        segment_key = \"{0}_{1}_{2}\".format(segment.speaker_id.strip(), str(segment.start_time).replace('.','_'),\n",
    "                                          str(segment.stop_time).replace('.','_'))\n",
    "\n",
    "        speech = None\n",
    "        # If not already exist\n",
    "        if segment.speaker_id not in test_speeches.keys():\n",
    "            # Connect to Cloud API to get Candidate Transcripts\n",
    "            source_file = os.path.join(os.getcwd(), 'TEDLIUM_release1', 'train','sph', '{}.sph'.format(segment.filename))\n",
    "            speech = Speech(speaker_id=segment.speaker_id,\n",
    "                                           speech_id = segment_key,\n",
    "                                           source_file=source_file,\n",
    "                                           ground_truth = ' '.join(segment.transcript.split()[:-1]),\n",
    "                                           start = segment.start_time,\n",
    "                                           stop = segment.stop_time,\n",
    "                                           audio_type = 'LINEAR16')\n",
    "        else:\n",
    "            speech = test_speeches[segment.speaker_id.strip()]\n",
    "            print('Already found speech in list at location: ', speech)\n",
    "        \n",
    "        \n",
    "        \n",
    "        test_speeches[segment_key] = speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Cloud Speech API Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_audio_size(audio_filepath):\n",
    "    statinfo = os.stat(audio_filepath)\n",
    "    return statinfo.st_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gcs_api_wrapper import GCSWrapper\n",
    "\n",
    "speaker_id, lexicon = list(lexicons.items())[0]\n",
    "gcs = GCSWrapper()\n",
    "cache_directory = os.path.join(os.getcwd(), 'datacache', 'speech_objects')\n",
    "for speech_id, speech in test_speeches.items():\n",
    "    # Not already saved in prepocess cache\n",
    "    cache_file = os.path.join(cache_directory,'{}_preprocess.p'.format(speech.speech_id))\n",
    "    if not speech.candidate_transcripts: \n",
    "        size = get_audio_size(speech.audio_file)\n",
    "        \n",
    "        #TODO: Split large audio file into new files, build new speech objects\n",
    "        if size < 10485760:\n",
    "            try:\n",
    "                result = gcs.transcribe_speech(speech.audio_file)\n",
    "            except:\n",
    "                result = None\n",
    "            if result:\n",
    "                speech.populate_gcs_results(result)\n",
    "                speech.preprocess_and_save()\n",
    "                print('Adding speech with candidate_transcripts to lexicon')\n",
    "                lexicon.add_speech(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM Net and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  500/4633 - Train Accuracy: 0.5290, Validation Accuracy: 0.6429, Loss: 3.9131\n",
      "Epoch   0 Batch 1000/4633 - Train Accuracy: 0.5708, Validation Accuracy: 0.6473, Loss: 3.4197\n",
      "Epoch   0 Batch 1500/4633 - Train Accuracy: 0.4297, Validation Accuracy: 0.6429, Loss: 4.2420\n",
      "Epoch   0 Batch 2000/4633 - Train Accuracy: 0.6562, Validation Accuracy: 0.6496, Loss: 2.5129\n",
      "Epoch   0 Batch 2500/4633 - Train Accuracy: 0.4543, Validation Accuracy: 0.6540, Loss: 4.0889\n",
      "Epoch   0 Batch 3000/4633 - Train Accuracy: 0.5670, Validation Accuracy: 0.6473, Loss: 3.0307\n",
      "Epoch   0 Batch 3500/4633 - Train Accuracy: 0.5580, Validation Accuracy: 0.6518, Loss: 3.0644\n",
      "Epoch   0 Batch 4000/4633 - Train Accuracy: 0.6317, Validation Accuracy: 0.6585, Loss: 2.6059\n",
      "Epoch   0 Batch 4500/4633 - Train Accuracy: 0.6808, Validation Accuracy: 0.6652, Loss: 2.1342\n",
      "Epoch   1 Batch  500/4633 - Train Accuracy: 0.5580, Validation Accuracy: 0.6629, Loss: 2.8747\n",
      "Epoch   1 Batch 1000/4633 - Train Accuracy: 0.6104, Validation Accuracy: 0.6763, Loss: 2.6090\n",
      "Epoch   1 Batch 1500/4633 - Train Accuracy: 0.4896, Validation Accuracy: 0.6719, Loss: 3.1711\n",
      "Epoch   1 Batch 2000/4633 - Train Accuracy: 0.6899, Validation Accuracy: 0.6763, Loss: 1.9286\n",
      "Epoch   1 Batch 2500/4633 - Train Accuracy: 0.5072, Validation Accuracy: 0.6741, Loss: 3.2042\n",
      "Epoch   1 Batch 3000/4633 - Train Accuracy: 0.6116, Validation Accuracy: 0.6875, Loss: 2.4120\n",
      "Epoch   1 Batch 3500/4633 - Train Accuracy: 0.6071, Validation Accuracy: 0.6875, Loss: 2.4847\n",
      "Epoch   1 Batch 4000/4633 - Train Accuracy: 0.6384, Validation Accuracy: 0.6786, Loss: 2.1315\n",
      "Epoch   1 Batch 4500/4633 - Train Accuracy: 0.7188, Validation Accuracy: 0.7009, Loss: 1.7160\n",
      "Epoch   2 Batch  500/4633 - Train Accuracy: 0.6004, Validation Accuracy: 0.6920, Loss: 2.4000\n",
      "Epoch   2 Batch 1000/4633 - Train Accuracy: 0.6396, Validation Accuracy: 0.6987, Loss: 2.2070\n",
      "Epoch   2 Batch 1500/4633 - Train Accuracy: 0.5130, Validation Accuracy: 0.7098, Loss: 2.6192\n",
      "Epoch   2 Batch 2000/4633 - Train Accuracy: 0.7644, Validation Accuracy: 0.7165, Loss: 1.5447\n",
      "Epoch   2 Batch 2500/4633 - Train Accuracy: 0.5409, Validation Accuracy: 0.7121, Loss: 2.7191\n",
      "Epoch   2 Batch 3000/4633 - Train Accuracy: 0.6652, Validation Accuracy: 0.7277, Loss: 2.0702\n",
      "Epoch   2 Batch 3500/4633 - Train Accuracy: 0.6429, Validation Accuracy: 0.7210, Loss: 2.1443\n",
      "Epoch   2 Batch 4000/4633 - Train Accuracy: 0.6763, Validation Accuracy: 0.7321, Loss: 1.8152\n",
      "Epoch   2 Batch 4500/4633 - Train Accuracy: 0.7366, Validation Accuracy: 0.7478, Loss: 1.4358\n",
      "Epoch   3 Batch  500/4633 - Train Accuracy: 0.6116, Validation Accuracy: 0.7790, Loss: 2.0553\n",
      "Epoch   3 Batch 1000/4633 - Train Accuracy: 0.6458, Validation Accuracy: 0.7545, Loss: 1.9477\n",
      "Epoch   3 Batch 1500/4633 - Train Accuracy: 0.5755, Validation Accuracy: 0.7701, Loss: 2.2385\n",
      "Epoch   3 Batch 2000/4633 - Train Accuracy: 0.7812, Validation Accuracy: 0.7946, Loss: 1.2694\n",
      "Epoch   3 Batch 2500/4633 - Train Accuracy: 0.5673, Validation Accuracy: 0.7835, Loss: 2.3364\n",
      "Epoch   3 Batch 3000/4633 - Train Accuracy: 0.6942, Validation Accuracy: 0.7857, Loss: 1.7678\n",
      "Epoch   3 Batch 3500/4633 - Train Accuracy: 0.6518, Validation Accuracy: 0.7946, Loss: 1.8480\n",
      "Epoch   3 Batch 4000/4633 - Train Accuracy: 0.6830, Validation Accuracy: 0.8125, Loss: 1.5762\n",
      "Epoch   3 Batch 4500/4633 - Train Accuracy: 0.7567, Validation Accuracy: 0.8103, Loss: 1.2439\n",
      "Epoch   4 Batch  500/4633 - Train Accuracy: 0.6406, Validation Accuracy: 0.8326, Loss: 1.7958\n",
      "Epoch   4 Batch 1000/4633 - Train Accuracy: 0.6604, Validation Accuracy: 0.7812, Loss: 1.7397\n",
      "Epoch   4 Batch 1500/4633 - Train Accuracy: 0.5964, Validation Accuracy: 0.8192, Loss: 1.9535\n",
      "Epoch   4 Batch 2000/4633 - Train Accuracy: 0.8101, Validation Accuracy: 0.8237, Loss: 1.0455\n",
      "Epoch   4 Batch 2500/4633 - Train Accuracy: 0.5962, Validation Accuracy: 0.8482, Loss: 2.0129\n"
     ]
    }
   ],
   "source": [
    "speaker_id, lexicon = list(lexicons.items())[0]\n",
    "lexicon.optimize(early_stop=False)\n",
    "#lexicon.evaluate_testset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate LSTM Net Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speaker_id, lexicon = list(lexicons.items())[0]\n",
    "print(\"List of Speeches:\", len(lexicon.speeches))\n",
    "lexicon.evaluate_testset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper \n",
    "# Save parameters for checkpoint\n",
    "speaker_id, lexicon = list(lexicons.items())[0]\n",
    "helper.save_params(lexicon.cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "speaker_id, lexicon = list(lexicons.items())[0]\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = helper.load_preprocess()\n",
    "load_path = helper.load_params(lexicon.cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate N-Gram Model on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gather all samples, load into dictionary\n",
    "# Prepare a plain text corpus from which we train a languague model\n",
    "import glob\n",
    "import operator\n",
    "\n",
    "# Gather all text files from directory\n",
    "WORKING_DIRECTORY = os.path.join(os.getcwd(),'LibriSpeech/')\n",
    "\n",
    "dev_path = \"{}{}{}{}\".format(WORKING_DIRECTORY, 'dev-clean/', '**/', '*.txt')\n",
    "train_path = \"{}{}{}{}{}\".format(WORKING_DIRECTORY, 'books/', 'utf-8/', '**/', '*.txt*')\n",
    "\n",
    "text_paths = sorted(glob.glob(dev_path, recursive=True))\n",
    "print('Found',len(text_paths),'text files in the directory:', dev_path)\n",
    "\n",
    "transcripts = {}\n",
    "for document in text_paths:\n",
    "    with codecs.open(document, 'r', 'utf-8') as filep:\n",
    "        for i,line in enumerate(filep):\n",
    "            transcripts[line.split()[0]] = ' '.join(line.split()[1:])\n",
    "\n",
    "## Evaluate all samples found ##\n",
    "cloud_speech_api_accuracy = []\n",
    "custom_lang_model_accuracy = []\n",
    "epsilon = 0.000000001\n",
    "api_weight = 0.85\n",
    "steps = 0\n",
    "# Pull In Audio File\n",
    "for filename, gt_transcript in transcripts.items():\n",
    "    steps += 1\n",
    "    dirs = filename.split('-')\n",
    "    \n",
    "    audio_filepath = dev_file_name_0 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    dirs[0],\n",
    "    dirs[1],\n",
    "    \"{0}.flac\".format(filename))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Load the audio into memory\n",
    "    with io.open(audio_filepath, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "        audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code='en-US',\n",
    "        max_alternatives=10,\n",
    "        profanity_filter=False,\n",
    "        enable_word_time_offsets=True)\n",
    "\n",
    "    # Detects speech and words in the audio file\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "    result = operation.result(timeout=90)\n",
    "    alternatives = result.results[0].alternatives\n",
    "\n",
    "\n",
    "    # Evaluate API Results for Re-Ranking:\n",
    "    rerank_results = {}\n",
    "    for alternative in alternatives:\n",
    "        sent = alternative.transcript\n",
    "        \n",
    "        # Strip punctuation\n",
    "        translate_table = dict((ord(char), None) for char in string.punctuation)        \n",
    "        sent = sent.translate(translate_table) # remove punctuations\n",
    "\n",
    "        words = nltk.tokenize.word_tokenize(sent)\n",
    "        probs = np.ones_like(words, dtype=np.float32)*epsilon\n",
    "\n",
    "        for word in words:\n",
    "            if words.index(word) < len(words)-1: \n",
    "                freq = cfreq_2gram[word].freq(words[words.index(word)+1])\n",
    "                probs[words.index(word)] = freq\n",
    "\n",
    "        lexicon_score = np.sum(probs)\n",
    "\n",
    "        # Re-rank alternatives using a weighted average of the two scores\n",
    "        confidence_score = alternative.confidence*api_weight + lexicon_score*(1-api_weight)\n",
    "        rerank_results[alternative.transcript] = confidence_score\n",
    "\n",
    "\n",
    "    \n",
    "    index, value = max(enumerate(list(rerank_results.values())), key=operator.itemgetter(1))\n",
    "    # Select Corresponding Transcript:\n",
    "    script=''\n",
    "    for trnscript, confidence in rerank_results.items():\n",
    "        if confidence == value:\n",
    "            script = trnscript\n",
    "                \n",
    "    # Compute the Accuracy, based on the Levenshtein Distance (a.k.a. Edit Distance)\n",
    "    gcs_ed = nltk.edit_distance(alternative.transcript.lower(), gt_transcript.lower())\n",
    "    gcs_upper_bound = max(len(alternative.transcript),len(gt_transcript))\n",
    "    gcs_accuracy = (1.0 - gcs_ed/gcs_upper_bound)\n",
    "    \n",
    "    clm_ed = nltk.edit_distance(script.lower(), gt_transcript.lower())\n",
    "    clm_upper_bound = max(len(script),len(gt_transcript))\n",
    "    clm_accuracy = (1.0 - clm_ed/clm_upper_bound)\n",
    "    \n",
    "    cloud_speech_api_accuracy.append(gcs_accuracy)\n",
    "    custom_lang_model_accuracy.append(clm_accuracy)\n",
    "\n",
    "    if steps % 100 == 0:\n",
    "        print(\"{0} Transcripts Processed.\".format(steps))\n",
    "        print('Average API Accuracy:', np.mean(cloud_speech_api_accuracy))\n",
    "        print('Average Custom Model Accuracy:', np.mean(custom_lang_model_accuracy))\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [sdc_dev]",
   "language": "python",
   "name": "Python [sdc_dev]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
