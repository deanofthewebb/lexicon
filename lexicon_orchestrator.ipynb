{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon - Orchestrator\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "For this project, I will build a simple custom ochestrator that processes data objects from the \"Lexicon\" class.\n",
    "    - These objects are custom datasets that are modeled after the Ted Talk speakers. \n",
    "    - Each Lexicon has a corpus and some helper methods aimed at training and prediction\n",
    "    - Lexicon class will also have a preprocessing and caching function.\n",
    "    - Each object will have two methods of prediction, n-gram language model and a recurrent neural network model\n",
    "    - Each object has a custom reporting function that reports the results of training\n",
    "    - Each object will be able to learn from any text data provided, and return a transcript with confidence values from input posed in speech utterances. \n",
    "        - I will use Google's cloud-based services to preprocess the input audio data and transcribe into an initial guess. Then I will train a model to improve on Google cloud speech API's response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use to reload modules\n",
    "from importlib import reload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']=os.path.join(os.getcwd(),'Lexicon-e94eff39fad7.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "\n",
    "librispeech_dataset_folder_path = 'LibriSpeech'\n",
    "tar_gz_path = 'dev-clean.tar.gz'\n",
    "\n",
    "books_path = 'original-books.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(books_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Librispeech Book Texts') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://www.openslr.org/resources/12/original-books.tar.gz',\n",
    "            books_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(librispeech_dataset_folder_path+'/books'):\n",
    "    with tarfile.open(books_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Librispeech dev-clean.tar.gz') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://www.openslr.org/resources/12/dev-clean.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(librispeech_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import speech\n",
    "from google.cloud.speech import enums\n",
    "from google.cloud.speech import types\n",
    "\n",
    "# Instantiates a client\n",
    "client = speech.SpeechClient()\n",
    "\n",
    "# The name of the dev-test audio file to transcribe\n",
    "dev_file_name_0 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0000.flac')\n",
    "gt0 = 'GO DO YOU HEAR'\n",
    "\n",
    "dev_file_name_1 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0001.flac')\n",
    "gt1 = 'BUT IN LESS THAN FIVE MINUTES THE STAIRCASE GROANED BENEATH AN EXTRAORDINARY WEIGHT'\n",
    "\n",
    "# The name of the test audio file to transcribe\n",
    "dev_file_name_2 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0002.flac')\n",
    "gt2 = 'AT THIS MOMENT THE WHOLE SOUL OF THE OLD MAN SEEMED CENTRED IN HIS EYES WHICH BECAME BLOODSHOT THE VEINS OF THE THROAT SWELLED HIS CHEEKS AND TEMPLES BECAME PURPLE AS THOUGH HE WAS STRUCK WITH EPILEPSY NOTHING WAS WANTING TO COMPLETE THIS BUT THE UTTERANCE OF A CRY'\n",
    "\n",
    "dev_file_name_3 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0003.flac')\n",
    "gt3 = 'AND THE CRY ISSUED FROM HIS PORES IF WE MAY THUS SPEAK A CRY FRIGHTFUL IN ITS SILENCE'\n",
    "\n",
    "dev_file_name_4 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0004.flac')\n",
    "gt4 = \"D'AVRIGNY RUSHED TOWARDS THE OLD MAN AND MADE HIM INHALE A POWERFUL RESTORATIVE\"\n",
    "\n",
    "\n",
    "test_file_name_1 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'RNN-Tutorial-master',\n",
    "    'data',\n",
    "    'raw',\n",
    "    'librivox',\n",
    "    'LibriSpeech',\n",
    "    'test-clean-wav',\n",
    "    '4507-16021-0019.wav')\n",
    "\n",
    "\n",
    "audio_files = {dev_file_name_0:gt0, dev_file_name_1:gt1, dev_file_name_2:gt2, dev_file_name_3:gt3, dev_file_name_4:gt4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 41 text files in the directories /src/lexicon/LibriSpeech/books/utf-8/**/*.txt*\n",
      "97 segmented text files in the /src/lexicon/LibriSpeech/dev-clean/**/*.txt* directory and \n",
      "774 stm files in directory: /src/lexicon/TEDLIUM_release1/train/**/*.stm:\n"
     ]
    }
   ],
   "source": [
    "# Prepare a plain text corpus from which we train a languague model\n",
    "import glob\n",
    "import os\n",
    "import utils\n",
    "import nltk\n",
    "\n",
    "# Gather all text files from directory\n",
    "LIBRISPEECH_DIRECTORY = os.path.join(os.getcwd(),'LibriSpeech/')\n",
    "TEDLIUM_DIRECTORY = os.path.join(os.getcwd(),'TEDLIUM_release1/')\n",
    "\n",
    "# TRAINING_DIRECTORY = os.path.abspath(os.path.join(os.sep,'Volumes',\"My\\ Passport\\ for\\ Mac\",'lexicon','LibriSpeech'))\n",
    "dev_librispeech_path = \"{}{}{}{}\".format(LIBRISPEECH_DIRECTORY, 'dev-clean/', '**/', '*.txt*')\n",
    "train_librispeech_path = \"{}{}{}{}{}\".format(LIBRISPEECH_DIRECTORY, 'books/', 'utf-8/', '**/', '*.txt*')\n",
    "TED_path = \"{}{}{}{}\".format(TEDLIUM_DIRECTORY,'train/','**/', '*.stm')\n",
    "\n",
    "text_paths = sorted(glob.glob(train_librispeech_path, recursive=True))\n",
    "segmented_text_paths = sorted(glob.glob(dev_librispeech_path, recursive=True))\n",
    "stm_paths = sorted(glob.glob(TED_path, recursive=True))\n",
    "\n",
    "print('Found:',len(text_paths),\"text files in the directories {0}\\n{1} segmented text files in the {2} directory and \\n{3} stm files in directory: {4}:\".format(train_librispeech_path, \n",
    "        len(segmented_text_paths), dev_librispeech_path, len(stm_paths),TED_path ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Text Corpuses for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "from lexicon import Lexicon\n",
    "from speech import Speech\n",
    "      \n",
    "librispeech_corpus = u\"\"\n",
    "stm_segments = []\n",
    "lexicons = {} # {speaker_id: lexicon_object}\n",
    "speeches = {} # {speech_id: speech_object}\n",
    "segmented_librispeeches = {}\n",
    "\n",
    "for book_filename in text_paths[:10]: # 1 Book\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        lines = book_file.read()\n",
    "        librispeech_corpus += lines\n",
    "for stm_filename in stm_paths: # Process STM files (Tedlium)\n",
    "        stm_segments.append(utils.parse_stm_file(stm_filename))\n",
    "        \n",
    "\n",
    "# Train on 3 speakers\n",
    "for segments in stm_segments[15:17]: \n",
    "    for segment in segments:\n",
    "        segment_key = \"{0}_{1}_{2}\".format(segment.speaker_id.strip(), str(segment.start_time).replace('.','_'),\n",
    "                                          str(segment.stop_time).replace('.','_'))\n",
    "        if segment.speaker_id not in speeches.keys():\n",
    "            source_file = os.path.join(os.getcwd(), 'TEDLIUM_release1',\n",
    "                                       'train','sph', '{}.sph'.format(segment.filename))\n",
    "            speech = Speech(speaker_id=segment.speaker_id,\n",
    "                                           speech_id = segment_key,\n",
    "                                           source_file=source_file,\n",
    "                                           ground_truth = ' '.join(segment.transcript.split()[:-1]),\n",
    "                                           start = segment.start_time,\n",
    "                                           stop = segment.stop_time,\n",
    "                                           audio_type = 'LINEAR16')\n",
    "        else:\n",
    "            speech = speeches[segment.speaker_id.strip()]\n",
    "            print('Already found speech in list at location: ', speech)\n",
    "        \n",
    "        speeches[segment_key] = speech\n",
    "\n",
    "        if segment.speaker_id not in lexicons.keys():\n",
    "            lexicon = Lexicon(base_corpus=librispeech_corpus, name=segment.speaker_id)\n",
    "            lexicons[segment.speaker_id.strip()] = lexicon\n",
    "        else:\n",
    "            lexicon = lexicons[segment.speaker_id.strip()]\n",
    "        \n",
    "        # Add Speech to Lexicon\n",
    "        if speech not in lexicon.speeches:\n",
    "            lexicon.add_speech(speech)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GCS Transcripts using GCS Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 57965\n",
      "Number of sentences: 27540\n",
      "Average number of words in a sentence: 24.005228758169935\n",
      "\n",
      "Transcript sentences 0 to 10:\n",
      " Merrick\n",
      " Would it not be better for me to send these\n",
      "papers by a messenger to your house?\"\n",
      "\n",
      "\"No; I'll take them myself\n",
      " No one will rob me\n",
      "\" And then the door\n",
      "swung open and, chuckling in his usual whimsical fashion, Uncle John\n",
      "came out, wearing his salt-and-pepper suit and stuffing; a bundle of\n",
      "papers into his inside pocket\n",
      "\n",
      "\n",
      "The Major stared at him haughtily, but made no attempt to openly\n",
      "recognize the man\n",
      " Uncle John gave a start, laughed, and then walked\n",
      "away briskly, throwing a hasty \"good-bye\" to the obsequious banker,\n",
      "who followed him out, bowing low\n",
      "\n",
      "\n",
      "The Major returned to his office with a grave face, and sat for the\n",
      "best part of three hours in a brown study\n",
      " Then he took his hat and\n",
      "went home\n",
      "\n",
      "\n",
      "Patsy asked anxiously if anything had happened, when she saw his face;\n",
      "but the Major shook his head\n",
      "\n",
      "\n",
      "Uncle John arrived just in time for dinner, in a very genial mood,\n",
      "and he and Patsy kept up a lively conversation at the table while the\n",
      "Major looked stern every time he caught the little man's eye\n",
      "\n",
      "Ground Truth sentences 0 to 10:\n",
      " Merrick\n",
      " Would it not be better for me to send these\n",
      "papers by a messenger to your house?\"\n",
      "\n",
      "\"No; I'll take them myself\n",
      " No one will rob me\n",
      "\" And then the door\n",
      "swung open and, chuckling in his usual whimsical fashion, Uncle John\n",
      "came out, wearing his salt-and-pepper suit and stuffing; a bundle of\n",
      "papers into his inside pocket\n",
      "\n",
      "\n",
      "The Major stared at him haughtily, but made no attempt to openly\n",
      "recognize the man\n",
      " Uncle John gave a start, laughed, and then walked\n",
      "away briskly, throwing a hasty \"good-bye\" to the obsequious banker,\n",
      "who followed him out, bowing low\n",
      "\n",
      "\n",
      "The Major returned to his office with a grave face, and sat for the\n",
      "best part of three hours in a brown study\n",
      " Then he took his hat and\n",
      "went home\n",
      "\n",
      "\n",
      "Patsy asked anxiously if anything had happened, when she saw his face;\n",
      "but the Major shook his head\n",
      "\n",
      "\n",
      "Uncle John arrived just in time for dinner, in a very genial mood,\n",
      "and he and Patsy kept up a lively conversation at the table while the\n",
      "Major looked stern every time he caught the little man's eye\n",
      "\n",
      "Dataset Stats\n",
      "Roughly the number of unique words: 58051\n",
      "Number of sentences: 27600\n",
      "Average number of words in a sentence: 24.12873188405797\n",
      "\n",
      "Transcript sentences 0 to 10:\n",
      " Merrick\n",
      " Would it not be better for me to send these\n",
      "papers by a messenger to your house?\"\n",
      "\n",
      "\"No; I'll take them myself\n",
      " No one will rob me\n",
      "\" And then the door\n",
      "swung open and, chuckling in his usual whimsical fashion, Uncle John\n",
      "came out, wearing his salt-and-pepper suit and stuffing; a bundle of\n",
      "papers into his inside pocket\n",
      "\n",
      "\n",
      "The Major stared at him haughtily, but made no attempt to openly\n",
      "recognize the man\n",
      " Uncle John gave a start, laughed, and then walked\n",
      "away briskly, throwing a hasty \"good-bye\" to the obsequious banker,\n",
      "who followed him out, bowing low\n",
      "\n",
      "\n",
      "The Major returned to his office with a grave face, and sat for the\n",
      "best part of three hours in a brown study\n",
      " Then he took his hat and\n",
      "went home\n",
      "\n",
      "\n",
      "Patsy asked anxiously if anything had happened, when she saw his face;\n",
      "but the Major shook his head\n",
      "\n",
      "\n",
      "Uncle John arrived just in time for dinner, in a very genial mood,\n",
      "and he and Patsy kept up a lively conversation at the table while the\n",
      "Major looked stern every time he caught the little man's eye\n",
      "\n",
      "Ground Truth sentences 0 to 10:\n",
      " Merrick\n",
      " Would it not be better for me to send these\n",
      "papers by a messenger to your house?\"\n",
      "\n",
      "\"No; I'll take them myself\n",
      " No one will rob me\n",
      "\" And then the door\n",
      "swung open and, chuckling in his usual whimsical fashion, Uncle John\n",
      "came out, wearing his salt-and-pepper suit and stuffing; a bundle of\n",
      "papers into his inside pocket\n",
      "\n",
      "\n",
      "The Major stared at him haughtily, but made no attempt to openly\n",
      "recognize the man\n",
      " Uncle John gave a start, laughed, and then walked\n",
      "away briskly, throwing a hasty \"good-bye\" to the obsequious banker,\n",
      "who followed him out, bowing low\n",
      "\n",
      "\n",
      "The Major returned to his office with a grave face, and sat for the\n",
      "best part of three hours in a brown study\n",
      " Then he took his hat and\n",
      "went home\n",
      "\n",
      "\n",
      "Patsy asked anxiously if anything had happened, when she saw his face;\n",
      "but the Major shook his head\n",
      "\n",
      "\n",
      "Uncle John arrived just in time for dinner, in a very genial mood,\n",
      "and he and Patsy kept up a lively conversation at the table while the\n",
      "Major looked stern every time he caught the little man's eye\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "view_sentence_range = (0, 10)\n",
    "\n",
    "for speaker_id, lexicon in lexicons.items():\n",
    "    print('Dataset Stats')\n",
    "    print('Roughly the number of unique words: {}'.format(lexicon.vocab_size))\n",
    "    \n",
    "    word_counts = [len(sentence.split()) for sentence in lexicon.corpus_sentences]\n",
    "    print('Number of sentences: {}'.format(len(lexicon.corpus_sentences)))\n",
    "    print('Average number of words in a sentence: {}'.format(np.average(word_counts)))\n",
    "\n",
    "    print()\n",
    "    print('Transcript sentences {} to {}:'.format(*view_sentence_range))\n",
    "    print('\\n'.join(lexicon.training_set[0][view_sentence_range[0]:view_sentence_range[1]]))\n",
    "    print()\n",
    "    print('Ground Truth sentences {} to {}:'.format(*view_sentence_range))\n",
    "    print('\\n'.join(lexicon.training_set[1][view_sentence_range[0]:view_sentence_range[1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset - Tokenize Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "# reading the file in unicode format using codecs library    \n",
    "stoplist = set(stopwords.words('english'))\n",
    "# Strip punctuation\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation) \n",
    "        \n",
    "corpus_raw = u\"\"\n",
    "for book_filename in text_paths:\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        lines = book_file.read()\n",
    "        corpus_raw += lines.translate(translate_table) # remove punctuations \n",
    "\n",
    "               \n",
    "# Tokenize\n",
    "tokenized_words = nltk.tokenize.word_tokenize(corpus_raw)\n",
    "\n",
    "## Clean the tokens ##\n",
    "# Remove stop words\n",
    "tokenized_words = [word for word in tokenized_words if word not in stoplist]\n",
    "\n",
    "# Remove single-character tokens (mostly punctuation)\n",
    "tokenized_words = [word for word in tokenized_words if len(word) > 1]\n",
    "\n",
    "# Remove numbers\n",
    "tokenized_words = [word for word in tokenized_words if not word.isnumeric()]\n",
    "\n",
    "# Lowercase all words (default_stopwords are lowercase too)\n",
    "tokenized_words = [word.lower() for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset - Extract N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.collocations import *\n",
    "from nltk.probability import FreqDist\n",
    "import nltk\n",
    "\n",
    "# extracting the bi-grams and sorting them according to their frequencies\n",
    "finder = BigramCollocationFinder.from_words(tokenized_words)\n",
    "# finder.apply_freq_filter(3)\n",
    "\n",
    "bigram_model = nltk.bigrams(tokenized_words)\n",
    "bigram_model = sorted(bigram_model, key=lambda item: item[1], reverse=True)  \n",
    "# print(bigram_model)\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "np.save(\"lang_model.npy\",bigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word|Freq:\n",
      "('project', 'gutenbergtm')|1095\n",
      "('project', 'gutenberg')|1014\n",
      "('greater', 'part')|532\n",
      "('captain', 'nemo')|452\n",
      "('united', 'states')|407\n",
      "('great', 'britain')|385\n",
      "('uncle', 'john')|364\n",
      "('gold', 'silver')|337\n",
      "('let', 'us')|331\n",
      "('of', 'course')|328\n",
      "('new', 'york')|310\n",
      "('old', 'man')|306\n",
      "('gutenbergtm', 'electronic')|306\n",
      "('mr', 'bounderby')|294\n",
      "('public', 'domain')|293\n",
      "('every', 'one')|291\n",
      "('young', 'man')|284\n",
      "('mrs', 'sparsit')|282\n",
      "('one', 'day')|281\n",
      "('one', 'another')|280\n",
      "('archive', 'foundation')|279\n",
      "('gutenberg', 'literary')|279\n",
      "('literary', 'archive')|279\n",
      "('dont', 'know')|275\n",
      "('electronic', 'works')|272\n",
      "('per', 'cent')|263\n",
      "('could', 'see')|262\n",
      "('ned', 'land')|254\n",
      "('good', 'deal')|247\n",
      "('two', 'three')|240\n",
      "('set', 'forth')|225\n",
      "('years', 'ago')|220\n",
      "('old', 'woman')|219\n",
      "('you', 'may')|218\n",
      "('it', 'would')|207\n",
      "('the', 'first')|206\n",
      "('next', 'day')|201\n",
      "('long', 'time')|200\n",
      "('said', 'mrs')|199\n",
      "('said', 'mr')|198\n",
      "('of', 'the')|198\n",
      "('first', 'time')|196\n",
      "('every', 'day')|193\n",
      "('one', 'thing')|193\n",
      "('small', 'print')|189\n",
      "('men', 'women')|187\n",
      "('electronic', 'work')|187\n",
      "('every', 'man')|182\n",
      "('mr', 'gradgrind')|173\n",
      "('it', 'may')|172\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(bigram_model)\n",
    "\n",
    "# Output top 50 words\n",
    "print(\"Word|Freq:\")\n",
    "for word, frequency in fdist.most_common(50):\n",
    "    print(u'{}|{}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Listing the words that can follow after 'greater':\n",
      " dict_keys(['valuable', 'smaller', 'gehenna', 'desolation', 'dexterity', 'opportunities', 'woe', 'returns', 'perithous', 'tenant', 'honor', 'indeed', 'agony', 'glorious', 'latter', 'account', 'divinity', 'sum', 'indignation', 'sin', 'advantage', 'transgression', 'flourish', 'necessary', 'teacher', 'activity', 'fortune', 'whole', 'rank', 'beauty', 'abundance', 'cost', 'disorders', 'sums', 'told', 'opening', 'action', 'parsimony', 'claim', 'worlds', 'beginning', 'convenience', 'labourers', 'change', 'supply', 'require', 'equal', 'found', 'weal', 'confidence', 'expected', 'knave', 'scarcity', 'quantity', 'gift', 'thoughts', 'trade', 'insult', 'deviation', 'stock', 'second', 'prince', 'extent', 'great', 'semblance', 'america', 'zeal', 'solidarity', 'clerk', 'want', 'among', 'rum', 'sun', 'riches', 'lesser', 'wealth', 'proportion', 'importation', 'slaves', 'liberty', 'grew', 'sorrow', 'intelligence', 'it', 'moment', 'need', 'inferiority', 'difference', 'vessels', 'as', 'writings', 'number', 'sometimes', 'men', 'sadness', 'little', 'clearness', 'distinctness', 'cheap', 'ones', 'rapidity', 'capital', 'countries', 'boldness', 'thirst', 'balance', 'crop', 'danger', 'shall', 'eloquence', 'never', 'whatever', 'peace', 'power', 'general', 'cheapness', 'usual', 'fast', 'warmth', 'titan', 'tartness', 'relevance', 'come', 'enthusiasm', 'body', 'sensation', 'and', 'dole', 'prospective', 'length', 'injustice', 'perpendicular', 'worldly', 'mans', 'depth', 'glory', 'otherwise', 'calamity', 'he', 'splendour', 'costage', 'heartiness', 'difficulties', 'salaries', 'frequently', 'gladness', 'influence', 'gain', 'gold', 'professed', 'melodies', 'strength', 'weight', 'desire', 'every', 'nails', 'present', 'incorporation', 'simplicity', 'environment', 'contained', 'wisdom', 'former', 'exportation', 'understanding', 'still', 'return', 'made', 'fortitude', 'therefore', 'produce', 'confusion', 'like', 'actually', 'force', 'guilds', 'remoteness', 'favour', 'france', 'well', 'alterations', 'use', 'delectation', 'already', 'if', 'than', 'peril', 'strain', 'life', 'at', 'range', 'sanctity', 'freedom', 'service', 'wrong', 'depths', 'herein', 'importance', 'goods', 'mountains', 'the', 'mass', 'rapture', 'content', 'business', 'sovereign', 'wonder', 'comfort', 'fame', 'art', 'things', 'mystery', 'far', 'subconscious', 'fiercer', 'either', 'without', 'continuing', 'restoration', 'rent', 'love', 'annual', 'portion', 'seems', 'land', 'effort', 'chance', 'impersonal', 'malversation', 'diligence', 'london', 'gifts', 'mastery', 'talents', 'to', 'waxen', 'augmentation', 'pasture', 'money', 'superabundance', 'upon', 'parts', 'quantities', 'vitality', 'such', 'ever', 'brewery', 'ii', 'capacities', 'believed', 'demand', 'name', 'pleasure', 'corn', 'strides', 'personal', 'steps', 'fire', 'economy', 'dilatation', 'no', 'rice', 'practicality', 'yet', 'place', 'would', 'view', 'death', 'variation', 'difficulty', 'shame', 'original', 'renown', 'amount', 'energy', 'revenue', 'antipathy', 'fund', 'expense', 'might', 'poets', 'triumph', 'circulation', 'universal', 'vanquished', 'field', 'fury', 'reduction', 'haste', 'favours', 'pressure', 'latitude', 'taint', 'less', 'circumstances', 'dangers', 'left', 'extensive', 'lawe', 'brightness', 'haytime', 'though', 'offence', 'lights', 'authority', 'crown', 'anyone', 'real', 'consequence', 'windbag', 'value', 'mind', 'saving', 'expression', 'wiser', 'share', 'facility', 'contemporary', 'enduring', 'point', 'degree', 'honourable', 'sufficient', 'foregoing', 'care', 'its', 'surplus', 'horn', 'pomp', 'attractions', 'favours\\x94', 'group', 'numbers', 'higher', 'first', 'satisfaction', 'advantages', 'imprudence', 'dawn', 'africa', 'fear', 'lasting', 'tax', 'ruritania', 'crime', 'could', 'variety', 'english', 'ease', 'rich', 'but', 'age', 'employs', 'thing', 'annoyance', 'inconveniency', 'time', 'kindness', 'evil', 'greater', 'perhaps', 'must', 'degrees', 'producing', 'leader', 'many', 'hope', 'admiration', 'interest', 'taxes', 'individual', 'none', 'frequency', 'success', 'harm', 'dignity', 'injudicious', 'heat', 'one', 'went', 'common', 'in', 'abroad', 'formerly', 'evils', 'done', 'modern', 'pain', 'past', 'height', 'honour', 'frequent', 'ordinary', 'security', 'approximation', 'levying', 'trespass', 'fault', 'profusion', 'price', 'slumbers', 'jefferies', 'togetherness', 'pride', 'pieces', 'speed', 'loss', 'reprobate', 'ned', 'stocks', 'rights', 'violence', 'silence', 'benefit', 'distress', 'fixed', 'effect', 'almost', 'end', 'grief', 'singleness', 'acorn', 'suited', 'competition', 'velocity', 'obstacles', 'rise', 'profit', 'seignorage', 'goodness', 'grace', 'encountering', 'cultivation', 'events', 'distance', 'part'])\n",
      "\n",
      "Listing 20 most frequent words to come after 'greater':\n",
      " [('part', 532), ('quantity', 105), ('number', 50), ('proportion', 43), ('value', 24), ('smaller', 16), ('share', 16), ('greater', 16), ('less', 12), ('profit', 11), ('capital', 9), ('importance', 9), ('the', 9), ('revenue', 9), ('surplus', 8), ('degree', 7), ('variety', 7), ('distance', 7), ('sum', 6), ('stock', 6)]\n"
     ]
    }
   ],
   "source": [
    "cfreq_2gram = nltk.ConditionalFreqDist(bigram_model)\n",
    "# print('Conditional Frequency Conditions:\\n', cfreq_2gram)\n",
    "print()\n",
    "\n",
    "# First access the FreqDist associated with \"one\", then the keys in that FreqDist\n",
    "print(\"Listing the words that can follow after 'greater':\\n\", cfreq_2gram[\"greater\"].keys())\n",
    "print()\n",
    "\n",
    "# Determine Most common in conditional frequency\n",
    "print(\"Listing 20 most frequent words to come after 'greater':\\n\", cfreq_2gram[\"greater\"].most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for operation to complete...\n",
      "\n",
      "\n",
      "RE-RANKED Results: \n",
      " {'go go do you hear': 0.85315002696588638, 'go do you here': 0.86520871338434524, 'I go do you hear': 0.81552877281792457, 'go do here': 0.85310941742492696, 'do you here': 0.75866525587625799, 'go do you hear': 0.77847528909333052, 'goat do you hear': 0.85976193998940287, 'do you hear': 0.75895958994515234, 'goat do you here': 0.85946760592050853, 'I go do you here': 0.81523443874903023}\n",
      "\n",
      "\n",
      "ORIGINAL Transcript: \n",
      "'goat do you here' \n",
      "with a confidence_score of: 0.9545454978942871\n",
      "\n",
      "\n",
      "RE-RANKED Transcript: \n",
      "'go do you here' \n",
      "with a confidence_score of: 0.8652087133843452\n",
      "\n",
      "\n",
      "GROUND TRUTH TRANSCRIPT: \n",
      "GO DO YOU HEAR\n",
      "\n",
      "The original transcript was RE-RANKED. The transcripts do not match!\n",
      "Differences between original and re-ranked:  ['goat']\n",
      "\n",
      "\n",
      "The original transcript DOES NOT MATCH ground truth.\n",
      "Differences between original and ground truth:  ['here', 'goat']\n",
      "\n",
      "\n",
      "The RE_RANKED transcript DOES NOT MATCH ground truth.\n",
      "Differences between Reranked and ground truth:  ['here']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL Edit Distance: \n",
      "4\n",
      "RE-RANKED Edit Distance: \n",
      "2\n",
      "\n",
      "\n",
      "Waiting for operation to complete...\n",
      "\n",
      "\n",
      "RE-RANKED Results: \n",
      " {'at this moment of the whole soul of the old man seem centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.87474450767040257, 'at this moment of the whole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.86880395710468294, 'at this moment the whole soul of the old man seems centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.88751497417688374, 'at this moment of the whole soul of the old man seemed centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.87742958217859279, 'at this moment the whole soul of the old man seem centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.88437801450490949, 'at this moment to the whole soul of the old man seemed centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.88942871093750009, 'at this moment to the whole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.87938940227031703, 'at this moment the whole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.87849078625440591, 'at this moment to the whole soul of the old man seem centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.88532995283603666, 'at this moment the whole soul of the old man seemed centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.88711463958024983}\n",
      "\n",
      "\n",
      "ORIGINAL Transcript: \n",
      "'at this moment of the whole soul of the old man seemed centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry' \n",
      "with a confidence_score of: 0.9498937726020813\n",
      "\n",
      "\n",
      "RE-RANKED Transcript: \n",
      "'at this moment to the whole soul of the old man seemed centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry' \n",
      "with a confidence_score of: 0.8894287109375001\n",
      "\n",
      "\n",
      "GROUND TRUTH TRANSCRIPT: \n",
      "AT THIS MOMENT THE WHOLE SOUL OF THE OLD MAN SEEMED CENTRED IN HIS EYES WHICH BECAME BLOODSHOT THE VEINS OF THE THROAT SWELLED HIS CHEEKS AND TEMPLES BECAME PURPLE AS THOUGH HE WAS STRUCK WITH EPILEPSY NOTHING WAS WANTING TO COMPLETE THIS BUT THE UTTERANCE OF A CRY\n",
      "\n",
      "No reranking was performed. The transcripts match!\n",
      "\n",
      "\n",
      "The original transcript DOES NOT MATCH ground truth.\n",
      "Differences between original and ground truth:  ['centered']\n",
      "\n",
      "\n",
      "The RE_RANKED transcript DOES NOT MATCH ground truth.\n",
      "Differences between Reranked and ground truth:  ['centered']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL Edit Distance: \n",
      "4\n",
      "RE-RANKED Edit Distance: \n",
      "4\n",
      "\n",
      "\n",
      "Waiting for operation to complete...\n",
      "\n",
      "\n",
      "RE-RANKED Results: \n",
      " {'Devon he rushed towards the old man and made him inhaler powerful restorative': 0.72487533390522008, 'deveny rushed towards the old man and made him and Halo powerful restorative': 0.74758408963680267, 'deveney rushed towards the old man and made him in Halo powerful restorative': 0.73207941949367528, 'Devon he rushed towards the old man and made him and Halo powerful restorative': 0.7115083783864975, 'Devin he rushed towards the old man and made him in Halo powerful restorative': 0.74116749465465548, 'Devin he rushed towards the old man and made him and Halo powerful restorative': 0.76070690453052525, 'Devon he rushed towards the old man and made him in Halo powerful restorative': 0.69701785147190098, 'deveney Rush towards the old man and made him and Halo powerful restorative': 0.69918780922889712, 'deveny Rush towards the old man and made him and Halo powerful restorative': 0.69918780922889712, 'deveney rushed towards the old man and made him and Halo powerful restorative': 0.74758408963680267}\n",
      "\n",
      "\n",
      "ORIGINAL Transcript: \n",
      "'Devon he rushed towards the old man and made him inhaler powerful restorative' \n",
      "with a confidence_score of: 0.7925808429718018\n",
      "\n",
      "\n",
      "RE-RANKED Transcript: \n",
      "'Devin he rushed towards the old man and made him and Halo powerful restorative' \n",
      "with a confidence_score of: 0.7607069045305253\n",
      "\n",
      "\n",
      "GROUND TRUTH TRANSCRIPT: \n",
      "D'AVRIGNY RUSHED TOWARDS THE OLD MAN AND MADE HIM INHALE A POWERFUL RESTORATIVE\n",
      "\n",
      "The original transcript was RE-RANKED. The transcripts do not match!\n",
      "Differences between original and re-ranked:  ['devon', 'inhaler']\n",
      "\n",
      "\n",
      "The original transcript DOES NOT MATCH ground truth.\n",
      "Differences between original and ground truth:  ['devon', 'he', 'inhaler']\n",
      "\n",
      "\n",
      "The RE_RANKED transcript DOES NOT MATCH ground truth.\n",
      "Differences between Reranked and ground truth:  ['devin', 'he', 'halo']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL Edit Distance: \n",
      "9\n",
      "RE-RANKED Edit Distance: \n",
      "13\n",
      "\n",
      "\n",
      "Waiting for operation to complete...\n",
      "\n",
      "\n",
      "RE-RANKED Results: \n",
      " {\"but in less than five minutes the staircase groaned when he's an extraordinary way.\": 0.75442036092281339, \"but I'm less than 5 minutes the staircase groaned when he's an extraordinary wait\": 0.7674720663577318, \"but in less than five minutes the staircase groaned when he's an extraordinary weight\": 0.80769927799701691, \"but in less than five minutes the staircase groaned when he's an extraordinary way\": 0.80769927799701691, \"but I'm less than 5 minutes the staircase groaned when he's an extraordinary way\": 0.7674720663577318, \"but in less than 5 minutes the staircase groaned when he's an extraordinary weight\": 0.7811811616644263, \"but in less than five minutes the staircase groaned when he's an extraordinary wait\": 0.80769927799701691, \"but in less than 5 minutes the staircase groaned when he's an extraordinary way\": 0.81482807900756593, \"but in less than 5 minutes the staircase groaned when he's an extraordinary wait\": 0.81482807900756593, \"but I'm less than 5 minutes the staircase groaned when he's an extraordinary weight\": 0.7674720663577318}\n",
      "\n",
      "\n",
      "ORIGINAL Transcript: \n",
      "'but I'm less than 5 minutes the staircase groaned when he's an extraordinary way' \n",
      "with a confidence_score of: 0.8512189984321594\n",
      "\n",
      "\n",
      "RE-RANKED Transcript: \n",
      "'but in less than 5 minutes the staircase groaned when he's an extraordinary wait' \n",
      "with a confidence_score of: 0.8148280790075659\n",
      "\n",
      "\n",
      "GROUND TRUTH TRANSCRIPT: \n",
      "BUT IN LESS THAN FIVE MINUTES THE STAIRCASE GROANED BENEATH AN EXTRAORDINARY WEIGHT\n",
      "\n",
      "The original transcript was RE-RANKED. The transcripts do not match!\n",
      "Differences between original and re-ranked:  ['i', \"'m\", 'way']\n",
      "\n",
      "\n",
      "The original transcript DOES NOT MATCH ground truth.\n",
      "Differences between original and ground truth:  ['5', 'when', \"'m\", 'he', \"'s\", 'i', 'way']\n",
      "\n",
      "\n",
      "The RE_RANKED transcript DOES NOT MATCH ground truth.\n",
      "Differences between Reranked and ground truth:  ['wait', '5', \"'s\", 'when', 'he']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL Edit Distance: \n",
      "18\n",
      "RE-RANKED Edit Distance: \n",
      "14\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for operation to complete...\n",
      "\n",
      "\n",
      "RE-RANKED Results: \n",
      " {'and the cry issued from his pores if we made us speak a cry frightful and its silence': 0.8229485416784883, 'and the cry issued from his pores if we made the speak a cry frightful and its silence': 0.81755691375583417, 'and the cry issued from his pores if we may the speak a cry frightful and its silence': 0.80107996519655, 'and the cry issued from his pores if we made the speak a cry frightful and it silence': 0.77812602724879987, 'and the cry issued from his pores if we made the speak a cry frightful in it silence': 0.77820050343871117, 'and the cry issued from his pores if we made us speak a cry frightful in its silence': 0.8230056628584862, \"and the cry issued from his pores if we made the speak a cry frightful and it's silence\": 0.82358051147311928, 'and the cry issued from his pores if we may the speak a cry frightful in it silence': 0.76172350123524668, 'and the cry issued from his pores if we made the speak a cry frightful in its silence': 0.81761403474956751, 'and the cry issued from his pores if we may the speak a cry frightful in its silence': 0.80190430544316771}\n",
      "\n",
      "\n",
      "ORIGINAL Transcript: \n",
      "'and the cry issued from his pores if we made us speak a cry frightful in its silence' \n",
      "with a confidence_score of: 0.9122896194458008\n",
      "\n",
      "\n",
      "RE-RANKED Transcript: \n",
      "'and the cry issued from his pores if we made the speak a cry frightful and it's silence' \n",
      "with a confidence_score of: 0.8235805114731193\n",
      "\n",
      "\n",
      "GROUND TRUTH TRANSCRIPT: \n",
      "AND THE CRY ISSUED FROM HIS PORES IF WE MAY THUS SPEAK A CRY FRIGHTFUL IN ITS SILENCE\n",
      "\n",
      "The original transcript was RE-RANKED. The transcripts do not match!\n",
      "Differences between original and re-ranked:  ['us', 'in', 'its']\n",
      "\n",
      "\n",
      "The original transcript DOES NOT MATCH ground truth.\n",
      "Differences between original and ground truth:  ['us', 'made']\n",
      "\n",
      "\n",
      "The RE_RANKED transcript DOES NOT MATCH ground truth.\n",
      "Differences between Reranked and ground truth:  ['it', \"'s\", 'made']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL Edit Distance: \n",
      "4\n",
      "RE-RANKED Edit Distance: \n",
      "7\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each word in the evaluation list:\n",
    "# Select word and determine its frequency distribution\n",
    "# Grab probability of second word in the list\n",
    "# Continue this process until the sentence is scored\n",
    "\n",
    "# Add small epsilon value to avoid division by zero\n",
    "epsilon = 0.0000001\n",
    "\n",
    "# Loads the audio into memory\n",
    "for audio, ground_truth in audio_files.items():\n",
    "    with io.open(audio, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "        audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code='en-US',\n",
    "        max_alternatives=10,\n",
    "        profanity_filter=False,\n",
    "        enable_word_time_offsets=True)\n",
    "\n",
    "    # Detects speech and words in the audio file\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "\n",
    "    print('Waiting for operation to complete...')\n",
    "    result = operation.result(timeout=90)\n",
    "\n",
    "    alternatives = result.results[0].alternatives\n",
    "\n",
    "\n",
    "    #print(\"API Results: \", alternatives)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    rerank_results = {}\n",
    "    for alternative in alternatives:\n",
    "        sent = alternative.transcript\n",
    "\n",
    "        words = nltk.tokenize.word_tokenize(sent)\n",
    "        probs = np.ones_like(words, dtype=np.float32)*epsilon\n",
    "        # print(words,'\\n',probs)\n",
    "        for word in words:\n",
    "            if words.index(word) < len(words)-1: \n",
    "                freq = cfreq_2gram[word].freq(words[words.index(word)+1])\n",
    "                probs[words.index(word)] = freq\n",
    "            # print(probs)\n",
    "\n",
    "        lexicon_score = np.sum(probs)\n",
    "        # print(word_score)\n",
    "\n",
    "        # Re-rank alternatives using a weighted average of the two scores\n",
    "        api_weight = 0.90\n",
    "        confidence_score = alternative.confidence*api_weight + lexicon_score*(1-api_weight)\n",
    "        rerank_results[alternative.transcript] = confidence_score\n",
    "\n",
    "    print(\"RE-RANKED Results: \\n\", rerank_results)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    import operator\n",
    "    index, value = max(enumerate(list(rerank_results.values())), key=operator.itemgetter(1))\n",
    "    # Select Corresponding Transcript:\n",
    "    script=''\n",
    "    for trnscript, confidence in rerank_results.items():\n",
    "        if confidence == value:\n",
    "            script = trnscript\n",
    "\n",
    "    # Evaluate the differences between the Original and the Reranked transcript:\n",
    "    print(\"ORIGINAL Transcript: \\n'{0}' \\nwith a confidence_score of: {1}\".format(alternative.transcript, alternative.confidence))\n",
    "    \n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(\"RE-RANKED Transcript: \\n'{0}' \\nwith a confidence_score of: {1}\".format(script, value))\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(\"GROUND TRUTH TRANSCRIPT: \\n{0}\".format(ground_truth))\n",
    "    print()\n",
    "    ranked_differences = list(set(nltk.tokenize.word_tokenize(alternative.transcript.lower())) -\n",
    "                              set(nltk.tokenize.word_tokenize(script.lower())))\n",
    "    if len(ranked_differences) == 0:  \n",
    "        print(\"No reranking was performed. The transcripts match!\")\n",
    "    else:\n",
    "        print(\"The original transcript was RE-RANKED. The transcripts do not match!\")\n",
    "        print(\"Differences between original and re-ranked: \", ranked_differences)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    # Evaluate Differences between the Original and Ground Truth:\n",
    "    gt_orig_diff = list(set(nltk.tokenize.word_tokenize(alternative.transcript.lower())) -\n",
    "                              set(nltk.tokenize.word_tokenize(ground_truth.lower())))\n",
    "    if len(gt_orig_diff) == 0:  \n",
    "        print(\"The ORIGINAL transcript matches ground truth!\")\n",
    "    else:\n",
    "        print(\"The original transcript DOES NOT MATCH ground truth.\")\n",
    "        print(\"Differences between original and ground truth: \", gt_orig_diff)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    gt_rr_diff = list(set(nltk.tokenize.word_tokenize(script.lower())) -\n",
    "                              set(nltk.tokenize.word_tokenize(ground_truth.lower())))\n",
    "    if len(gt_rr_diff) == 0:  \n",
    "        print(\"The RE-RANKED transcript matches ground truth!\")\n",
    "    else:\n",
    "        print(\"The RE_RANKED transcript DOES NOT MATCH ground truth.\")\n",
    "        print(\"Differences between Reranked and ground truth: \", gt_rr_diff)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # Compute the Levenshtein Distance (a.k.a. Edit Distance)\n",
    "#     import nltk.metrics.distance as lev_dist\n",
    "    \n",
    "    # Google API Edit Distance\n",
    "    goog_edit_distance = nltk.edit_distance(alternative.transcript.lower(), ground_truth.lower())\n",
    "    \n",
    "    # Re-Ranked Edit Distance\n",
    "    rr_edit_distance = nltk.edit_distance(script.lower(), ground_truth.lower())\n",
    "\n",
    "    \n",
    "    print(\"ORIGINAL Edit Distance: \\n{0}\".format(goog_edit_distance))\n",
    "    print(\"RE-RANKED Edit Distance: \\n{0}\".format(rr_edit_distance))\n",
    "    print()\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate N-Gram Model on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 97 text files in the directory: /src/lexicon/LibriSpeech/dev-clean/**/*.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_DeadlineExceededError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.5/site-packages/google/gax/retry.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mto_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_timeout_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mto_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.5/site-packages/google/gax/retry.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mupdated_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0ma_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mupdated_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.5/site-packages/google/gax/__init__.py\u001b[0m in \u001b[0;36m_done_check\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0m_DeadlineExceededError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_DeadlineExceededError\u001b[0m: Deadline Exceeded",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ae178e66cb33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Detects speech and words in the audio file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0moperation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong_running_recognize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0malternatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malternatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.5/site-packages/google/gax/__init__.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \"\"\"\n\u001b[1;32m    594\u001b[0m         \u001b[0;31m# Check exceptional case: raise if no response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHasField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mGaxError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_operation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.5/site-packages/google/gax/__init__.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;31m# Start polling, and return the final result from `_done_check`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mretryable_done_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.5/site-packages/google/gax/retry.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# expected delay.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0mto_sleep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelay\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_sleep\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0m_MILLIS_PER_SECOND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0mdelay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdelay_mult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_delay_millis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Gather all samples, load into dictionary\n",
    "# Prepare a plain text corpus from which we train a languague model\n",
    "import glob\n",
    "import operator\n",
    "\n",
    "# Gather all text files from directory\n",
    "WORKING_DIRECTORY = os.path.join(os.getcwd(),'LibriSpeech/')\n",
    "\n",
    "dev_path = \"{}{}{}{}\".format(WORKING_DIRECTORY, 'dev-clean/', '**/', '*.txt')\n",
    "train_path = \"{}{}{}{}{}\".format(WORKING_DIRECTORY, 'books/', 'utf-8/', '**/', '*.txt*')\n",
    "\n",
    "text_paths = sorted(glob.glob(dev_path, recursive=True))\n",
    "print('Found',len(text_paths),'text files in the directory:', dev_path)\n",
    "\n",
    "transcripts = {}\n",
    "for document in text_paths:\n",
    "    with codecs.open(document, 'r', 'utf-8') as filep:\n",
    "        for i,line in enumerate(filep):\n",
    "            transcripts[line.split()[0]] = ' '.join(line.split()[1:])\n",
    "\n",
    "## Evaluate all samples found ##\n",
    "cloud_speech_api_accuracy = []\n",
    "custom_lang_model_accuracy = []\n",
    "epsilon = 0.000000001\n",
    "api_weight = 0.85\n",
    "steps = 0\n",
    "# Pull In Audio File\n",
    "for filename, gt_transcript in transcripts.items():\n",
    "    steps += 1\n",
    "    dirs = filename.split('-')\n",
    "    \n",
    "    audio_filepath = dev_file_name_0 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    dirs[0],\n",
    "    dirs[1],\n",
    "    \"{0}.flac\".format(filename))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Load the audio into memory\n",
    "    with io.open(audio_filepath, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "        audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code='en-US',\n",
    "        max_alternatives=10,\n",
    "        profanity_filter=False,\n",
    "        enable_word_time_offsets=True)\n",
    "\n",
    "    # Detects speech and words in the audio file\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "    result = operation.result(timeout=90)\n",
    "    alternatives = result.results[0].alternatives\n",
    "\n",
    "\n",
    "    # Evaluate API Results for Re-Ranking:\n",
    "    rerank_results = {}\n",
    "    for alternative in alternatives:\n",
    "        sent = alternative.transcript\n",
    "        \n",
    "        # Strip punctuation\n",
    "        translate_table = dict((ord(char), None) for char in string.punctuation)        \n",
    "        sent = sent.translate(translate_table) # remove punctuations\n",
    "\n",
    "        words = nltk.tokenize.word_tokenize(sent)\n",
    "        probs = np.ones_like(words, dtype=np.float32)*epsilon\n",
    "\n",
    "        for word in words:\n",
    "            if words.index(word) < len(words)-1: \n",
    "                freq = cfreq_2gram[word].freq(words[words.index(word)+1])\n",
    "                probs[words.index(word)] = freq\n",
    "\n",
    "        lexicon_score = np.sum(probs)\n",
    "\n",
    "        # Re-rank alternatives using a weighted average of the two scores\n",
    "        confidence_score = alternative.confidence*api_weight + lexicon_score*(1-api_weight)\n",
    "        rerank_results[alternative.transcript] = confidence_score\n",
    "\n",
    "\n",
    "    \n",
    "    index, value = max(enumerate(list(rerank_results.values())), key=operator.itemgetter(1))\n",
    "    # Select Corresponding Transcript:\n",
    "    script=''\n",
    "    for trnscript, confidence in rerank_results.items():\n",
    "        if confidence == value:\n",
    "            script = trnscript\n",
    "                \n",
    "    # Compute the Accuracy, based on the Levenshtein Distance (a.k.a. Edit Distance)\n",
    "    gcs_ed = nltk.edit_distance(alternative.transcript.lower(), gt_transcript.lower())\n",
    "    gcs_upper_bound = max(len(alternative.transcript),len(gt_transcript))\n",
    "    gcs_accuracy = (1.0 - gcs_ed/gcs_upper_bound)\n",
    "    \n",
    "    clm_ed = nltk.edit_distance(script.lower(), gt_transcript.lower())\n",
    "    clm_upper_bound = max(len(script),len(gt_transcript))\n",
    "    clm_accuracy = (1.0 - clm_ed/clm_upper_bound)\n",
    "    \n",
    "    cloud_speech_api_accuracy.append(gcs_accuracy)\n",
    "    custom_lang_model_accuracy.append(clm_accuracy)\n",
    "\n",
    "    if steps % 100 == 0:\n",
    "        print(\"{0} Transcripts Processed.\".format(steps))\n",
    "        print('Average API Accuracy:', np.mean(cloud_speech_api_accuracy))\n",
    "        print('Average Custom Model Accuracy:', np.mean(custom_lang_model_accuracy))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use other TED speeches for building test set\n",
    "test_speeches = {}\n",
    "for segments in stm_segments:\n",
    "    for segment in segments:\n",
    "        segment_key = \"{0}_{1}_{2}\".format(segment.speaker_id.strip(), str(segment.start_time).replace('.','_'),\n",
    "                                          str(segment.stop_time).replace('.','_'))\n",
    "\n",
    "        speech = None\n",
    "        # If not already exist\n",
    "        if segment.speaker_id not in test_speeches.keys():\n",
    "            # Connect to Cloud API to get Candidate Transcripts\n",
    "            source_file = os.path.join(os.getcwd(), 'TEDLIUM_release1', 'train','sph', '{}.sph'.format(segment.filename))\n",
    "            speech = Speech(speaker_id=segment.speaker_id,\n",
    "                                           speech_id = segment_key,\n",
    "                                           source_file=source_file,\n",
    "                                           ground_truth = ' '.join(segment.transcript.split()[:-1]),\n",
    "                                           start = segment.start_time,\n",
    "                                           stop = segment.stop_time,\n",
    "                                           audio_type = 'LINEAR16')\n",
    "        else:\n",
    "            speech = test_speeches[segment.speaker_id.strip()]\n",
    "            print('Already found speech in list at location: ', speech)\n",
    "        \n",
    "        \n",
    "        \n",
    "        test_speeches[segment_key] = speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Cloud Speech API Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_size(audio_filepath):\n",
    "    statinfo = os.stat(audio_filepath)\n",
    "    return statinfo.st_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcs_api_wrapper import GCSWrapper\n",
    "\n",
    "speaker_id, lexicon = list(lexicons.items())[0]\n",
    "gcs = GCSWrapper()\n",
    "cache_directory = os.path.join(os.getcwd(), 'datacache', 'speech_objects')\n",
    "for speech_id, speech in test_speeches.items():\n",
    "    # Not already saved in prepocess cache\n",
    "    cache_file = os.path.join(cache_directory,'{}_preprocess.p'.format(speech.speech_id))\n",
    "    if not speech.candidate_transcripts: \n",
    "        size = get_audio_size(speech.audio_file)\n",
    "        \n",
    "        #TODO: Split large audio file into new files, build new speech objects\n",
    "        if size < 10485760:\n",
    "            try:\n",
    "                result = gcs.transcribe_speech(speech.audio_file)\n",
    "            except:\n",
    "                result = None\n",
    "            if result:\n",
    "                speech.populate_gcs_results(result)\n",
    "                speech.preprocess_and_save()\n",
    "                print('Adding speech with candidate_transcripts to lexicon')\n",
    "                lexicon.add_speech(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM Net and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  500/2400 - Train Accuracy: 0.5570, Validation Accuracy: 0.6339, Loss: 3.0404\n",
      "Epoch   0 Batch 1000/2400 - Train Accuracy: 0.3839, Validation Accuracy: 0.6339, Loss: 4.5766\n",
      "Epoch   0 Batch 1500/2400 - Train Accuracy: 0.6307, Validation Accuracy: 0.6339, Loss: 2.7718\n",
      "Epoch   0 Batch 2000/2400 - Train Accuracy: 0.3582, Validation Accuracy: 0.6339, Loss: 4.4786\n",
      "Epoch   1 Batch  500/2400 - Train Accuracy: 0.6232, Validation Accuracy: 0.6339, Loss: 2.5335\n",
      "Epoch   1 Batch 1000/2400 - Train Accuracy: 0.4174, Validation Accuracy: 0.6339, Loss: 3.7214\n",
      "Epoch   1 Batch 1500/2400 - Train Accuracy: 0.6705, Validation Accuracy: 0.6339, Loss: 2.1842\n",
      "Epoch   1 Batch 2000/2400 - Train Accuracy: 0.4183, Validation Accuracy: 0.6339, Loss: 3.9006\n",
      "Epoch   2 Batch  500/2400 - Train Accuracy: 0.6415, Validation Accuracy: 0.6339, Loss: 2.2166\n",
      "Epoch   2 Batch 1000/2400 - Train Accuracy: 0.4576, Validation Accuracy: 0.6339, Loss: 3.2150\n",
      "Epoch   2 Batch 1500/2400 - Train Accuracy: 0.6903, Validation Accuracy: 0.6339, Loss: 1.9266\n",
      "Epoch   2 Batch 2000/2400 - Train Accuracy: 0.4495, Validation Accuracy: 0.6339, Loss: 3.4746\n",
      "Epoch   3 Batch  500/2400 - Train Accuracy: 0.6728, Validation Accuracy: 0.6339, Loss: 2.0024\n",
      "Epoch   3 Batch 1000/2400 - Train Accuracy: 0.4777, Validation Accuracy: 0.6339, Loss: 2.8544\n",
      "Epoch   3 Batch 1500/2400 - Train Accuracy: 0.7216, Validation Accuracy: 0.6339, Loss: 1.6540\n",
      "Epoch   3 Batch 2000/2400 - Train Accuracy: 0.4567, Validation Accuracy: 0.6339, Loss: 3.1523\n",
      "Epoch   4 Batch  500/2400 - Train Accuracy: 0.6783, Validation Accuracy: 0.6339, Loss: 1.8692\n",
      "Epoch   4 Batch 1000/2400 - Train Accuracy: 0.4777, Validation Accuracy: 0.6339, Loss: 2.5700\n",
      "Epoch   4 Batch 1500/2400 - Train Accuracy: 0.7301, Validation Accuracy: 0.6339, Loss: 1.4682\n",
      "Epoch   4 Batch 2000/2400 - Train Accuracy: 0.4736, Validation Accuracy: 0.6339, Loss: 2.8515\n",
      "Epoch   5 Batch  500/2400 - Train Accuracy: 0.6893, Validation Accuracy: 0.6339, Loss: 1.7067\n",
      "Epoch   5 Batch 1000/2400 - Train Accuracy: 0.4933, Validation Accuracy: 0.6339, Loss: 2.3715\n",
      "Epoch   5 Batch 1500/2400 - Train Accuracy: 0.7500, Validation Accuracy: 0.6339, Loss: 1.2849\n",
      "Epoch   5 Batch 2000/2400 - Train Accuracy: 0.4808, Validation Accuracy: 0.6339, Loss: 2.6013\n",
      "Epoch   6 Batch  500/2400 - Train Accuracy: 0.6985, Validation Accuracy: 0.6339, Loss: 1.5452\n",
      "Epoch   6 Batch 1000/2400 - Train Accuracy: 0.5379, Validation Accuracy: 0.6339, Loss: 2.1082\n",
      "Epoch   6 Batch 1500/2400 - Train Accuracy: 0.7585, Validation Accuracy: 0.6339, Loss: 1.1517\n",
      "Epoch   6 Batch 2000/2400 - Train Accuracy: 0.4928, Validation Accuracy: 0.6339, Loss: 2.3818\n",
      "Epoch   7 Batch  500/2400 - Train Accuracy: 0.6912, Validation Accuracy: 0.6339, Loss: 1.4616\n",
      "Epoch   7 Batch 1000/2400 - Train Accuracy: 0.5312, Validation Accuracy: 0.6339, Loss: 1.9536\n",
      "Epoch   7 Batch 1500/2400 - Train Accuracy: 0.7699, Validation Accuracy: 0.6339, Loss: 1.0182\n",
      "Epoch   7 Batch 2000/2400 - Train Accuracy: 0.5192, Validation Accuracy: 0.6339, Loss: 2.1623\n",
      "Epoch   8 Batch  500/2400 - Train Accuracy: 0.6967, Validation Accuracy: 0.6362, Loss: 1.3247\n",
      "Epoch   8 Batch 1000/2400 - Train Accuracy: 0.5781, Validation Accuracy: 0.6339, Loss: 1.7735\n",
      "Epoch   8 Batch 1500/2400 - Train Accuracy: 0.7926, Validation Accuracy: 0.6339, Loss: 0.8861\n",
      "Epoch   8 Batch 2000/2400 - Train Accuracy: 0.5433, Validation Accuracy: 0.6339, Loss: 1.9498\n",
      "Epoch   9 Batch  500/2400 - Train Accuracy: 0.7096, Validation Accuracy: 0.6339, Loss: 1.2163\n",
      "Epoch   9 Batch 1000/2400 - Train Accuracy: 0.6049, Validation Accuracy: 0.6339, Loss: 1.5981\n",
      "Epoch   9 Batch 1500/2400 - Train Accuracy: 0.8125, Validation Accuracy: 0.6339, Loss: 0.7869\n",
      "Epoch   9 Batch 2000/2400 - Train Accuracy: 0.5553, Validation Accuracy: 0.6339, Loss: 1.7822\n",
      "Epoch  10 Batch  500/2400 - Train Accuracy: 0.7243, Validation Accuracy: 0.6339, Loss: 1.1307\n",
      "Epoch  10 Batch 1000/2400 - Train Accuracy: 0.6406, Validation Accuracy: 0.6339, Loss: 1.4611\n",
      "Epoch  10 Batch 1500/2400 - Train Accuracy: 0.8295, Validation Accuracy: 0.6339, Loss: 0.6797\n",
      "Epoch  10 Batch 2000/2400 - Train Accuracy: 0.5938, Validation Accuracy: 0.6339, Loss: 1.6302\n",
      "Epoch  11 Batch  500/2400 - Train Accuracy: 0.7463, Validation Accuracy: 0.6339, Loss: 1.0599\n",
      "Epoch  11 Batch 1000/2400 - Train Accuracy: 0.6629, Validation Accuracy: 0.6339, Loss: 1.3507\n",
      "Epoch  11 Batch 1500/2400 - Train Accuracy: 0.8580, Validation Accuracy: 0.6339, Loss: 0.5859\n",
      "Epoch  11 Batch 2000/2400 - Train Accuracy: 0.6034, Validation Accuracy: 0.6339, Loss: 1.5344\n",
      "Epoch  12 Batch  500/2400 - Train Accuracy: 0.7610, Validation Accuracy: 0.6339, Loss: 0.9842\n",
      "Epoch  12 Batch 1000/2400 - Train Accuracy: 0.6473, Validation Accuracy: 0.6339, Loss: 1.2971\n",
      "Epoch  12 Batch 1500/2400 - Train Accuracy: 0.8580, Validation Accuracy: 0.6339, Loss: 0.5255\n",
      "Epoch  12 Batch 2000/2400 - Train Accuracy: 0.6010, Validation Accuracy: 0.6339, Loss: 1.3935\n",
      "Epoch  13 Batch  500/2400 - Train Accuracy: 0.7721, Validation Accuracy: 0.6339, Loss: 0.8974\n",
      "Epoch  13 Batch 1000/2400 - Train Accuracy: 0.6629, Validation Accuracy: 0.6339, Loss: 1.2005\n",
      "Epoch  13 Batch 1500/2400 - Train Accuracy: 0.8949, Validation Accuracy: 0.6339, Loss: 0.4541\n",
      "Epoch  13 Batch 2000/2400 - Train Accuracy: 0.6298, Validation Accuracy: 0.6339, Loss: 1.2525\n",
      "Epoch  14 Batch  500/2400 - Train Accuracy: 0.7923, Validation Accuracy: 0.6339, Loss: 0.8303\n",
      "Epoch  14 Batch 1000/2400 - Train Accuracy: 0.6719, Validation Accuracy: 0.6339, Loss: 1.0979\n",
      "Epoch  14 Batch 1500/2400 - Train Accuracy: 0.8778, Validation Accuracy: 0.6339, Loss: 0.3918\n",
      "Epoch  14 Batch 2000/2400 - Train Accuracy: 0.6635, Validation Accuracy: 0.6339, Loss: 1.1165\n",
      "Epoch  15 Batch  500/2400 - Train Accuracy: 0.8015, Validation Accuracy: 0.6339, Loss: 0.8186\n",
      "Epoch  15 Batch 1000/2400 - Train Accuracy: 0.7411, Validation Accuracy: 0.6339, Loss: 0.9573\n",
      "Epoch  15 Batch 1500/2400 - Train Accuracy: 0.9034, Validation Accuracy: 0.6339, Loss: 0.3815\n",
      "Epoch  15 Batch 2000/2400 - Train Accuracy: 0.6322, Validation Accuracy: 0.6339, Loss: 1.0469\n",
      "Epoch  16 Batch  500/2400 - Train Accuracy: 0.8088, Validation Accuracy: 0.6339, Loss: 0.7384\n",
      "Epoch  16 Batch 1000/2400 - Train Accuracy: 0.6853, Validation Accuracy: 0.6339, Loss: 0.8702\n",
      "Epoch  16 Batch 1500/2400 - Train Accuracy: 0.9062, Validation Accuracy: 0.6339, Loss: 0.3387\n",
      "Epoch  16 Batch 2000/2400 - Train Accuracy: 0.6779, Validation Accuracy: 0.6339, Loss: 0.9736\n",
      "Epoch  17 Batch  500/2400 - Train Accuracy: 0.8180, Validation Accuracy: 0.6339, Loss: 0.7337\n",
      "Epoch  17 Batch 1000/2400 - Train Accuracy: 0.7634, Validation Accuracy: 0.6339, Loss: 0.8291\n",
      "Epoch  17 Batch 1500/2400 - Train Accuracy: 0.9318, Validation Accuracy: 0.6339, Loss: 0.2843\n",
      "Epoch  17 Batch 2000/2400 - Train Accuracy: 0.6971, Validation Accuracy: 0.6339, Loss: 0.9246\n",
      "Epoch  18 Batch  500/2400 - Train Accuracy: 0.7941, Validation Accuracy: 0.6339, Loss: 0.6937\n",
      "Epoch  18 Batch 1000/2400 - Train Accuracy: 0.7812, Validation Accuracy: 0.6339, Loss: 0.7610\n",
      "Epoch  18 Batch 1500/2400 - Train Accuracy: 0.9347, Validation Accuracy: 0.6339, Loss: 0.2580\n",
      "Epoch  18 Batch 2000/2400 - Train Accuracy: 0.7019, Validation Accuracy: 0.6339, Loss: 0.8523\n",
      "Epoch  19 Batch  500/2400 - Train Accuracy: 0.8327, Validation Accuracy: 0.6339, Loss: 0.6111\n",
      "Epoch  19 Batch 1000/2400 - Train Accuracy: 0.7746, Validation Accuracy: 0.6339, Loss: 0.7092\n",
      "Epoch  19 Batch 1500/2400 - Train Accuracy: 0.9432, Validation Accuracy: 0.6339, Loss: 0.2428\n",
      "Epoch  19 Batch 2000/2400 - Train Accuracy: 0.7620, Validation Accuracy: 0.6339, Loss: 0.7962\n",
      "Epoch  20 Batch  500/2400 - Train Accuracy: 0.8438, Validation Accuracy: 0.6339, Loss: 0.5840\n",
      "Epoch  20 Batch 1000/2400 - Train Accuracy: 0.7522, Validation Accuracy: 0.6339, Loss: 0.6448\n",
      "Epoch  20 Batch 1500/2400 - Train Accuracy: 0.9460, Validation Accuracy: 0.6362, Loss: 0.2140\n",
      "Epoch  20 Batch 2000/2400 - Train Accuracy: 0.7500, Validation Accuracy: 0.6362, Loss: 0.7080\n",
      "Epoch  21 Batch  500/2400 - Train Accuracy: 0.8640, Validation Accuracy: 0.6339, Loss: 0.5800\n",
      "Epoch  21 Batch 1000/2400 - Train Accuracy: 0.7589, Validation Accuracy: 0.6339, Loss: 0.6473\n",
      "Epoch  21 Batch 1500/2400 - Train Accuracy: 0.9602, Validation Accuracy: 0.6339, Loss: 0.1847\n",
      "Epoch  21 Batch 2000/2400 - Train Accuracy: 0.7139, Validation Accuracy: 0.6384, Loss: 0.6637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  22 Batch  500/2400 - Train Accuracy: 0.8585, Validation Accuracy: 0.6339, Loss: 0.5283\n",
      "Epoch  22 Batch 1000/2400 - Train Accuracy: 0.7723, Validation Accuracy: 0.6339, Loss: 0.5823\n",
      "Epoch  22 Batch 1500/2400 - Train Accuracy: 0.9489, Validation Accuracy: 0.6339, Loss: 0.1528\n",
      "Epoch  22 Batch 2000/2400 - Train Accuracy: 0.7788, Validation Accuracy: 0.6384, Loss: 0.6134\n",
      "Epoch  23 Batch  500/2400 - Train Accuracy: 0.8493, Validation Accuracy: 0.6339, Loss: 0.4685\n",
      "Epoch  23 Batch 1000/2400 - Train Accuracy: 0.7969, Validation Accuracy: 0.6339, Loss: 0.5515\n",
      "Epoch  23 Batch 1500/2400 - Train Accuracy: 0.9716, Validation Accuracy: 0.6339, Loss: 0.1543\n",
      "Epoch  23 Batch 2000/2400 - Train Accuracy: 0.7620, Validation Accuracy: 0.6362, Loss: 0.5848\n",
      "Epoch  24 Batch  500/2400 - Train Accuracy: 0.8676, Validation Accuracy: 0.6339, Loss: 0.4613\n",
      "Epoch  24 Batch 1000/2400 - Train Accuracy: 0.8013, Validation Accuracy: 0.6339, Loss: 0.5133\n",
      "Epoch  24 Batch 1500/2400 - Train Accuracy: 0.9631, Validation Accuracy: 0.6339, Loss: 0.1362\n",
      "Epoch  24 Batch 2000/2400 - Train Accuracy: 0.8125, Validation Accuracy: 0.6362, Loss: 0.5467\n",
      "Epoch  25 Batch  500/2400 - Train Accuracy: 0.8566, Validation Accuracy: 0.6339, Loss: 0.4405\n",
      "Epoch  25 Batch 1000/2400 - Train Accuracy: 0.8371, Validation Accuracy: 0.6339, Loss: 0.4634\n",
      "Epoch  25 Batch 1500/2400 - Train Accuracy: 0.9716, Validation Accuracy: 0.6339, Loss: 0.1353\n",
      "Epoch  25 Batch 2000/2400 - Train Accuracy: 0.7812, Validation Accuracy: 0.6384, Loss: 0.5157\n",
      "Epoch  26 Batch  500/2400 - Train Accuracy: 0.8511, Validation Accuracy: 0.6339, Loss: 0.4270\n",
      "Epoch  26 Batch 1000/2400 - Train Accuracy: 0.8460, Validation Accuracy: 0.6339, Loss: 0.4068\n",
      "Epoch  26 Batch 1500/2400 - Train Accuracy: 0.9659, Validation Accuracy: 0.6339, Loss: 0.1237\n",
      "Epoch  26 Batch 2000/2400 - Train Accuracy: 0.8149, Validation Accuracy: 0.6384, Loss: 0.4771\n",
      "Epoch  27 Batch  500/2400 - Train Accuracy: 0.8511, Validation Accuracy: 0.6339, Loss: 0.4089\n",
      "Epoch  27 Batch 1000/2400 - Train Accuracy: 0.8504, Validation Accuracy: 0.6339, Loss: 0.3670\n",
      "Epoch  27 Batch 1500/2400 - Train Accuracy: 0.9688, Validation Accuracy: 0.6362, Loss: 0.0948\n",
      "Epoch  27 Batch 2000/2400 - Train Accuracy: 0.8269, Validation Accuracy: 0.6384, Loss: 0.4333\n",
      "Epoch  28 Batch  500/2400 - Train Accuracy: 0.8658, Validation Accuracy: 0.6362, Loss: 0.3985\n",
      "Epoch  28 Batch 1000/2400 - Train Accuracy: 0.8348, Validation Accuracy: 0.6339, Loss: 0.3839\n",
      "Epoch  28 Batch 1500/2400 - Train Accuracy: 0.9773, Validation Accuracy: 0.6339, Loss: 0.1016\n",
      "Epoch  28 Batch 2000/2400 - Train Accuracy: 0.8029, Validation Accuracy: 0.6362, Loss: 0.4031\n",
      "Epoch  29 Batch  500/2400 - Train Accuracy: 0.8787, Validation Accuracy: 0.6339, Loss: 0.3768\n",
      "Epoch  29 Batch 1000/2400 - Train Accuracy: 0.8415, Validation Accuracy: 0.6362, Loss: 0.3458\n",
      "Epoch  29 Batch 1500/2400 - Train Accuracy: 0.9830, Validation Accuracy: 0.6339, Loss: 0.0966\n",
      "Epoch  29 Batch 2000/2400 - Train Accuracy: 0.8438, Validation Accuracy: 0.6362, Loss: 0.3798\n",
      "Epoch  30 Batch  500/2400 - Train Accuracy: 0.8676, Validation Accuracy: 0.6339, Loss: 0.3389\n",
      "Epoch  30 Batch 1000/2400 - Train Accuracy: 0.8571, Validation Accuracy: 0.6362, Loss: 0.3212\n",
      "Epoch  30 Batch 1500/2400 - Train Accuracy: 0.9744, Validation Accuracy: 0.6362, Loss: 0.0786\n",
      "Epoch  30 Batch 2000/2400 - Train Accuracy: 0.8822, Validation Accuracy: 0.6362, Loss: 0.3821\n",
      "Epoch  31 Batch  500/2400 - Train Accuracy: 0.8732, Validation Accuracy: 0.6362, Loss: 0.3289\n",
      "Epoch  31 Batch 1000/2400 - Train Accuracy: 0.8415, Validation Accuracy: 0.6339, Loss: 0.3226\n",
      "Epoch  31 Batch 1500/2400 - Train Accuracy: 0.9886, Validation Accuracy: 0.6339, Loss: 0.0878\n",
      "Epoch  31 Batch 2000/2400 - Train Accuracy: 0.8606, Validation Accuracy: 0.6384, Loss: 0.3134\n",
      "Epoch  32 Batch  500/2400 - Train Accuracy: 0.8603, Validation Accuracy: 0.6339, Loss: 0.3268\n",
      "Epoch  32 Batch 1000/2400 - Train Accuracy: 0.9040, Validation Accuracy: 0.6339, Loss: 0.2952\n",
      "Epoch  32 Batch 1500/2400 - Train Accuracy: 0.9773, Validation Accuracy: 0.6339, Loss: 0.0615\n",
      "Epoch  32 Batch 2000/2400 - Train Accuracy: 0.8365, Validation Accuracy: 0.6384, Loss: 0.3058\n",
      "Epoch  33 Batch  500/2400 - Train Accuracy: 0.8879, Validation Accuracy: 0.6339, Loss: 0.3062\n",
      "Epoch  33 Batch 1000/2400 - Train Accuracy: 0.9152, Validation Accuracy: 0.6339, Loss: 0.2599\n",
      "Epoch  33 Batch 1500/2400 - Train Accuracy: 0.9773, Validation Accuracy: 0.6339, Loss: 0.0742\n",
      "Epoch  33 Batch 2000/2400 - Train Accuracy: 0.8702, Validation Accuracy: 0.6339, Loss: 0.3450\n",
      "Epoch  34 Batch  500/2400 - Train Accuracy: 0.8713, Validation Accuracy: 0.6339, Loss: 0.2665\n",
      "Epoch  34 Batch 1000/2400 - Train Accuracy: 0.8996, Validation Accuracy: 0.6339, Loss: 0.2396\n",
      "Epoch  34 Batch 1500/2400 - Train Accuracy: 0.9915, Validation Accuracy: 0.6362, Loss: 0.0561\n",
      "Epoch  34 Batch 2000/2400 - Train Accuracy: 0.8990, Validation Accuracy: 0.6362, Loss: 0.2825\n",
      "Epoch  35 Batch  500/2400 - Train Accuracy: 0.8879, Validation Accuracy: 0.6339, Loss: 0.2788\n",
      "Epoch  35 Batch 1000/2400 - Train Accuracy: 0.9062, Validation Accuracy: 0.6362, Loss: 0.2616\n",
      "Epoch  35 Batch 1500/2400 - Train Accuracy: 0.9830, Validation Accuracy: 0.6339, Loss: 0.0416\n",
      "Epoch  35 Batch 2000/2400 - Train Accuracy: 0.8942, Validation Accuracy: 0.6362, Loss: 0.2956\n",
      "Epoch  36 Batch  500/2400 - Train Accuracy: 0.8952, Validation Accuracy: 0.6339, Loss: 0.2750\n",
      "Epoch  36 Batch 1000/2400 - Train Accuracy: 0.9241, Validation Accuracy: 0.6339, Loss: 0.2404\n",
      "Epoch  36 Batch 1500/2400 - Train Accuracy: 0.9744, Validation Accuracy: 0.6339, Loss: 0.0535\n",
      "Epoch  36 Batch 2000/2400 - Train Accuracy: 0.8678, Validation Accuracy: 0.6362, Loss: 0.2449\n",
      "Epoch  37 Batch  500/2400 - Train Accuracy: 0.8860, Validation Accuracy: 0.6362, Loss: 0.2426\n",
      "Epoch  37 Batch 1000/2400 - Train Accuracy: 0.8973, Validation Accuracy: 0.6339, Loss: 0.2280\n",
      "Epoch  37 Batch 1500/2400 - Train Accuracy: 0.9830, Validation Accuracy: 0.6362, Loss: 0.0763\n",
      "Epoch  37 Batch 2000/2400 - Train Accuracy: 0.8966, Validation Accuracy: 0.6362, Loss: 0.2536\n",
      "Epoch  38 Batch  500/2400 - Train Accuracy: 0.9044, Validation Accuracy: 0.6384, Loss: 0.2438\n",
      "Epoch  38 Batch 1000/2400 - Train Accuracy: 0.8973, Validation Accuracy: 0.6339, Loss: 0.1792\n",
      "Epoch  38 Batch 1500/2400 - Train Accuracy: 0.9716, Validation Accuracy: 0.6362, Loss: 0.0512\n",
      "Epoch  38 Batch 2000/2400 - Train Accuracy: 0.9135, Validation Accuracy: 0.6362, Loss: 0.2638\n",
      "Epoch  39 Batch  500/2400 - Train Accuracy: 0.8952, Validation Accuracy: 0.6339, Loss: 0.2388\n",
      "Epoch  39 Batch 1000/2400 - Train Accuracy: 0.8951, Validation Accuracy: 0.6339, Loss: 0.1637\n",
      "Epoch  39 Batch 1500/2400 - Train Accuracy: 0.9915, Validation Accuracy: 0.6339, Loss: 0.0374\n",
      "Epoch  39 Batch 2000/2400 - Train Accuracy: 0.8918, Validation Accuracy: 0.6362, Loss: 0.2389\n",
      "Epoch  40 Batch  500/2400 - Train Accuracy: 0.9210, Validation Accuracy: 0.6339, Loss: 0.2273\n",
      "Epoch  40 Batch 1000/2400 - Train Accuracy: 0.9196, Validation Accuracy: 0.6339, Loss: 0.1760\n",
      "Epoch  40 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6362, Loss: 0.0464\n",
      "Epoch  40 Batch 2000/2400 - Train Accuracy: 0.8726, Validation Accuracy: 0.6362, Loss: 0.2212\n",
      "Epoch  41 Batch  500/2400 - Train Accuracy: 0.8787, Validation Accuracy: 0.6384, Loss: 0.2444\n",
      "Epoch  41 Batch 1000/2400 - Train Accuracy: 0.9353, Validation Accuracy: 0.6339, Loss: 0.1539\n",
      "Epoch  41 Batch 1500/2400 - Train Accuracy: 0.9773, Validation Accuracy: 0.6384, Loss: 0.0501\n",
      "Epoch  41 Batch 2000/2400 - Train Accuracy: 0.8606, Validation Accuracy: 0.6362, Loss: 0.1887\n",
      "Epoch  42 Batch  500/2400 - Train Accuracy: 0.9136, Validation Accuracy: 0.6339, Loss: 0.2276\n",
      "Epoch  42 Batch 1000/2400 - Train Accuracy: 0.9375, Validation Accuracy: 0.6339, Loss: 0.1541\n",
      "Epoch  42 Batch 1500/2400 - Train Accuracy: 0.9915, Validation Accuracy: 0.6362, Loss: 0.0365\n",
      "Epoch  42 Batch 2000/2400 - Train Accuracy: 0.8942, Validation Accuracy: 0.6362, Loss: 0.2104\n",
      "Epoch  43 Batch  500/2400 - Train Accuracy: 0.9375, Validation Accuracy: 0.6339, Loss: 0.1884\n",
      "Epoch  43 Batch 1000/2400 - Train Accuracy: 0.9107, Validation Accuracy: 0.6339, Loss: 0.1835\n",
      "Epoch  43 Batch 1500/2400 - Train Accuracy: 0.9972, Validation Accuracy: 0.6362, Loss: 0.0341\n",
      "Epoch  43 Batch 2000/2400 - Train Accuracy: 0.9159, Validation Accuracy: 0.6339, Loss: 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  44 Batch  500/2400 - Train Accuracy: 0.9375, Validation Accuracy: 0.6362, Loss: 0.1641\n",
      "Epoch  44 Batch 1000/2400 - Train Accuracy: 0.9442, Validation Accuracy: 0.6339, Loss: 0.1380\n",
      "Epoch  44 Batch 1500/2400 - Train Accuracy: 0.9943, Validation Accuracy: 0.6339, Loss: 0.0358\n",
      "Epoch  44 Batch 2000/2400 - Train Accuracy: 0.8654, Validation Accuracy: 0.6362, Loss: 0.2172\n",
      "Epoch  45 Batch  500/2400 - Train Accuracy: 0.9191, Validation Accuracy: 0.6384, Loss: 0.1719\n",
      "Epoch  45 Batch 1000/2400 - Train Accuracy: 0.9509, Validation Accuracy: 0.6339, Loss: 0.1356\n",
      "Epoch  45 Batch 1500/2400 - Train Accuracy: 0.9972, Validation Accuracy: 0.6362, Loss: 0.0339\n",
      "Epoch  45 Batch 2000/2400 - Train Accuracy: 0.8942, Validation Accuracy: 0.6339, Loss: 0.1802\n",
      "Epoch  46 Batch  500/2400 - Train Accuracy: 0.9283, Validation Accuracy: 0.6339, Loss: 0.1879\n",
      "Epoch  46 Batch 1000/2400 - Train Accuracy: 0.9129, Validation Accuracy: 0.6339, Loss: 0.1220\n",
      "Epoch  46 Batch 1500/2400 - Train Accuracy: 0.9716, Validation Accuracy: 0.6362, Loss: 0.0290\n",
      "Epoch  46 Batch 2000/2400 - Train Accuracy: 0.9423, Validation Accuracy: 0.6406, Loss: 0.1382\n",
      "Epoch  47 Batch  500/2400 - Train Accuracy: 0.9540, Validation Accuracy: 0.6339, Loss: 0.1799\n",
      "Epoch  47 Batch 1000/2400 - Train Accuracy: 0.9598, Validation Accuracy: 0.6339, Loss: 0.1299\n",
      "Epoch  47 Batch 1500/2400 - Train Accuracy: 0.9972, Validation Accuracy: 0.6339, Loss: 0.0227\n",
      "Epoch  47 Batch 2000/2400 - Train Accuracy: 0.9447, Validation Accuracy: 0.6362, Loss: 0.1260\n",
      "Epoch  48 Batch  500/2400 - Train Accuracy: 0.9338, Validation Accuracy: 0.6339, Loss: 0.1546\n",
      "Epoch  48 Batch 1000/2400 - Train Accuracy: 0.9643, Validation Accuracy: 0.6339, Loss: 0.1172\n",
      "Epoch  48 Batch 1500/2400 - Train Accuracy: 0.9915, Validation Accuracy: 0.6362, Loss: 0.0264\n",
      "Epoch  48 Batch 2000/2400 - Train Accuracy: 0.9303, Validation Accuracy: 0.6339, Loss: 0.1500\n",
      "Epoch  49 Batch  500/2400 - Train Accuracy: 0.9283, Validation Accuracy: 0.6339, Loss: 0.1290\n",
      "Epoch  49 Batch 1000/2400 - Train Accuracy: 0.9286, Validation Accuracy: 0.6339, Loss: 0.0936\n",
      "Epoch  49 Batch 1500/2400 - Train Accuracy: 0.9886, Validation Accuracy: 0.6362, Loss: 0.0314\n",
      "Epoch  49 Batch 2000/2400 - Train Accuracy: 0.9784, Validation Accuracy: 0.6339, Loss: 0.1293\n",
      "Epoch  50 Batch  500/2400 - Train Accuracy: 0.9375, Validation Accuracy: 0.6339, Loss: 0.1285\n",
      "Epoch  50 Batch 1000/2400 - Train Accuracy: 0.9576, Validation Accuracy: 0.6362, Loss: 0.1085\n",
      "Epoch  50 Batch 1500/2400 - Train Accuracy: 0.9830, Validation Accuracy: 0.6339, Loss: 0.0186\n",
      "Epoch  50 Batch 2000/2400 - Train Accuracy: 0.9375, Validation Accuracy: 0.6339, Loss: 0.1443\n",
      "Epoch  51 Batch  500/2400 - Train Accuracy: 0.9577, Validation Accuracy: 0.6362, Loss: 0.1446\n",
      "Epoch  51 Batch 1000/2400 - Train Accuracy: 0.9531, Validation Accuracy: 0.6339, Loss: 0.1003\n",
      "Epoch  51 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6339, Loss: 0.0229\n",
      "Epoch  51 Batch 2000/2400 - Train Accuracy: 0.9231, Validation Accuracy: 0.6339, Loss: 0.1405\n",
      "Epoch  52 Batch  500/2400 - Train Accuracy: 0.9210, Validation Accuracy: 0.6362, Loss: 0.1247\n",
      "Epoch  52 Batch 1000/2400 - Train Accuracy: 0.9866, Validation Accuracy: 0.6362, Loss: 0.0688\n",
      "Epoch  52 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6362, Loss: 0.0247\n",
      "Epoch  52 Batch 2000/2400 - Train Accuracy: 0.9423, Validation Accuracy: 0.6362, Loss: 0.1144\n",
      "Epoch  53 Batch  500/2400 - Train Accuracy: 0.9375, Validation Accuracy: 0.6362, Loss: 0.1256\n",
      "Epoch  53 Batch 1000/2400 - Train Accuracy: 0.9688, Validation Accuracy: 0.6362, Loss: 0.0971\n",
      "Epoch  53 Batch 1500/2400 - Train Accuracy: 0.9773, Validation Accuracy: 0.6339, Loss: 0.0272\n",
      "Epoch  53 Batch 2000/2400 - Train Accuracy: 0.9279, Validation Accuracy: 0.6339, Loss: 0.1503\n",
      "Epoch  54 Batch  500/2400 - Train Accuracy: 0.9614, Validation Accuracy: 0.6339, Loss: 0.1076\n",
      "Epoch  54 Batch 1000/2400 - Train Accuracy: 0.9509, Validation Accuracy: 0.6362, Loss: 0.0895\n",
      "Epoch  54 Batch 1500/2400 - Train Accuracy: 0.9972, Validation Accuracy: 0.6339, Loss: 0.0214\n",
      "Epoch  54 Batch 2000/2400 - Train Accuracy: 0.9663, Validation Accuracy: 0.6362, Loss: 0.1065\n",
      "Epoch  55 Batch  500/2400 - Train Accuracy: 0.9651, Validation Accuracy: 0.6362, Loss: 0.1130\n",
      "Epoch  55 Batch 1000/2400 - Train Accuracy: 0.9844, Validation Accuracy: 0.6339, Loss: 0.0643\n",
      "Epoch  55 Batch 1500/2400 - Train Accuracy: 0.9915, Validation Accuracy: 0.6339, Loss: 0.0216\n",
      "Epoch  55 Batch 2000/2400 - Train Accuracy: 0.9567, Validation Accuracy: 0.6339, Loss: 0.0888\n",
      "Epoch  56 Batch  500/2400 - Train Accuracy: 0.9688, Validation Accuracy: 0.6339, Loss: 0.1098\n",
      "Epoch  56 Batch 1000/2400 - Train Accuracy: 0.9621, Validation Accuracy: 0.6362, Loss: 0.0730\n",
      "Epoch  56 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6339, Loss: 0.0171\n",
      "Epoch  56 Batch 2000/2400 - Train Accuracy: 0.9471, Validation Accuracy: 0.6384, Loss: 0.1046\n",
      "Epoch  57 Batch  500/2400 - Train Accuracy: 0.9430, Validation Accuracy: 0.6339, Loss: 0.1291\n",
      "Epoch  57 Batch 1000/2400 - Train Accuracy: 0.9799, Validation Accuracy: 0.6339, Loss: 0.0745\n",
      "Epoch  57 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6362, Loss: 0.0191\n",
      "Epoch  57 Batch 2000/2400 - Train Accuracy: 0.9399, Validation Accuracy: 0.6362, Loss: 0.1102\n",
      "Epoch  58 Batch  500/2400 - Train Accuracy: 0.9688, Validation Accuracy: 0.6339, Loss: 0.1312\n",
      "Epoch  58 Batch 1000/2400 - Train Accuracy: 0.9732, Validation Accuracy: 0.6339, Loss: 0.0695\n",
      "Epoch  58 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6339, Loss: 0.0175\n",
      "Epoch  58 Batch 2000/2400 - Train Accuracy: 0.9615, Validation Accuracy: 0.6362, Loss: 0.1031\n",
      "Epoch  59 Batch  500/2400 - Train Accuracy: 0.9430, Validation Accuracy: 0.6339, Loss: 0.1139\n",
      "Epoch  59 Batch 1000/2400 - Train Accuracy: 0.9487, Validation Accuracy: 0.6362, Loss: 0.0846\n",
      "Epoch  59 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6339, Loss: 0.0217\n",
      "Epoch  59 Batch 2000/2400 - Train Accuracy: 0.9736, Validation Accuracy: 0.6339, Loss: 0.1029\n",
      "Epoch  60 Batch  500/2400 - Train Accuracy: 0.9412, Validation Accuracy: 0.6339, Loss: 0.1038\n",
      "Epoch  60 Batch 1000/2400 - Train Accuracy: 0.9442, Validation Accuracy: 0.6339, Loss: 0.0980\n",
      "Epoch  60 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6339, Loss: 0.0162\n",
      "Epoch  60 Batch 2000/2400 - Train Accuracy: 0.9519, Validation Accuracy: 0.6362, Loss: 0.1130\n",
      "Epoch  61 Batch  500/2400 - Train Accuracy: 0.9449, Validation Accuracy: 0.6339, Loss: 0.0936\n",
      "Epoch  61 Batch 1000/2400 - Train Accuracy: 0.9710, Validation Accuracy: 0.6339, Loss: 0.0903\n",
      "Epoch  61 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6339, Loss: 0.0223\n",
      "Epoch  61 Batch 2000/2400 - Train Accuracy: 0.9760, Validation Accuracy: 0.6339, Loss: 0.0850\n",
      "Epoch  62 Batch  500/2400 - Train Accuracy: 0.9504, Validation Accuracy: 0.6339, Loss: 0.1195\n",
      "Epoch  62 Batch 1000/2400 - Train Accuracy: 0.9844, Validation Accuracy: 0.6339, Loss: 0.0836\n",
      "Epoch  62 Batch 1500/2400 - Train Accuracy: 0.9943, Validation Accuracy: 0.6339, Loss: 0.0175\n",
      "Epoch  62 Batch 2000/2400 - Train Accuracy: 0.9688, Validation Accuracy: 0.6339, Loss: 0.0639\n",
      "Epoch  63 Batch  500/2400 - Train Accuracy: 0.9375, Validation Accuracy: 0.6339, Loss: 0.0960\n",
      "Epoch  63 Batch 1000/2400 - Train Accuracy: 0.9710, Validation Accuracy: 0.6362, Loss: 0.0803\n",
      "Epoch  63 Batch 1500/2400 - Train Accuracy: 0.9943, Validation Accuracy: 0.6339, Loss: 0.0220\n",
      "Epoch  63 Batch 2000/2400 - Train Accuracy: 0.9760, Validation Accuracy: 0.6339, Loss: 0.1181\n",
      "Epoch  64 Batch  500/2400 - Train Accuracy: 0.9412, Validation Accuracy: 0.6339, Loss: 0.0972\n",
      "Epoch  64 Batch 1000/2400 - Train Accuracy: 0.9821, Validation Accuracy: 0.6339, Loss: 0.0682\n",
      "Epoch  64 Batch 1500/2400 - Train Accuracy: 0.9943, Validation Accuracy: 0.6339, Loss: 0.0232\n",
      "Epoch  64 Batch 2000/2400 - Train Accuracy: 0.9712, Validation Accuracy: 0.6339, Loss: 0.0637\n",
      "Epoch  65 Batch  500/2400 - Train Accuracy: 0.9393, Validation Accuracy: 0.6362, Loss: 0.0818\n",
      "Epoch  65 Batch 1000/2400 - Train Accuracy: 0.9554, Validation Accuracy: 0.6339, Loss: 0.0633\n",
      "Epoch  65 Batch 1500/2400 - Train Accuracy: 0.9886, Validation Accuracy: 0.6339, Loss: 0.0194\n",
      "Epoch  65 Batch 2000/2400 - Train Accuracy: 0.9591, Validation Accuracy: 0.6339, Loss: 0.0715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  66 Batch  500/2400 - Train Accuracy: 0.9393, Validation Accuracy: 0.6339, Loss: 0.0905\n",
      "Epoch  66 Batch 1000/2400 - Train Accuracy: 0.9866, Validation Accuracy: 0.6339, Loss: 0.0568\n",
      "Epoch  66 Batch 1500/2400 - Train Accuracy: 0.9744, Validation Accuracy: 0.6362, Loss: 0.0192\n",
      "Epoch  66 Batch 2000/2400 - Train Accuracy: 0.9760, Validation Accuracy: 0.6339, Loss: 0.0857\n",
      "Epoch  67 Batch  500/2400 - Train Accuracy: 0.9540, Validation Accuracy: 0.6339, Loss: 0.0758\n",
      "Epoch  67 Batch 1000/2400 - Train Accuracy: 0.9777, Validation Accuracy: 0.6339, Loss: 0.0533\n",
      "Epoch  67 Batch 1500/2400 - Train Accuracy: 0.9858, Validation Accuracy: 0.6362, Loss: 0.0218\n",
      "Epoch  67 Batch 2000/2400 - Train Accuracy: 0.9832, Validation Accuracy: 0.6339, Loss: 0.0672\n",
      "Epoch  68 Batch  500/2400 - Train Accuracy: 0.9467, Validation Accuracy: 0.6339, Loss: 0.0947\n",
      "Epoch  68 Batch 1000/2400 - Train Accuracy: 0.9777, Validation Accuracy: 0.6339, Loss: 0.0587\n",
      "Epoch  68 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6339, Loss: 0.0113\n",
      "Epoch  68 Batch 2000/2400 - Train Accuracy: 0.9760, Validation Accuracy: 0.6339, Loss: 0.0460\n",
      "Epoch  69 Batch  500/2400 - Train Accuracy: 0.9651, Validation Accuracy: 0.6339, Loss: 0.1071\n",
      "Epoch  69 Batch 1000/2400 - Train Accuracy: 0.9799, Validation Accuracy: 0.6339, Loss: 0.0662\n",
      "Epoch  69 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6339, Loss: 0.0197\n",
      "Epoch  69 Batch 2000/2400 - Train Accuracy: 0.9760, Validation Accuracy: 0.6339, Loss: 0.0713\n",
      "Epoch  70 Batch  500/2400 - Train Accuracy: 0.9706, Validation Accuracy: 0.6339, Loss: 0.0865\n",
      "Epoch  70 Batch 1000/2400 - Train Accuracy: 0.9955, Validation Accuracy: 0.6339, Loss: 0.0444\n",
      "Epoch  70 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6339, Loss: 0.0130\n",
      "Epoch  70 Batch 2000/2400 - Train Accuracy: 0.9712, Validation Accuracy: 0.6362, Loss: 0.0825\n",
      "Epoch  71 Batch  500/2400 - Train Accuracy: 0.9449, Validation Accuracy: 0.6339, Loss: 0.0898\n",
      "Epoch  71 Batch 1000/2400 - Train Accuracy: 0.9464, Validation Accuracy: 0.6339, Loss: 0.0690\n",
      "Epoch  71 Batch 1500/2400 - Train Accuracy: 0.9972, Validation Accuracy: 0.6339, Loss: 0.0092\n",
      "Epoch  71 Batch 2000/2400 - Train Accuracy: 0.9856, Validation Accuracy: 0.6339, Loss: 0.0593\n",
      "Epoch  72 Batch  500/2400 - Train Accuracy: 0.9504, Validation Accuracy: 0.6339, Loss: 0.0731\n",
      "Epoch  72 Batch 1000/2400 - Train Accuracy: 0.9844, Validation Accuracy: 0.6339, Loss: 0.0629\n",
      "Epoch  72 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6339, Loss: 0.0086\n",
      "Epoch  72 Batch 2000/2400 - Train Accuracy: 0.9639, Validation Accuracy: 0.6339, Loss: 0.0517\n",
      "Epoch  73 Batch  500/2400 - Train Accuracy: 0.9577, Validation Accuracy: 0.6339, Loss: 0.0638\n",
      "Epoch  73 Batch 1000/2400 - Train Accuracy: 0.9598, Validation Accuracy: 0.6339, Loss: 0.0658\n",
      "Epoch  73 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6339, Loss: 0.0153\n",
      "Epoch  73 Batch 2000/2400 - Train Accuracy: 0.9832, Validation Accuracy: 0.6339, Loss: 0.0552\n",
      "Epoch  74 Batch  500/2400 - Train Accuracy: 0.9724, Validation Accuracy: 0.6339, Loss: 0.0785\n",
      "Epoch  74 Batch 1000/2400 - Train Accuracy: 0.9799, Validation Accuracy: 0.6339, Loss: 0.0437\n",
      "Epoch  74 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6339, Loss: 0.0095\n",
      "Epoch  74 Batch 2000/2400 - Train Accuracy: 0.9688, Validation Accuracy: 0.6339, Loss: 0.0825\n",
      "Epoch  75 Batch  500/2400 - Train Accuracy: 0.9614, Validation Accuracy: 0.6339, Loss: 0.0679\n",
      "Epoch  75 Batch 1000/2400 - Train Accuracy: 0.9598, Validation Accuracy: 0.6339, Loss: 0.0646\n",
      "Epoch  75 Batch 1500/2400 - Train Accuracy: 0.9972, Validation Accuracy: 0.6339, Loss: 0.0091\n",
      "Epoch  75 Batch 2000/2400 - Train Accuracy: 0.9784, Validation Accuracy: 0.6339, Loss: 0.0625\n",
      "Epoch  76 Batch  500/2400 - Train Accuracy: 0.9614, Validation Accuracy: 0.6339, Loss: 0.0786\n",
      "Epoch  76 Batch 1000/2400 - Train Accuracy: 0.9911, Validation Accuracy: 0.6339, Loss: 0.0419\n",
      "Epoch  76 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6362, Loss: 0.0108\n",
      "Epoch  76 Batch 2000/2400 - Train Accuracy: 0.9736, Validation Accuracy: 0.6362, Loss: 0.0555\n",
      "Epoch  77 Batch  500/2400 - Train Accuracy: 0.9632, Validation Accuracy: 0.6362, Loss: 0.0579\n",
      "Epoch  77 Batch 1000/2400 - Train Accuracy: 0.9866, Validation Accuracy: 0.6339, Loss: 0.0388\n",
      "Epoch  77 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6339, Loss: 0.0063\n",
      "Epoch  77 Batch 2000/2400 - Train Accuracy: 0.9688, Validation Accuracy: 0.6339, Loss: 0.0422\n",
      "Epoch  78 Batch  500/2400 - Train Accuracy: 0.9706, Validation Accuracy: 0.6362, Loss: 0.0727\n",
      "Epoch  78 Batch 1000/2400 - Train Accuracy: 0.9821, Validation Accuracy: 0.6339, Loss: 0.0257\n",
      "Epoch  78 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6362, Loss: 0.0131\n",
      "Epoch  78 Batch 2000/2400 - Train Accuracy: 0.9736, Validation Accuracy: 0.6362, Loss: 0.0612\n",
      "Epoch  79 Batch  500/2400 - Train Accuracy: 0.9835, Validation Accuracy: 0.6362, Loss: 0.0559\n",
      "Epoch  79 Batch 1000/2400 - Train Accuracy: 0.9888, Validation Accuracy: 0.6384, Loss: 0.0444\n",
      "Epoch  79 Batch 1500/2400 - Train Accuracy: 0.9972, Validation Accuracy: 0.6339, Loss: 0.0111\n",
      "Epoch  79 Batch 2000/2400 - Train Accuracy: 0.9591, Validation Accuracy: 0.6362, Loss: 0.0400\n",
      "Epoch  80 Batch  500/2400 - Train Accuracy: 0.9706, Validation Accuracy: 0.6339, Loss: 0.0448\n",
      "Epoch  80 Batch 1000/2400 - Train Accuracy: 0.9754, Validation Accuracy: 0.6362, Loss: 0.0429\n",
      "Epoch  80 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6339, Loss: 0.0081\n",
      "Epoch  80 Batch 2000/2400 - Train Accuracy: 0.9880, Validation Accuracy: 0.6339, Loss: 0.0639\n",
      "Epoch  81 Batch  500/2400 - Train Accuracy: 0.9559, Validation Accuracy: 0.6339, Loss: 0.0572\n",
      "Epoch  81 Batch 1000/2400 - Train Accuracy: 0.9665, Validation Accuracy: 0.6362, Loss: 0.0411\n",
      "Epoch  81 Batch 1500/2400 - Train Accuracy: 0.9972, Validation Accuracy: 0.6339, Loss: 0.0113\n",
      "Epoch  81 Batch 2000/2400 - Train Accuracy: 0.9760, Validation Accuracy: 0.6339, Loss: 0.0683\n",
      "Epoch  82 Batch  500/2400 - Train Accuracy: 0.9596, Validation Accuracy: 0.6339, Loss: 0.0489\n",
      "Epoch  82 Batch 1000/2400 - Train Accuracy: 0.9866, Validation Accuracy: 0.6339, Loss: 0.0312\n",
      "Epoch  82 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6339, Loss: 0.0150\n",
      "Epoch  82 Batch 2000/2400 - Train Accuracy: 0.9832, Validation Accuracy: 0.6339, Loss: 0.0417\n",
      "Epoch  83 Batch  500/2400 - Train Accuracy: 0.9651, Validation Accuracy: 0.6339, Loss: 0.0798\n",
      "Epoch  83 Batch 1000/2400 - Train Accuracy: 0.9754, Validation Accuracy: 0.6339, Loss: 0.0387\n",
      "Epoch  83 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6339, Loss: 0.0070\n",
      "Epoch  83 Batch 2000/2400 - Train Accuracy: 0.9952, Validation Accuracy: 0.6339, Loss: 0.0522\n",
      "Epoch  84 Batch  500/2400 - Train Accuracy: 0.9577, Validation Accuracy: 0.6339, Loss: 0.0522\n",
      "Epoch  84 Batch 1000/2400 - Train Accuracy: 0.9844, Validation Accuracy: 0.6339, Loss: 0.0495\n",
      "Epoch  84 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6339, Loss: 0.0116\n",
      "Epoch  84 Batch 2000/2400 - Train Accuracy: 0.9784, Validation Accuracy: 0.6339, Loss: 0.0565\n",
      "Epoch  85 Batch  500/2400 - Train Accuracy: 0.9688, Validation Accuracy: 0.6339, Loss: 0.0640\n",
      "Epoch  85 Batch 1000/2400 - Train Accuracy: 0.9911, Validation Accuracy: 0.6362, Loss: 0.0448\n",
      "Epoch  85 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6339, Loss: 0.0155\n",
      "Epoch  85 Batch 2000/2400 - Train Accuracy: 0.9567, Validation Accuracy: 0.6339, Loss: 0.0615\n",
      "Epoch  86 Batch  500/2400 - Train Accuracy: 0.9688, Validation Accuracy: 0.6339, Loss: 0.0496\n",
      "Epoch  86 Batch 1000/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6362, Loss: 0.0458\n",
      "Epoch  86 Batch 1500/2400 - Train Accuracy: 0.9943, Validation Accuracy: 0.6339, Loss: 0.0155\n",
      "Epoch  86 Batch 2000/2400 - Train Accuracy: 0.9880, Validation Accuracy: 0.6362, Loss: 0.0187\n",
      "Epoch  87 Batch  500/2400 - Train Accuracy: 0.9596, Validation Accuracy: 0.6339, Loss: 0.0506\n",
      "Epoch  87 Batch 1000/2400 - Train Accuracy: 0.9621, Validation Accuracy: 0.6339, Loss: 0.0269\n",
      "Epoch  87 Batch 1500/2400 - Train Accuracy: 0.9972, Validation Accuracy: 0.6339, Loss: 0.0077\n",
      "Epoch  87 Batch 2000/2400 - Train Accuracy: 0.9688, Validation Accuracy: 0.6339, Loss: 0.0526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  88 Batch  500/2400 - Train Accuracy: 0.9614, Validation Accuracy: 0.6339, Loss: 0.0491\n",
      "Epoch  88 Batch 1000/2400 - Train Accuracy: 0.9777, Validation Accuracy: 0.6339, Loss: 0.0569\n",
      "Epoch  88 Batch 1500/2400 - Train Accuracy: 0.9943, Validation Accuracy: 0.6339, Loss: 0.0110\n",
      "Epoch  88 Batch 2000/2400 - Train Accuracy: 0.9880, Validation Accuracy: 0.6339, Loss: 0.0480\n",
      "Epoch  89 Batch  500/2400 - Train Accuracy: 0.9651, Validation Accuracy: 0.6339, Loss: 0.0488\n",
      "Epoch  89 Batch 1000/2400 - Train Accuracy: 0.9911, Validation Accuracy: 0.6339, Loss: 0.0445\n",
      "Epoch  89 Batch 1500/2400 - Train Accuracy: 1.0000, Validation Accuracy: 0.6339, Loss: 0.0130\n",
      "Epoch  89 Batch 2000/2400 - Train Accuracy: 0.9856, Validation Accuracy: 0.6339, Loss: 0.0293\n"
     ]
    }
   ],
   "source": [
    "speaker_id, lexicon = list(lexicons.items())[0]\n",
    "lexicon.optimize(early_stop=True)\n",
    "#lexicon.evaluate_testset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate LSTM Net Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id, lexicon = list(lexicons.items())[0]\n",
    "print(\"List of Speeches:\", len(lexicon.speeches))\n",
    "lexicon.evaluate_testset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper \n",
    "# Save parameters for checkpoint\n",
    "speaker_id, lexicon = list(lexicons.items())[0]\n",
    "helper.save_params(lexicon.cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "speaker_id, lexicon = list(lexicons.items())[0]\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = helper.load_preprocess()\n",
    "load_path = helper.load_params(lexicon.cache_dir)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
