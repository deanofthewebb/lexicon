{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon - Custom Language Model\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "For this project, I will build a simple custom language model that is able to learn from any text data provided, and return a transcript with confidence values from input posed in speech utterances. I will use Google's cloud-based services to preprocess the input audio data and transcribe into an initial guess. Then I will train a model to improve on Google cloud speech API's response.\n",
    "\n",
    "\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "In order to use Google's cloud-based services, you first need to create an account on the [Google Cloud Platform](https://cloud.google.com//).\n",
    "\n",
    "Then, for each service you want to use, you have to enable use of that service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: google-cloud-speech in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages\n",
      "Collecting google-gax<0.16dev,>=0.15.14 (from google-cloud-speech)\n",
      "  Downloading google-gax-0.15.15.tar.gz (109kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 1.4MB/s \n",
      "\u001b[?25hRequirement already up-to-date: googleapis-common-protos[grpc]<2.0dev,>=1.5.2 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from google-cloud-speech)\n",
      "Requirement already up-to-date: google-cloud-core<0.28dev,>=0.27.0 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from google-cloud-speech)\n",
      "Requirement already up-to-date: dill<0.3dev,>=0.2.5 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: future<0.17dev,>=0.16.0 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Collecting grpcio<1.6dev,>=1.0.2 (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "  Downloading grpcio-1.4.0-cp35-cp35m-macosx_10_7_intel.whl (1.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.5MB 307kB/s \n",
      "\u001b[?25hRequirement already up-to-date: google-auth<2.0dev,>=1.0.0 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: ply==3.8 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: protobuf<4.0dev,>=3.0.0 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: requests<3.0dev,>=2.13.0 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Collecting setuptools>=34.0.0 (from google-cloud-core<0.28dev,>=0.27.0->google-cloud-speech)\n",
      "  Downloading setuptools-36.5.0-py2.py3-none-any.whl (478kB)\n",
      "\u001b[K    100% |████████████████████████████████| 481kB 839kB/s \n",
      "\u001b[?25hCollecting six>=1.10.0 (from google-cloud-core<0.28dev,>=0.27.0->google-cloud-speech)\n",
      "  Downloading six-1.11.0-py2.py3-none-any.whl\n",
      "Requirement already up-to-date: pyasn1-modules>=0.0.5 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from google-auth<2.0dev,>=1.0.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: cachetools>=2.0.0 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from google-auth<2.0dev,>=1.0.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: rsa>=3.1.4 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from google-auth<2.0dev,>=1.0.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Collecting pyasn1>=0.1.7 (from google-auth<2.0dev,>=1.0.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "  Downloading pyasn1-0.3.5-py2.py3-none-any.whl (63kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 4.2MB/s \n",
      "\u001b[?25hRequirement already up-to-date: urllib3<1.23,>=1.21.1 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from requests<3.0dev,>=2.13.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: certifi>=2017.4.17 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from requests<3.0dev,>=2.13.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: idna<2.7,>=2.5 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from requests<3.0dev,>=2.13.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: chardet<3.1.0,>=3.0.2 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from requests<3.0dev,>=2.13.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Building wheels for collected packages: google-gax\n",
      "  Running setup.py bdist_wheel for google-gax ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/deanmwebb/Library/Caches/pip/wheels/47/2c/d6/1c98d54675550a39421ae92ffd0e3e72f1eaee7725bf738131\n",
      "Successfully built google-gax\n",
      "Installing collected packages: six, grpcio, google-gax, setuptools, pyasn1\n",
      "  Found existing installation: six 1.10.0\n",
      "\u001b[31m    DEPRECATION: Uninstalling a distutils installed project (six) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\u001b[0m\n",
      "    Uninstalling six-1.10.0:\n",
      "      Successfully uninstalled six-1.10.0\n",
      "  Found existing installation: grpcio 1.6.0\n",
      "    Uninstalling grpcio-1.6.0:\n",
      "      Successfully uninstalled grpcio-1.6.0\n",
      "  Found existing installation: google-gax 0.15.14\n",
      "    Uninstalling google-gax-0.15.14:\n",
      "      Successfully uninstalled google-gax-0.15.14\n",
      "  Found existing installation: setuptools 36.4.0\n",
      "    Uninstalling setuptools-36.4.0:\n",
      "      Successfully uninstalled setuptools-36.4.0\n",
      "  Found existing installation: pyasn1 0.3.4\n",
      "    Uninstalling pyasn1-0.3.4:\n",
      "      Successfully uninstalled pyasn1-0.3.4\n",
      "Successfully installed google-gax-0.15.15 grpcio-1.4.0 pyasn1-0.3.5 setuptools-36.5.0 six-1.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-cloud-speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Google Cloud SDK: https://cloud.google.com/sdk/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Google Cloud SDK!\n",
      "\n",
      "To help improve the quality of this product, we collect anonymized usage data\n",
      "and anonymized stacktraces when crashes are encountered; additional information\n",
      "is available at <https://cloud.google.com/sdk/usage-statistics>. You may choose\n",
      "to opt out of this collection now (by choosing 'N' at the below prompt), or at\n",
      "any time in the future by running the following command:\n",
      "\n",
      "    gcloud config set disable_usage_reporting true\n",
      "\n",
      "\n",
      "Your current Cloud SDK version is: 170.0.1\n",
      "The latest available version is: 171.0.0\n",
      "\n",
      "┌────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│                                                   Components                                                   │\n",
      "├──────────────────┬──────────────────────────────────────────────────────┬──────────────────────────┬───────────┤\n",
      "│      Status      │                         Name                         │            ID            │    Size   │\n",
      "├──────────────────┼──────────────────────────────────────────────────────┼──────────────────────────┼───────────┤\n",
      "│ Update Available │ Cloud SDK Core Libraries                             │ core                     │   6.7 MiB │\n",
      "│ Not Installed    │ App Engine Go Extensions                             │ app-engine-go            │  97.7 MiB │\n",
      "│ Not Installed    │ Cloud Bigtable Command Line Tool                     │ cbt                      │   4.0 MiB │\n",
      "│ Not Installed    │ Cloud Bigtable Emulator                              │ bigtable                 │   3.5 MiB │\n",
      "│ Not Installed    │ Cloud Datalab Command Line Tool                      │ datalab                  │   < 1 MiB │\n",
      "│ Not Installed    │ Cloud Datastore Emulator                             │ cloud-datastore-emulator │  15.4 MiB │\n",
      "│ Not Installed    │ Cloud Datastore Emulator (Legacy)                    │ gcd-emulator             │  38.1 MiB │\n",
      "│ Not Installed    │ Cloud Pub/Sub Emulator                               │ pubsub-emulator          │  33.2 MiB │\n",
      "│ Not Installed    │ Emulator Reverse Proxy                               │ emulator-reverse-proxy   │  14.5 MiB │\n",
      "│ Not Installed    │ Google Container Local Builder                       │ container-builder-local  │   3.7 MiB │\n",
      "│ Not Installed    │ Google Container Registry's Docker credential helper │ docker-credential-gcr    │   2.2 MiB │\n",
      "│ Not Installed    │ gcloud Alpha Commands                                │ alpha                    │   < 1 MiB │\n",
      "│ Not Installed    │ gcloud Beta Commands                                 │ beta                     │   < 1 MiB │\n",
      "│ Not Installed    │ gcloud app Java Extensions                           │ app-engine-java          │ 130.8 MiB │\n",
      "│ Not Installed    │ gcloud app PHP Extensions                            │ app-engine-php           │  21.9 MiB │\n",
      "│ Not Installed    │ gcloud app Python Extensions                         │ app-engine-python        │   6.3 MiB │\n",
      "│ Not Installed    │ kubectl                                              │ kubectl                  │  15.9 MiB │\n",
      "│ Installed        │ BigQuery Command Line Tool                           │ bq                       │   < 1 MiB │\n",
      "│ Installed        │ Cloud Storage Command Line Tool                      │ gsutil                   │   3.0 MiB │\n",
      "└──────────────────┴──────────────────────────────────────────────────────┴──────────────────────────┴───────────┘\n",
      "To install or remove components at your current SDK version [170.0.1], run:\n",
      "  $ gcloud components install COMPONENT_ID\n",
      "  $ gcloud components remove COMPONENT_ID\n",
      "\n",
      "To update your SDK installation to the latest version [171.0.0], run:\n",
      "  $ gcloud components update\n",
      "\n",
      "==> Source [/Users/deanmwebb/Google Drive/Development/consulting/lexicon/google-cloud-sdk/completion.bash.inc] in your profile to enable shell command completion for gcloud.\n",
      "==> Source [/Users/deanmwebb/Google Drive/Development/consulting/lexicon/google-cloud-sdk/path.bash.inc] in your profile to add the Google Cloud SDK command line tools to your $PATH.\n",
      "\n",
      "For more information on how to get started, please visit:\n",
      "  https://cloud.google.com/sdk/docs/quickstarts\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!CLOUDSDK_CORE_DISABLE_PROMPTS=1 ./google-cloud-sdk/install.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate with Google Cloud API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activated service account credentials for: [lexicon-bot@exemplary-oath-179301.iam.gserviceaccount.com]\n",
      "\n",
      "\n",
      "Updates are available for some Cloud SDK components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!source google-cloud-sdk/completion.bash.inc && \\\n",
    "source google-cloud-sdk/path.bash.inc && \\\n",
    "gcloud auth activate-service-account lexicon-bot@exemplary-oath-179301.iam.gserviceaccount.com --key-file=Lexicon-e94eff39fad7.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']='/Users/deanmwebb/Google Drive/Development/consulting/lexicon/Lexicon-e94eff39fad7.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test out Cloud Spech API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for operation to complete...\n",
      "Transcript: at this moment to the whole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\n",
      "Confidence Score: 0.9484339356422424\n",
      "Word: at, start_time (s): 0.1, end_time (s): 0.5, total_time (s): 0.4\n",
      "Word: this, start_time (s): 0.5, end_time (s): 0.6000000000000001, total_time (s): 0.10000000000000009\n",
      "Word: moment, start_time (s): 0.6000000000000001, end_time (s): 0.8, total_time (s): 0.19999999999999996\n",
      "Word: to, start_time (s): 0.8, end_time (s): 1.1, total_time (s): 0.30000000000000004\n",
      "Word: the, start_time (s): 1.1, end_time (s): 1.3, total_time (s): 0.19999999999999996\n",
      "Word: whole, start_time (s): 1.3, end_time (s): 1.5, total_time (s): 0.19999999999999996\n",
      "Word: soul, start_time (s): 1.5, end_time (s): 2.0, total_time (s): 0.5\n",
      "Word: of, start_time (s): 2.0, end_time (s): 2.2, total_time (s): 0.20000000000000018\n",
      "Word: the, start_time (s): 2.2, end_time (s): 2.3, total_time (s): 0.09999999999999964\n",
      "Word: Old, start_time (s): 2.3, end_time (s): 2.5, total_time (s): 0.20000000000000018\n",
      "Word: Man, start_time (s): 2.5, end_time (s): 2.7, total_time (s): 0.20000000000000018\n",
      "Word: scene, start_time (s): 2.7, end_time (s): 3.0, total_time (s): 0.2999999999999998\n",
      "Word: centered, start_time (s): 3.0, end_time (s): 3.4, total_time (s): 0.3999999999999999\n",
      "Word: in, start_time (s): 3.4, end_time (s): 3.5, total_time (s): 0.10000000000000009\n",
      "Word: his, start_time (s): 3.5, end_time (s): 3.7, total_time (s): 0.20000000000000018\n",
      "Word: eyes, start_time (s): 3.7, end_time (s): 4.1, total_time (s): 0.39999999999999947\n",
      "Word: which, start_time (s): 4.1, end_time (s): 4.2, total_time (s): 0.10000000000000053\n",
      "Word: became, start_time (s): 4.2, end_time (s): 4.5, total_time (s): 0.2999999999999998\n",
      "Word: bloodshot, start_time (s): 4.5, end_time (s): 5.3, total_time (s): 0.7999999999999998\n",
      "Word: the, start_time (s): 5.3, end_time (s): 5.6, total_time (s): 0.2999999999999998\n",
      "Word: veins, start_time (s): 5.6, end_time (s): 5.9, total_time (s): 0.3000000000000007\n",
      "Word: of, start_time (s): 5.9, end_time (s): 6.1, total_time (s): 0.1999999999999993\n",
      "Word: the, start_time (s): 6.1, end_time (s): 6.1, total_time (s): 0.0\n",
      "Word: throat, start_time (s): 6.1, end_time (s): 6.4, total_time (s): 0.3000000000000007\n",
      "Word: swelled, start_time (s): 6.4, end_time (s): 6.9, total_time (s): 0.5\n",
      "Word: his, start_time (s): 6.9, end_time (s): 7.2, total_time (s): 0.2999999999999998\n",
      "Word: cheeks, start_time (s): 7.2, end_time (s): 7.6, total_time (s): 0.39999999999999947\n",
      "Word: and, start_time (s): 7.6, end_time (s): 7.6, total_time (s): 0.0\n",
      "Word: temples, start_time (s): 7.6, end_time (s): 8.1, total_time (s): 0.5\n",
      "Word: became, start_time (s): 8.1, end_time (s): 8.4, total_time (s): 0.3000000000000007\n",
      "Word: purple, start_time (s): 8.4, end_time (s): 8.8, total_time (s): 0.40000000000000036\n",
      "Word: as, start_time (s): 8.8, end_time (s): 9.0, total_time (s): 0.1999999999999993\n",
      "Word: though, start_time (s): 9.0, end_time (s): 9.1, total_time (s): 0.09999999999999964\n",
      "Word: he, start_time (s): 9.1, end_time (s): 9.2, total_time (s): 0.09999999999999964\n",
      "Word: was, start_time (s): 9.2, end_time (s): 9.3, total_time (s): 0.10000000000000142\n",
      "Word: struck, start_time (s): 9.3, end_time (s): 9.7, total_time (s): 0.3999999999999986\n",
      "Word: with, start_time (s): 9.7, end_time (s): 9.8, total_time (s): 0.10000000000000142\n",
      "Word: epilepsy, start_time (s): 9.8, end_time (s): 10.2, total_time (s): 0.3999999999999986\n",
      "Word: nothing, start_time (s): 10.2, end_time (s): 10.9, total_time (s): 0.7000000000000011\n",
      "Word: was, start_time (s): 10.9, end_time (s): 11.1, total_time (s): 0.1999999999999993\n",
      "Word: wanting, start_time (s): 11.1, end_time (s): 11.4, total_time (s): 0.3000000000000007\n",
      "Word: to, start_time (s): 11.4, end_time (s): 11.5, total_time (s): 0.09999999999999964\n",
      "Word: complete, start_time (s): 11.5, end_time (s): 11.9, total_time (s): 0.40000000000000036\n",
      "Word: this, start_time (s): 11.9, end_time (s): 12.0, total_time (s): 0.09999999999999964\n",
      "Word: but, start_time (s): 12.0, end_time (s): 12.3, total_time (s): 0.3000000000000007\n",
      "Word: the, start_time (s): 12.3, end_time (s): 12.4, total_time (s): 0.09999999999999964\n",
      "Word: utterance, start_time (s): 12.4, end_time (s): 12.8, total_time (s): 0.40000000000000036\n",
      "Word: of, start_time (s): 12.8, end_time (s): 12.8, total_time (s): 0.0\n",
      "Word: a, start_time (s): 12.8, end_time (s): 13.0, total_time (s): 0.1999999999999993\n",
      "Word: cry, start_time (s): 13.0, end_time (s): 13.2, total_time (s): 0.1999999999999993\n",
      "Transcript: at this moment to the whole soul of the old man seemed centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\n",
      "Confidence Score: 0.9514756202697754\n",
      "Transcript: at this moment to the whole soul of the old man seem centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\n",
      "Confidence Score: 0.9415985941886902\n",
      "Transcript: at this moment to the whole sole of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\n",
      "Confidence Score: 0.9484888315200806\n",
      "Transcript: at this moment to the whole sole of the old man seemed centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\n",
      "Confidence Score: 0.9531012773513794\n",
      "Transcript: at this moment to the whole sole of the old man seem centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\n",
      "Confidence Score: 0.9432242512702942\n",
      "Transcript: at this moment to the whole soul of the old man seems centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\n",
      "Confidence Score: 0.9610214233398438\n",
      "Transcript: at this moment to the whole song of The Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\n",
      "Confidence Score: 0.9349250793457031\n",
      "Transcript: at this moment to the whole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this put the utterance of a cry\n",
      "Confidence Score: 0.9473556280136108\n",
      "Transcript: at this moment to the hole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\n",
      "Confidence Score: 0.9477002620697021\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import speech\n",
    "from google.cloud.speech import enums\n",
    "from google.cloud.speech import types\n",
    "\n",
    "# Instantiates a client\n",
    "client = speech.SpeechClient()\n",
    "\n",
    "# The name of the dev-test audio file to transcribe\n",
    "dev_file_name_0 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0000.flac')\n",
    "gt0 = 'GO DO YOU HEAR'\n",
    "\n",
    "dev_file_name_1 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0001.flac')\n",
    "gt1 = 'BUT IN LESS THAN FIVE MINUTES THE STAIRCASE GROANED BENEATH AN EXTRAORDINARY WEIGHT'\n",
    "\n",
    "# The name of the test audio file to transcribe\n",
    "dev_file_name_2 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0002.flac')\n",
    "gt2 = 'AT THIS MOMENT THE WHOLE SOUL OF THE OLD MAN SEEMED CENTRED IN HIS EYES WHICH BECAME BLOODSHOT THE VEINS OF THE THROAT SWELLED HIS CHEEKS AND TEMPLES BECAME PURPLE AS THOUGH HE WAS STRUCK WITH EPILEPSY NOTHING WAS WANTING TO COMPLETE THIS BUT THE UTTERANCE OF A CRY'\n",
    "\n",
    "dev_file_name_3 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0003.flac')\n",
    "gt3 = 'AND THE CRY ISSUED FROM HIS PORES IF WE MAY THUS SPEAK A CRY FRIGHTFUL IN ITS SILENCE'\n",
    "\n",
    "dev_file_name_4 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0004.flac')\n",
    "gt4 = \"D'AVRIGNY RUSHED TOWARDS THE OLD MAN AND MADE HIM INHALE A POWERFUL RESTORATIVE\"\n",
    "\n",
    "\n",
    "test_file_name_1 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'RNN-Tutorial-master',\n",
    "    'data',\n",
    "    'raw',\n",
    "    'librivox',\n",
    "    'LibriSpeech',\n",
    "    'test-clean-wav',\n",
    "    '4507-16021-0019.wav')\n",
    "\n",
    "\n",
    "audio_files = {dev_file_name_0:gt0, dev_file_name_1:gt1, dev_file_name_2:gt2, dev_file_name_3:gt3, dev_file_name_4:gt4}\n",
    "\n",
    "\n",
    "# Loads the audio into memory\n",
    "with io.open(dev_file_name_2, 'rb') as audio_file:\n",
    "    content = audio_file.read()\n",
    "    audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "config = types.RecognitionConfig(\n",
    "    encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
    "    sample_rate_hertz=16000,\n",
    "    language_code='en-US',\n",
    "    max_alternatives=10,\n",
    "    profanity_filter=False,\n",
    "    enable_word_time_offsets=True)\n",
    "\n",
    "# Detects speech and words in the audio file\n",
    "operation = client.long_running_recognize(config, audio)\n",
    "\n",
    "print('Waiting for operation to complete...')\n",
    "result = operation.result(timeout=90)\n",
    "\n",
    "alternatives = result.results[0].alternatives\n",
    "for alternative in alternatives:\n",
    "    print('Transcript: {}'.format(alternative.transcript))\n",
    "    print('Confidence Score: {}'.format(alternative.confidence))\n",
    "\n",
    "    for word_info in alternative.words:\n",
    "        word = word_info.word\n",
    "        start_time = word_info.start_time\n",
    "        end_time = word_info.end_time\n",
    "        start = start_time.seconds + start_time.nanos * 1e-9\n",
    "        end = end_time.seconds + end_time.nanos * 1e-9\n",
    "        delta = end - start\n",
    "        \n",
    "        print('Word: {}, start_time (s): {}, end_time (s): {}, total_time (s): {}'.format(\n",
    "            word,\n",
    "            start,\n",
    "            end,\n",
    "            delta))\n",
    "        \n",
    "        #TODO: Do we need to figure out how to assign words to alternatives?\n",
    "            # If same amounts, assign words to index of parsed word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Download the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "\n",
    "librispeech_dataset_folder_path = 'LibriSpeech'\n",
    "tar_gz_path = 'dev-clean.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Librispeech dev-clean.tar.gz') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://www.openslr.org/resources/12/dev-clean.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(librispeech_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Preprocess Dataset - Download Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "[nltk_data] Downloading package punkt to /Users/deanmwebb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/deanmwebb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk #NLP Toolkit\n",
    "nltk.download()\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gensim Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages\n",
      "Requirement already up-to-date: scipy>=0.18.1 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already up-to-date: numpy>=1.11.3 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already up-to-date: six>=1.5.0 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already up-to-date: smart-open>=1.2.1 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already up-to-date: requests in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already up-to-date: bz2file in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already up-to-date: boto>=2.32 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already up-to-date: certifi>=2017.4.17 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already up-to-date: chardet<3.1.0,>=3.0.2 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already up-to-date: idna<2.7,>=2.5 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already up-to-date: urllib3<1.23,>=1.21.1 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41 text files in the directory: /Users/deanmwebb/Google Drive/Development/consulting/lexicon/LibriSpeech/books/utf-8/**/*.txt*\n"
     ]
    }
   ],
   "source": [
    "# Prepare a plain text corpus from which we train a languague model\n",
    "import glob\n",
    "\n",
    "# Gather all text files from directory\n",
    "WORKING_DIRECTORY = os.path.join(os.getcwd(),'LibriSpeech/')\n",
    "\n",
    "# TRAINING_DIRECTORY = os.path.abspath(os.path.join(os.sep,'Volumes',\"My\\ Passport\\ for\\ Mac\",'lexicon','LibriSpeech'))\n",
    "dev_path = \"{}{}{}{}\".format(WORKING_DIRECTORY, 'dev-clean/', '**/', '*.txt')\n",
    "train_path = \"{}{}{}{}{}\".format(WORKING_DIRECTORY, 'books/', 'utf-8/', '**/', '*.txt*')\n",
    "\n",
    "text_paths = sorted(glob.glob(train_path, recursive=True))\n",
    "print('Found',len(text_paths),'text files in the directory:', train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset - Tokenize Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "# reading the file in unicode format using codecs library    \n",
    "stoplist = set(stopwords.words('english'))\n",
    "# Strip punctuation\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation) \n",
    "        \n",
    "corpus_raw = u\"\"\n",
    "for book_filename in text_paths:\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        lines = book_file.read()\n",
    "        corpus_raw += lines.translate(translate_table) # remove punctuations \n",
    "\n",
    "               \n",
    "# Tokenize\n",
    "tokenized_words = nltk.tokenize.word_tokenize(corpus_raw)\n",
    "\n",
    "## Clean the tokens ##\n",
    "# Remove stop words\n",
    "tokenized_words = [word for word in tokenized_words if word not in stoplist]\n",
    "\n",
    "# Remove single-character tokens (mostly punctuation)\n",
    "tokenized_words = [word for word in tokenized_words if len(word) > 1]\n",
    "\n",
    "# Remove numbers\n",
    "tokenized_words = [word for word in tokenized_words if not word.isnumeric()]\n",
    "\n",
    "# Lowercase all words (default_stopwords are lowercase too)\n",
    "tokenized_words = [word.lower() for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset - Extract N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9d1b1fed6272>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lang_model.npy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbigram_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.probability import LidstoneProbDist\n",
    "from nltk.collocations import *\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "bigram_model = nltk.bigrams(tokenized_words)\n",
    "bigram_model = sorted(bigram_model, key=lambda item: item[1], reverse=True)  \n",
    "# print(bigram_model)\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "np.save(\"lang_model.npy\",bigram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word|Freq:\n",
      "('project', 'gutenbergtm')|1095\n",
      "('project', 'gutenberg')|1014\n",
      "('greater', 'part')|532\n",
      "('captain', 'nemo')|452\n",
      "('united', 'states')|407\n",
      "('great', 'britain')|385\n",
      "('uncle', 'john')|364\n",
      "('gold', 'silver')|337\n",
      "('let', 'us')|324\n",
      "('new', 'york')|309\n",
      "('gutenbergtm', 'electronic')|306\n",
      "('old', 'man')|303\n",
      "('public', 'domain')|293\n",
      "('every', 'one')|290\n",
      "('one', 'day')|281\n",
      "('young', 'man')|281\n",
      "('gutenberg', 'literary')|279\n",
      "('literary', 'archive')|279\n",
      "('archive', 'foundation')|279\n",
      "('dont', 'know')|275\n",
      "('of', 'course')|274\n",
      "('one', 'another')|273\n",
      "('electronic', 'works')|272\n",
      "('per', 'cent')|263\n",
      "('could', 'see')|260\n",
      "('ned', 'land')|254\n",
      "('good', 'deal')|247\n",
      "('mrs', 'sparsit')|244\n",
      "('two', 'three')|240\n",
      "('mr', 'bounderby')|236\n",
      "('set', 'forth')|225\n",
      "('old', 'woman')|218\n",
      "('years', 'ago')|217\n",
      "('the', 'first')|206\n",
      "('you', 'may')|205\n",
      "('it', 'would')|200\n",
      "('next', 'day')|200\n",
      "('long', 'time')|199\n",
      "('said', 'mrs')|198\n",
      "('of', 'the')|198\n",
      "('said', 'mr')|196\n",
      "('first', 'time')|195\n",
      "('every', 'day')|192\n",
      "('one', 'thing')|189\n",
      "('small', 'print')|189\n",
      "('electronic', 'work')|187\n",
      "('men', 'women')|186\n",
      "('every', 'man')|181\n",
      "('it', 'may')|171\n",
      "('and', 'said')|169\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(bigram_model)\n",
    "\n",
    "# Output top 50 words\n",
    "print(\"Word|Freq:\")\n",
    "for word, frequency in fdist.most_common(50):\n",
    "    print(u'{}|{}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Listing the words that can follow after 'said':\n",
      " dict_keys(['taxes', 'revenue', 'latter', 'deviation', 'injustice', 'told', 'shall', 'ease', 'depths', 'mass', 'difficulty', 'insult', 'favour', 'range', 'danger', 'frequency', 'things', 'than', 'competition', 'already', 'cultivation', 'convenience', 'many', 'would', 'perpendicular', 'writings', 'crown', 'usual', 'fire', 'heartiness', 'valuable', 'variation', 'variety', 'teacher', 'could', 'loss’', 'body', 'vitality', 'honor', 'circulation', 'salaries', 'ruritania', 'taint', 'pressure', 'distinctness', 'lights', 'parsimony', 'portion', 'amount', 'haytime', 'solidarity', 'satisfaction', 'mystery', 'wisdom', 'frequently', 'dangers', 'calamity', 'approximation', 'business', 'rights', 'diligence', 'impersonal', 'parts', 'advantages', 'violence', 'english', 'remoteness', 'part', 'but', 'sun', 'abundance', 'understanding', 'account', 'fear', 'universal', 'higher', 'singleness', 'opportunities', 'the', 'africa', 'profusion', 'never', 'warmth', 'difference', 'abroad', 'dole', 'capacities', 'number', 'expression', 'glory', 'grief', 'proportion', 'thirst', 'rent', 'slumbers', 'like', 'foregoing', 'original', 'goods', 'sometimes', 'renown', 'economy', 'london', 'goodness', 'horn', 'men', 'encountering', 'eloquence', 'sadness', 'saving', 'yet', 'no', 'superabundance', 'energy', 'simplicity', 'desire', 'favours', 'fault', 'grace', 'togetherness', 'pride', 'alterations', 'its', 'care', 'benefit', 'sanctity', 'rum', 'indignation', 'land', 'mind', 'death', 'gladness', 'smaller', 'dignity', 'imprudence', 'mountains', 'producing', 'herein', 'wrong', 'worlds', 'strides', 'acorn', 'events', 'profit', 'environment', 'numbers', 'great', 'authority', 'scarcity', 'expense', 'divinity', 'velocity', 'exportation', 'activity', 'place', 'art', 'without', 'distance', 'ned', 'supply', 'continuing', 'honourable', 'rank', 'intelligence', 'dexterity', 'almost', 'annual', 'dawn', 'surplus', 'practicality', 'glorious', 'must', 'poets', 'actually', 'sensation', 'went', 'quantity', 'force', 'action', 'might', 'rapture', 'tartness', 'thing', 'done', 'height', 'degrees', 'perhaps', 'evil', 'thoughts', 'lasting', 'pieces', 'circumstances', 'ordinary', 'service', 'individual', 'heat', 'influence', 'peril', 'greater', 'clerk', 'prospective', 'vessels', 'cheap', 'windbag', 'money', 'peace', 'left', 'corn', 'change', 'gifts', 'trade', 'indeed', 'confusion', 'less', 'confidence', 'jefferies', 'reduction', 'extensive', 'sums', 'none', 'gehenna', 'rice', 'latitude', 'france', 'view', 'splendour', 'nails', 'semblance', 'knave', 'disorders', 'wealth', 'slaves', 'augmentation', 'first', 'haste', 'sorrow', 'if', 'strain', 'facility', 'sufficient', 'perithous', 'employs', 'difficulties', 'effort', 'loss', 'claim', 'real', 'fixed', 'leader', 'whatever', 'rich', 'general', 'one', 'favours\\x94', 'lesser', 'whole', 'believed', 'return', 'quantities', 'among', 'weal', 'he', 'costage', 'consequence', 'dilatation', 'contemporary', 'america', 'comfort', 'advantage', 'personal', 'hope', 'at', 'wonder', 'content', 'antipathy', 'importation', 'riches', 'anyone', 'stock', 'and', 'to', 'success', 'length', 'opening', 'offence', 'rise', 'beginning', 'admiration', 'fast', 'enthusiasm', 'talents', 'little', 'inferiority', 'made', 'vanquished', 'modern', 'suited', 'balance', 'labourers', 'subconscious', 'past', 'incorporation', 'gain', 'either', 'restoration', 'countries', 'produce', 'waxen', 'common', 'fortitude', 'speed', 'kindness', 'professed', 'power', 'steps', 'chance', 'use', 'depth', 'interest', 'value', 'rapidity', 'want', 'enduring', 'guilds', 'age', 'honour', 'shame', 'flourish', 'pain', 'field', 'annoyance', 'mastery', 'love', 'cheapness', 'demand', 'share', 'therefore', 'woe', 'formerly', 'stocks', 'mans', 'zeal', 'second', 'fame', 'titan', 'security', 'extent', 'grew', 'upon', 'importance', 'fortune', 'clearness', 'worldly', 'fund', 'returns', 'pasture', 'liberty', 'price', 'sin', 'as', 'freedom', 'beauty', 'in', 'expected', 'brewery', 'crop', 'injudicious', 'necessary', 'contained', 'effect', 'seems', 'attractions', 'name', 'ii', 'such', 'silence', 'point', 'tenant', 'sum', 'lawe', 'group', 'sovereign', 'obstacles', 'otherwise', 'malversation', 'strength', 'every', 'evils', 'degree', 'though', 'fiercer', 'transgression', 'agony', 'reprobate', 'wiser', 'gold', 'equal', 'seignorage', 'tax', 'former', 'brightness', 'end', 'trespass', 'capital', 'prince', 'frequent', 'come', 'distress', 'weight', 'ever', 'found', 'melodies', 'pleasure', 'life', 'gift', 'boldness', 'far', 'harm', 'ones', 'relevance', 'inconveniency', 'require', 'it', 'well', 'fury', 'present', 'levying', 'moment', 'pomp', 'still', 'need', 'cost', 'delectation', 'triumph', 'time', 'desolation', 'crime'])\n",
      "\n",
      "Listing 20 most frequent words to come after 'said':\n",
      " [('part', 532), ('quantity', 105), ('number', 50), ('proportion', 43), ('value', 24), ('smaller', 16), ('greater', 16), ('share', 16), ('less', 12), ('profit', 11), ('revenue', 9), ('the', 9), ('importance', 9), ('capital', 9), ('surplus', 8), ('variety', 7), ('distance', 7), ('degree', 7), ('ease', 6), ('stock', 6)]\n"
     ]
    }
   ],
   "source": [
    "cfreq_2gram = nltk.ConditionalFreqDist(bigram_model)\n",
    "# print('Conditional Frequency Conditions:\\n', cfreq_2gram)\n",
    "print()\n",
    "\n",
    "# First access the FreqDist associated with \"one\", then the keys in that FreqDist\n",
    "print(\"Listing the words that can follow after 'greater':\\n\", cfreq_2gram[\"greater\"].keys())\n",
    "print()\n",
    "\n",
    "# Determine Most common in conditional frequency\n",
    "print(\"Listing 20 most frequent words to come after 'greater':\\n\", cfreq_2gram[\"greater\"].most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Language Model using KenLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-10-af07a94aa03d>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-af07a94aa03d>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    get_ipython().system('which kenlm')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade https://github.com/kpu/kenlm/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'kenlm'...\n",
      "remote: Counting objects: 7307, done.\u001b[K\n",
      "remote: Total 7307 (delta 0), reused 0 (delta 0), pack-reused 7307\u001b[K\n",
      "Receiving objects: 100% (7307/7307), 3.39 MiB | 1.28 MiB/s, done.\n",
      "Resolving deltas: 100% (3234/3234), done.\n",
      "~/Google Drive/Development/consulting/lexicon/kenlm ~/Google Drive/Development/consulting/lexicon\n",
      "~/Google Drive/Development/consulting/lexicon/kenlm/jam-files/engine ~/Google Drive/Development/consulting/lexicon/kenlm\n",
      "###\n",
      "### Using 'darwin' toolset.\n",
      "###\n",
      "rm -rf bootstrap\n",
      "mkdir bootstrap\n",
      "cc -o bootstrap/jam0 command.c compile.c constants.c debug.c function.c glob.c hash.c hdrmacro.c headers.c jam.c jambase.c jamgram.c lists.c make.c make1.c object.c option.c output.c parse.c pathunix.c regexp.c rules.c scan.c search.c subst.c timestamp.c variable.c modules.c strings.c filesys.c builtins.c pwd.c class.c native.c md5.c w32_getreg.c modules/set.c modules/path.c modules/regex.c modules/property-set.c modules/sequence.c modules/order.c execunix.c fileunix.c\n",
      "\u001b[1mfunction.c:3109:31: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mformat specifies type 'char *' but the argument has\n",
      "      type 'OBJECT *' (aka 'struct _object *') [-Wformat]\u001b[0m\n",
      "                printf( \"%s\", formal_arg->arg_name );\n",
      "\u001b[0;1;32m                         ~~   ^~~~~~~~~~~~~~~~~~~~\n",
      "\u001b[0m1 warning generated.\n",
      "\u001b[1mmake.c:273:37: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1madding 'int' to a string does not append to the string\n",
      "      [-Wstring-plus-int]\u001b[0m\n",
      "        printf( \"make\\t--\\t%s%s\\n\", spaces( depth ), object_str( t->name ) );\n",
      "\u001b[0;1;32m                                    ^~~~~~~~~~~~~~~\n",
      "\u001b[0m\u001b[1mmake.c:102:45: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro 'spaces'\u001b[0m\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "\u001b[0;1;32m                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "\u001b[0m\u001b[1mmake.c:273:37: \u001b[0m\u001b[0;1;30mnote: \u001b[0muse array indexing to silence this warning\u001b[0m\n",
      "\u001b[1mmake.c:102:45: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro 'spaces'\u001b[0m\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "\u001b[0;1;32m                                            ^\n",
      "\u001b[0m\u001b[1mmake.c:280:37: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1madding 'int' to a string does not append to the string\n",
      "      [-Wstring-plus-int]\u001b[0m\n",
      "        printf( \"make\\t--\\t%s%s\\n\", spaces( depth ), object_str( t->name ) );\n",
      "\u001b[0;1;32m                                    ^~~~~~~~~~~~~~~\n",
      "\u001b[0m\u001b[1mmake.c:102:45: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro 'spaces'\u001b[0m\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "\u001b[0;1;32m                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "\u001b[0m\u001b[1mmake.c:280:37: \u001b[0m\u001b[0;1;30mnote: \u001b[0muse array indexing to silence this warning\u001b[0m\n",
      "\u001b[1mmake.c:102:45: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro 'spaces'\u001b[0m\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "\u001b[0;1;32m                                            ^\n",
      "\u001b[0m\u001b[1mmake.c:351:17: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1madding 'int' to a string does not append to the string\n",
      "      [-Wstring-plus-int]\u001b[0m\n",
      "                spaces( depth ), object_str( t->name ), object_str( t->b...\n",
      "\u001b[0;1;32m                ^~~~~~~~~~~~~~~\n",
      "\u001b[0m\u001b[1mmake.c:102:45: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro 'spaces'\u001b[0m\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "\u001b[0;1;32m                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "\u001b[0m\u001b[1mmake.c:351:17: \u001b[0m\u001b[0;1;30mnote: \u001b[0muse array indexing to silence this warning\u001b[0m\n",
      "\u001b[1mmake.c:102:45: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro 'spaces'\u001b[0m\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "\u001b[0;1;32m                                            ^\n",
      "\u001b[0m\u001b[1mmake.c:359:17: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1madding 'int' to a string does not append to the string\n",
      "      [-Wstring-plus-int]\u001b[0m\n",
      "                spaces( depth ), object_str( t->name ), target_bind[ (in...\n",
      "\u001b[0;1;32m                ^~~~~~~~~~~~~~~\n",
      "\u001b[0m\u001b[1mmake.c:102:45: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro 'spaces'\u001b[0m\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "\u001b[0;1;32m                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "\u001b[0m\u001b[1mmake.c:359:17: \u001b[0m\u001b[0;1;30mnote: \u001b[0muse array indexing to silence this warning\u001b[0m\n",
      "\u001b[1mmake.c:102:45: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro 'spaces'\u001b[0m\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "\u001b[0;1;32m                                            ^\n",
      "\u001b[0m\u001b[1mmake.c:364:17: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1madding 'int' to a string does not append to the string\n",
      "      [-Wstring-plus-int]\u001b[0m\n",
      "                spaces( depth ), object_str( t->name ), ctime( &t->time ) );\n",
      "\u001b[0;1;32m                ^~~~~~~~~~~~~~~\n",
      "\u001b[0m\u001b[1mmake.c:102:45: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro 'spaces'\u001b[0m\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "\u001b[0;1;32m                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "\u001b[0m\u001b[1mmake.c:364:17: \u001b[0m\u001b[0;1;30mnote: \u001b[0muse array indexing to silence this warning\u001b[0m\n",
      "\u001b[1mmake.c:102:45: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro 'spaces'\u001b[0m\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "\u001b[0;1;32m                                            ^\n",
      "\u001b[0m\u001b[1mmake.c:645:13: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1madding 'int' to a string does not append to the string\n",
      "      [-Wstring-plus-int]\u001b[0m\n",
      "            spaces( depth ), object_str( t->name ) );\n",
      "\u001b[0;1;32m            ^~~~~~~~~~~~~~~\n",
      "\u001b[0m\u001b[1mmake.c:102:45: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro 'spaces'\u001b[0m\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "\u001b[0;1;32m                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "\u001b[0m\u001b[1mmake.c:645:13: \u001b[0m\u001b[0;1;30mnote: \u001b[0muse array indexing to silence this warning\u001b[0m\n",
      "\u001b[1mmake.c:102:45: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro 'spaces'\u001b[0m\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "\u001b[0;1;32m                                            ^\n",
      "\u001b[0m6 warnings generated.\n",
      "./bootstrap/jam0 -f build.jam --toolset=darwin --toolset-root= clean\n",
      "...found 1 target...\n",
      "...updating 1 target...\n",
      "...updated 1 target...\n",
      "./bootstrap/jam0 -f build.jam --toolset=darwin --toolset-root=\n",
      "...found 50 targets...\n",
      "...updating 3 targets...\n",
      "[MKDIR] bin.macosxx86_64\n",
      "[COMPILE] bin.macosxx86_64/b2\n",
      "function.c:3109:31: warning: format specifies type 'char *' but the argument has type 'OBJECT *' (aka 'struct _object *') [-Wformat]\n",
      "                printf( \"%s\", formal_arg->arg_name );\n",
      "                         ~~   ^~~~~~~~~~~~~~~~~~~~\n",
      "1 warning generated.\n",
      "make.c:273:37: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "        printf( \"make\\t--\\t%s%s\\n\", spaces( depth ), object_str( t->name ) );\n",
      "                                    ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:273:37: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:280:37: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "        printf( \"make\\t--\\t%s%s\\n\", spaces( depth ), object_str( t->name ) );\n",
      "                                    ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:280:37: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:351:17: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "                spaces( depth ), object_str( t->name ), object_str( t->boundname ) );\n",
      "                ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:351:17: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:359:17: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "                spaces( depth ), object_str( t->name ), target_bind[ (int) t->binding ] );\n",
      "                ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:359:17: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:364:17: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "                spaces( depth ), object_str( t->name ), ctime( &t->time ) );\n",
      "                ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:364:17: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:645:13: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "            spaces( depth ), object_str( t->name ) );\n",
      "            ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:645:13: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:682:39: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "        printf( \"->%s%2d Name: %s\\n\", spaces( depth ), depth, target_name( t ) );\n",
      "                                      ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:682:39: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:685:39: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "        printf( \"  %s%2d Name: %s\\n\", spaces( depth ), depth, target_name( t ) );\n",
      "                                      ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:685:39: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:690:38: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "        printf( \"  %s    Loc: %s\\n\", spaces( depth ), object_str( t->boundname ) );\n",
      "                                     ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:690:38: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:695:42: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "        printf( \"  %s       : Stable\\n\", spaces( depth ) );\n",
      "                                         ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:695:42: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:698:41: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "        printf( \"  %s       : Newer\\n\", spaces( depth ) );\n",
      "                                        ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:698:41: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:701:56: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "        printf( \"  %s       : Up to date temp file\\n\", spaces( depth ) );\n",
      "                                                       ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:701:56: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:704:65: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "        printf( \"  %s       : Temporary file, to be updated\\n\", spaces( depth ) );\n",
      "                                                                ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:704:65: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:707:61: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "        printf( \"  %s       : Been touched, updating it\\n\", spaces( depth ) );\n",
      "                                                            ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:707:61: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:710:56: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "        printf( \"  %s       : Missing, creating it\\n\", spaces( depth ) );\n",
      "                                                       ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:710:56: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:713:57: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "        printf( \"  %s       : Outdated, updating it\\n\", spaces( depth ) );\n",
      "                                                        ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:713:57: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:716:56: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "        printf( \"  %s       : Rebuild, updating it\\n\", spaces( depth ) );\n",
      "                                                       ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:716:56: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:719:47: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "        printf( \"  %s       : Updating it\\n\", spaces( depth ) );\n",
      "                                              ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:719:47: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:722:51: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "        printf( \"  %s       : Can not find it\\n\", spaces( depth ) );\n",
      "                                                  ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:722:51: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:725:47: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "        printf( \"  %s       : Can make it\\n\", spaces( depth ) );\n",
      "                                              ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:725:47: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:731:34: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "        printf( \"  %s       : \", spaces( depth ) );\n",
      "                                 ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:731:34: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "make.c:743:52: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]\n",
      "        printf( \"  %s       : Depends on %s (%s)\", spaces( depth ),\n",
      "                                                   ^~~~~~~~~~~~~~~\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                     ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n",
      "make.c:743:52: note: use array indexing to silence this warning\n",
      "make.c:102:45: note: expanded from macro 'spaces'\n",
      "# define spaces(x) ( \"                    \" + ( x > 20 ? 0 : 20-x ) )\n",
      "                                            ^\n",
      "22 warnings generated.\n",
      "[COPY] bin.macosxx86_64/bjam\n",
      "...updated 3 targets...\n",
      "~/Google Drive/Development/consulting/lexicon/kenlm\n",
      "Failed to run bash -c \"g++  -dM -x c++ -E /dev/null -include boost/version.hpp 2>/dev/null |grep '#define BOOST_'\"\n",
      "Boost does not seem to be installed or g++ is confused.\n"
     ]
    }
   ],
   "source": [
    "# ! git clone https://github.com/vchahun/kenlm.git && \\\n",
    "# pushd kenlm && \\\n",
    "# ./bjam && \\\n",
    "# python setup.py install && \\\n",
    "# popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Install to Terminal\n",
    "# !wget -O boost_1_60_0.tar.gz https://sourceforge.net/projects/boost/files/boost/1.60.0/boost_1_60_0.tar.gz/download && \\\n",
    "#     ./bootstrap.sh && \\\n",
    "#     ./bjam -j4\n",
    "# !./bj2 install\n",
    "\n",
    "# !wget -O - http://kheafield.com/code/kenlm.tar.gz |tar xz && \\\n",
    "#     cd kenlm && \\\n",
    "#     ./bjam -j4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk.model' has no attribute 'NgramModel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6dd58fc92f62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Build N-Gram Language model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLidstoneProbDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNgramModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'This'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'song'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'that'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'never'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ends'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'nltk.model' has no attribute 'NgramModel'"
     ]
    }
   ],
   "source": [
    "from nltk.model import NgramModel\n",
    "\n",
    "# Build N-Gram Language model\n",
    "est = lambda fdist, bins: LidstoneProbDist(fdist, 0.2)\n",
    "lm = NgramModel(3, tokenized_words, estimator=est)\n",
    "print(lm.entropy(['This', 'is', 'a', 'song', 'that', 'never', 'ends']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEMO - Evaluate Sentences Using Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for operation to complete...\n",
      "API Results:  [transcript: \"at this moment to the whole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\"\n",
      "confidence: 0.948432981967926\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"at\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"this\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"moment\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  word: \"to\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  word: \"the\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"whole\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "  }\n",
      "  word: \"soul\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  word: \"of\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  word: \"the\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"Old\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  word: \"Man\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "  }\n",
      "  word: \"scene\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  word: \"centered\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"in\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  word: \"his\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 4\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  word: \"eyes\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 4\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 4\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  word: \"which\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 4\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 4\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"became\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 4\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 5\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  word: \"bloodshot\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 5\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 5\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"the\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 5\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 5\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"veins\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 5\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 6\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  word: \"of\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 6\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 6\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  word: \"the\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 6\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 6\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  word: \"throat\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 6\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 6\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"swelled\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 6\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 7\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  word: \"his\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 7\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 7\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"cheeks\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 7\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 7\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"and\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 7\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 8\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  word: \"temples\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 8\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 8\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  word: \"became\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 8\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 8\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"purple\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 8\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 9\n",
      "  }\n",
      "  word: \"as\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 9\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 9\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  word: \"though\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 9\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 9\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  word: \"he\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 9\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 9\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  word: \"was\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 9\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 9\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  word: \"struck\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 9\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 9\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"with\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 9\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 10\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  word: \"epilepsy\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 10\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 10\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"nothing\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 10\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 11\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  word: \"was\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 11\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 11\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  word: \"wanting\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 11\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 11\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"to\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 11\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 11\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"complete\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 11\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 12\n",
      "  }\n",
      "  word: \"this\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 12\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 12\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  word: \"but\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 12\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 12\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  word: \"the\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 12\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 12\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"utterance\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 12\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 12\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"of\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 12\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 13\n",
      "  }\n",
      "  word: \"a\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 13\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 13\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  word: \"cry\"\n",
      "}\n",
      ", transcript: \"at this moment to the whole soul of the old man seemed centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\"\n",
      "confidence: 0.951474666595459\n",
      ", transcript: \"at this moment to the whole soul of the old man seem centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\"\n",
      "confidence: 0.9415977001190186\n",
      ", transcript: \"at this moment to the whole sole of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\"\n",
      "confidence: 0.9484882950782776\n",
      ", transcript: \"at this moment to the whole sole of the old man seemed centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\"\n",
      "confidence: 0.9531007409095764\n",
      ", transcript: \"at this moment to the whole sole of the old man seem centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\"\n",
      "confidence: 0.9432237148284912\n",
      ", transcript: \"at this moment to the whole soul of the old man seems centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\"\n",
      "confidence: 0.9610205888748169\n",
      ", transcript: \"at this moment to the whole song of The Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\"\n",
      "confidence: 0.9349242448806763\n",
      ", transcript: \"at this moment to the whole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this put the utterance of a cry\"\n",
      "confidence: 0.9473547339439392\n",
      ", transcript: \"at this moment to the hole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\"\n",
      "confidence: 0.9476993680000305\n",
      "]\n",
      "\n",
      "\n",
      "RE-RANKED Results: \n",
      " {'at this moment to the hole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.9081002712249755, 'at this moment to the whole sole of the old man seemed centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.91804185062646859, 'at this moment to the whole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this put the utterance of a cry': 0.90842670500278477, 'at this moment to the whole soul of the old man seemed centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.91686590313911431, 'at this moment to the whole sole of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.90913219824433322, 'at this moment to the whole sole of the old man seem centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.90858307033777241, 'at this moment to the whole song of The Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.89226038940250874, 'at this moment to the whole soul of the old man seem centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.90740718096494677, 'at this moment to the whole soul of the old man seems centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.92589252740144734, 'at this moment to the whole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.90944847464561462}\n",
      "\n",
      "\n",
      "ORIGINAL Transcript: \n",
      "'at this moment to the hole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry' \n",
      "with a confidence_score of: 0.9476993680000305\n",
      "RE-RANKED Transcript: \n",
      "'at this moment to the whole soul of the old man seems centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry' \n",
      "with a confidence_score of: 0.9258925274014473\n",
      "GROUND TRUTH TRANSCRIPT: \n",
      "AT THIS MOMENT THE WHOLE SOUL OF THE OLD MAN SEEMED CENTRED IN HIS EYES WHICH BECAME BLOODSHOT THE VEINS OF THE THROAT SWELLED HIS CHEEKS AND TEMPLES BECAME PURPLE AS THOUGH HE WAS STRUCK WITH EPILEPSY NOTHING WAS WANTING TO COMPLETE THIS BUT THE UTTERANCE OF A CRY\n",
      "\n",
      "The original transcript was RE-RANKED. The transcripts do not match!\n",
      "Differences between original and re-ranked:  ['scene', 'hole']\n",
      "\n",
      "\n",
      "The original transcript DOES NOT MATCH ground truth.\n",
      "Differences between original and ground truth:  ['scene', 'centered', 'hole']\n",
      "\n",
      "\n",
      "The RE_RANKED transcript DOES NOT MATCH ground truth.\n",
      "Differences between Reranked and ground truth:  ['centered', 'seems']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL Edit Distance: \n",
      "8\n",
      "RE-RANKED Edit Distance: \n",
      "6\n",
      "\n",
      "\n",
      "Waiting for operation to complete...\n",
      "API Results:  [transcript: \"but in less than five minutes the staircase groaned beneath an extraordinary weight\"\n",
      "confidence: 0.9087771773338318\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"but\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"in\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"less\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"than\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  word: \"five\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  word: \"minutes\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"the\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  word: \"staircase\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"groaned\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"beneath\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"an\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  word: \"extraordinary\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"weight\"\n",
      "}\n",
      ", transcript: \"but in less than 5 minutes the staircase groaned beneath an extraordinary weight\"\n",
      "confidence: 0.8721786737442017\n",
      ", transcript: \"but in less than five minutes the staircase groaned beneath an extraordinary wait\"\n",
      "confidence: 0.9087771773338318\n",
      ", transcript: \"but in less than 5 minutes the staircase groaned beneath an extraordinary wait\"\n",
      "confidence: 0.8926088809967041\n",
      ", transcript: \"but in less than five minutes the staircase ground beneath an extraordinary weight\"\n",
      "confidence: 0.8667746186256409\n",
      ", transcript: \"but in less than 5 minutes the staircase ground beneath an extraordinary weight\"\n",
      "confidence: 0.8506063222885132\n",
      ", transcript: \"but in less than five minutes the staircase ground beneath an extraordinary wait\"\n",
      "confidence: 0.8667746186256409\n",
      ", transcript: \"but in less than five minutes the staircase Grande beneath an extraordinary weight\"\n",
      "confidence: 0.9208987355232239\n",
      ", transcript: \"but in less than 5 minutes the staircase ground beneath an extraordinary wait\"\n",
      "confidence: 0.8506063222885132\n",
      ", transcript: \"but in less than 5 minutes the staircase Grande beneath an extraordinary weight\"\n",
      "confidence: 0.904730498790741\n",
      "]\n",
      "\n",
      "\n",
      "RE-RANKED Results: \n",
      " {'but in less than 5 minutes the staircase Grande beneath an extraordinary weight': 0.86024251016788178, 'but in less than 5 minutes the staircase groaned beneath an extraordinary wait': 0.84872697326354674, 'but in less than five minutes the staircase ground beneath an extraordinary wait': 0.82771951705217361, 'but in less than 5 minutes the staircase groaned beneath an extraordinary weight': 0.82931827637366951, 'but in less than five minutes the staircase groaned beneath an extraordinary wait': 0.86744813546538346, 'but in less than five minutes the staircase Grande beneath an extraordinary weight': 0.8789636157453059, 'but in less than five minutes the staircase groaned beneath an extraordinary weight': 0.86744813546538346, 'but in less than five minutes the staircase ground beneath an extraordinary weight': 0.82771951705217361, 'but in less than 5 minutes the staircase ground beneath an extraordinary wait': 0.80899835480377069, 'but in less than 5 minutes the staircase ground beneath an extraordinary weight': 0.80899835480377069}\n",
      "\n",
      "\n",
      "ORIGINAL Transcript: \n",
      "'but in less than 5 minutes the staircase Grande beneath an extraordinary weight' \n",
      "with a confidence_score of: 0.904730498790741\n",
      "RE-RANKED Transcript: \n",
      "'but in less than five minutes the staircase Grande beneath an extraordinary weight' \n",
      "with a confidence_score of: 0.8789636157453059\n",
      "GROUND TRUTH TRANSCRIPT: \n",
      "BUT IN LESS THAN FIVE MINUTES THE STAIRCASE GROANED BENEATH AN EXTRAORDINARY WEIGHT\n",
      "\n",
      "The original transcript was RE-RANKED. The transcripts do not match!\n",
      "Differences between original and re-ranked:  ['5']\n",
      "\n",
      "\n",
      "The original transcript DOES NOT MATCH ground truth.\n",
      "Differences between original and ground truth:  ['5', 'grande']\n",
      "\n",
      "\n",
      "The RE_RANKED transcript DOES NOT MATCH ground truth.\n",
      "Differences between Reranked and ground truth:  ['grande']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL Edit Distance: \n",
      "7\n",
      "RE-RANKED Edit Distance: \n",
      "3\n",
      "\n",
      "\n",
      "Waiting for operation to complete...\n",
      "API Results:  [transcript: \"go do you hear\"\n",
      "confidence: 0.8830462694168091\n",
      "words {\n",
      "  start_time {\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  word: \"go\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"do\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"you\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"hear\"\n",
      "}\n",
      ", transcript: \"go do you here\"\n",
      "confidence: 0.9464384913444519\n",
      ", transcript: \"so do you hear\"\n",
      "confidence: 0.9483599662780762\n",
      ", transcript: \"no do you hear\"\n",
      "confidence: 0.9307782053947449\n",
      "]\n",
      "\n",
      "\n",
      "RE-RANKED Results: \n",
      " {'no do you hear': 0.88448826898820687, 'go do you here': 0.89926208801334717, 'so do you hear': 0.90119094182737169, 'go do you hear': 0.83915772710461167}\n",
      "\n",
      "\n",
      "ORIGINAL Transcript: \n",
      "'no do you hear' \n",
      "with a confidence_score of: 0.9307782053947449\n",
      "RE-RANKED Transcript: \n",
      "'so do you hear' \n",
      "with a confidence_score of: 0.9011909418273717\n",
      "GROUND TRUTH TRANSCRIPT: \n",
      "GO DO YOU HEAR\n",
      "\n",
      "The original transcript was RE-RANKED. The transcripts do not match!\n",
      "Differences between original and re-ranked:  ['no']\n",
      "\n",
      "\n",
      "The original transcript DOES NOT MATCH ground truth.\n",
      "Differences between original and ground truth:  ['no']\n",
      "\n",
      "\n",
      "The RE_RANKED transcript DOES NOT MATCH ground truth.\n",
      "Differences between Reranked and ground truth:  ['so']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL Edit Distance: \n",
      "1\n",
      "RE-RANKED Edit Distance: \n",
      "1\n",
      "\n",
      "\n",
      "Waiting for operation to complete...\n",
      "API Results:  [transcript: \"and the cry issued from his pores if we made us speak a cry frightful in its silence\"\n",
      "confidence: 0.9140974879264832\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"and\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"the\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"cry\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"issued\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  word: \"from\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"his\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  word: \"pores\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "  }\n",
      "  word: \"if\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  word: \"we\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  word: \"made\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"us\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"speak\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 4\n",
      "  }\n",
      "  word: \"a\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 4\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 4\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"cry\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 4\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 5\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  word: \"frightful\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 5\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 5\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"in\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 5\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 5\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"its\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 5\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 6\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  word: \"silence\"\n",
      "}\n",
      ", transcript: \"and the cry issued from his pores if we made us speak a cry frightful in it silence\"\n",
      "confidence: 0.9043365120887756\n",
      ", transcript: \"and the cry issued from his pores if we made us peek a cry frightful in its silence\"\n",
      "confidence: 0.8717085719108582\n",
      ", transcript: \"and the cry issued from his pores if we made us speak a cry frightful and its silence\"\n",
      "confidence: 0.8714524507522583\n",
      ", transcript: \"and the cry issued from his pores if we made us peek a cry frightful in it silence\"\n",
      "confidence: 0.8600011467933655\n",
      ", transcript: \"and the cry issued from his pores if we made us speak a cry frightful and it silence\"\n",
      "confidence: 0.8597450256347656\n",
      ", transcript: \"and the cry issued from his pores if we made us speak a cry frightful init silence\"\n",
      "confidence: 0.9458029270172119\n",
      ", transcript: \"and the cry issued from his pores if we made us speak a cry frightful in it silenced\"\n",
      "confidence: 0.9043365120887756\n",
      ", transcript: \"and the cry issued from his pores if we made us peek a cry frightful and its silence\"\n",
      "confidence: 0.8271170854568481\n",
      ", transcript: \"and the cry issued from his pores if we made us peek a cry frightful and it silence\"\n",
      "confidence: 0.8154096603393555\n",
      "]\n",
      "\n",
      "\n",
      "RE-RANKED Results: \n",
      " {'and the cry issued from his pores if we made us peek a cry frightful and its silence': 0.78660132335498922, 'and the cry issued from his pores if we made us speak a cry frightful in its silence': 0.86929597854614249, 'and the cry issued from his pores if we made us peek a cry frightful in it silence': 0.81784991631284354, 'and the cry issued from his pores if we made us speak a cry frightful and it silence': 0.81766114430502057, 'and the cry issued from his pores if we made us speak a cry frightful in it silenced': 0.86003179121762507, 'and the cry issued from his pores if we made us peek a cry frightful in its silence': 0.82896323045715692, 'and the cry issued from his pores if we made us speak a cry frightful in it silence': 0.86003179121762507, 'and the cry issued from his pores if we made us speak a cry frightful init silence': 0.89941614568233486, 'and the cry issued from his pores if we made us speak a cry frightful and its silence': 0.82878319816663859, 'and the cry issued from his pores if we made us peek a cry frightful and it silence': 0.7754792694933712}\n",
      "\n",
      "\n",
      "ORIGINAL Transcript: \n",
      "'and the cry issued from his pores if we made us peek a cry frightful and it silence' \n",
      "with a confidence_score of: 0.8154096603393555\n",
      "RE-RANKED Transcript: \n",
      "'and the cry issued from his pores if we made us speak a cry frightful init silence' \n",
      "with a confidence_score of: 0.8994161456823349\n",
      "GROUND TRUTH TRANSCRIPT: \n",
      "AND THE CRY ISSUED FROM HIS PORES IF WE MAY THUS SPEAK A CRY FRIGHTFUL IN ITS SILENCE\n",
      "\n",
      "The original transcript was RE-RANKED. The transcripts do not match!\n",
      "Differences between original and re-ranked:  ['peek', 'it']\n",
      "\n",
      "\n",
      "The original transcript DOES NOT MATCH ground truth.\n",
      "Differences between original and ground truth:  ['made', 'peek', 'it', 'us']\n",
      "\n",
      "\n",
      "The RE_RANKED transcript DOES NOT MATCH ground truth.\n",
      "Differences between Reranked and ground truth:  ['made', 'init', 'us']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL Edit Distance: \n",
      "9\n",
      "RE-RANKED Edit Distance: \n",
      "6\n",
      "\n",
      "\n",
      "Waiting for operation to complete...\n",
      "API Results:  [transcript: \"deveny Rush towards the old man and made him and Halo powerful restorative\"\n",
      "confidence: 0.8434420228004456\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  word: \"deveny\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "  }\n",
      "  word: \"Rush\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  word: \"towards\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  word: \"the\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"old\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"man\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "  }\n",
      "  word: \"and\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  word: \"made\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  word: \"him\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"and\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"Halo\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  word: \"powerful\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"restorative\"\n",
      "}\n",
      ", transcript: \"deveney Rush towards the old man and made him and Halo powerful restorative\"\n",
      "confidence: 0.8541163802146912\n",
      ", transcript: \"deveny rushed towards the old man and made him and Halo powerful restorative\"\n",
      "confidence: 0.8281424045562744\n",
      ", transcript: \"deveny Rush towards the old man and made him and hail powerful restorative\"\n",
      "confidence: 0.7980682849884033\n",
      ", transcript: \"deveney Rush towards the old man and made him and hail powerful restorative\"\n",
      "confidence: 0.8087425231933594\n",
      ", transcript: \"deveny rushed towards the old man and made him and hail powerful restorative\"\n",
      "confidence: 0.7827685475349426\n",
      ", transcript: \"deveney rushed towards the old man and made him and Halo powerful restorative\"\n",
      "confidence: 0.8448580503463745\n",
      ", transcript: \"deveny Rush towards the old man and made him and Hale powerful restorative\"\n",
      "confidence: 0.8532139658927917\n",
      ", transcript: \"devony Rush towards the old man and made him and Halo powerful restorative\"\n",
      "confidence: 0.8753230571746826\n",
      ", transcript: \"deveney Rush towards the old man and made him and Hale powerful restorative\"\n",
      "confidence: 0.8638883829116821\n",
      "]\n",
      "\n",
      "\n",
      "RE-RANKED Results: \n",
      " {'deveny rushed towards the old man and made him and hail powerful restorative': 0.75041957125067704, 'deveney Rush towards the old man and made him and hail powerful restorative': 0.77479364275932305, 'deveney Rush towards the old man and made him and Hale powerful restorative': 0.8271822094917296, 'deveny Rush towards the old man and made him and Hale powerful restorative': 0.81704151332378383, 'deveny Rush towards the old man and made him and hail powerful restorative': 0.76465311646461476, 'deveney rushed towards the old man and made him and Halo powerful restorative': 0.80940459892153738, 'deveney Rush towards the old man and made him and Halo powerful restorative': 0.81789880692958827, 'deveny Rush towards the old man and made him and Halo powerful restorative': 0.80775816738605488, 'deveny rushed towards the old man and made him and Halo powerful restorative': 0.79352473542094226, 'devony Rush towards the old man and made him and Halo powerful restorative': 0.83804515004158009}\n",
      "\n",
      "\n",
      "ORIGINAL Transcript: \n",
      "'deveney Rush towards the old man and made him and Hale powerful restorative' \n",
      "with a confidence_score of: 0.8638883829116821\n",
      "RE-RANKED Transcript: \n",
      "'devony Rush towards the old man and made him and Halo powerful restorative' \n",
      "with a confidence_score of: 0.8380451500415801\n",
      "GROUND TRUTH TRANSCRIPT: \n",
      "D'AVRIGNY RUSHED TOWARDS THE OLD MAN AND MADE HIM INHALE A POWERFUL RESTORATIVE\n",
      "\n",
      "The original transcript was RE-RANKED. The transcripts do not match!\n",
      "Differences between original and re-ranked:  ['hale', 'deveney']\n",
      "\n",
      "\n",
      "The original transcript DOES NOT MATCH ground truth.\n",
      "Differences between original and ground truth:  ['hale', 'deveney', 'rush']\n",
      "\n",
      "\n",
      "The RE_RANKED transcript DOES NOT MATCH ground truth.\n",
      "Differences between Reranked and ground truth:  ['devony', 'halo', 'rush']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL Edit Distance: \n",
      "13\n",
      "RE-RANKED Edit Distance: \n",
      "13\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each word in the evaluation list:\n",
    "# Select word and determine its frequency distribution\n",
    "# Grab probability of second word in the list\n",
    "# Continue this process until the sentence is scored\n",
    "\n",
    "# Add small epsilon value to avoid division by zero\n",
    "epsilon = 0.0000001\n",
    "\n",
    "# Loads the audio into memory\n",
    "for audio, ground_truth in audio_files.items():\n",
    "    with io.open(audio, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "        audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code='en-US',\n",
    "        max_alternatives=10,\n",
    "        profanity_filter=False,\n",
    "        enable_word_time_offsets=True)\n",
    "\n",
    "    # Detects speech and words in the audio file\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "\n",
    "    print('Waiting for operation to complete...')\n",
    "    result = operation.result(timeout=90)\n",
    "\n",
    "    alternatives = result.results[0].alternatives\n",
    "\n",
    "\n",
    "    print(\"API Results: \", alternatives)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    rerank_results = {}\n",
    "    for alternative in alternatives:\n",
    "        sent = alternative.transcript\n",
    "\n",
    "        words = nltk.tokenize.word_tokenize(sent)\n",
    "        probs = np.ones_like(words, dtype=np.float32)*epsilon\n",
    "        # print(words,'\\n',probs)\n",
    "        for word in words:\n",
    "            if words.index(word) < len(words)-1: \n",
    "                freq = cfreq_2gram[word].freq(words[words.index(word)+1])\n",
    "                probs[words.index(word)] = freq\n",
    "            # print(probs)\n",
    "\n",
    "        lexicon_score = np.sum(probs)\n",
    "        # print(word_score)\n",
    "\n",
    "        # Re-rank alternatives using a weighted average of the two scores\n",
    "        api_weight = 0.95\n",
    "        confidence_score = alternative.confidence*api_weight + lexicon_score*(1-api_weight)\n",
    "        rerank_results[alternative.transcript] = confidence_score\n",
    "\n",
    "    print(\"RE-RANKED Results: \\n\", rerank_results)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    import operator\n",
    "    index, value = max(enumerate(list(rerank_results.values())), key=operator.itemgetter(1))\n",
    "    # Select Corresponding Transcript:\n",
    "    script=''\n",
    "    for trnscript, confidence in rerank_results.items():\n",
    "        if confidence == value:\n",
    "            script = trnscript\n",
    "\n",
    "    # Evaluate the differences between the Original and the Reranked transcript:\n",
    "    print(\"ORIGINAL Transcript: \\n'{0}' \\nwith a confidence_score of: {1}\".format(alternative.transcript, alternative.confidence))\n",
    "    print(\"RE-RANKED Transcript: \\n'{0}' \\nwith a confidence_score of: {1}\".format(script, value))\n",
    "    print(\"GROUND TRUTH TRANSCRIPT: \\n{0}\".format(ground_truth))\n",
    "    print()\n",
    "    ranked_differences = list(set(nltk.tokenize.word_tokenize(alternative.transcript.lower())) -\n",
    "                              set(nltk.tokenize.word_tokenize(script.lower())))\n",
    "    if len(ranked_differences) == 0:  \n",
    "        print(\"No reranking was performed. The transcripts match!\")\n",
    "    else:\n",
    "        print(\"The original transcript was RE-RANKED. The transcripts do not match!\")\n",
    "        print(\"Differences between original and re-ranked: \", ranked_differences)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    # Evaluate Differences between the Original and Ground Truth:\n",
    "    gt_orig_diff = list(set(nltk.tokenize.word_tokenize(alternative.transcript.lower())) -\n",
    "                              set(nltk.tokenize.word_tokenize(ground_truth.lower())))\n",
    "    if len(gt_orig_diff) == 0:  \n",
    "        print(\"The ORIGINAL transcript matches ground truth!\")\n",
    "    else:\n",
    "        print(\"The original transcript DOES NOT MATCH ground truth.\")\n",
    "        print(\"Differences between original and ground truth: \", gt_orig_diff)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    gt_rr_diff = list(set(nltk.tokenize.word_tokenize(script.lower())) -\n",
    "                              set(nltk.tokenize.word_tokenize(ground_truth.lower())))\n",
    "    if len(gt_rr_diff) == 0:  \n",
    "        print(\"The RE-RANKED transcript matches ground truth!\")\n",
    "    else:\n",
    "        print(\"The RE_RANKED transcript DOES NOT MATCH ground truth.\")\n",
    "        print(\"Differences between Reranked and ground truth: \", gt_rr_diff)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # Compute the Levenshtein Distance (a.k.a. Edit Distance)\n",
    "#     import nltk.metrics.distance as lev_dist\n",
    "    \n",
    "    # Google API Edit Distance\n",
    "    goog_edit_distance = nltk.edit_distance(alternative.transcript.lower(), ground_truth.lower())\n",
    "    \n",
    "    # Re-Ranked Edit Distance\n",
    "    rr_edit_distance = nltk.edit_distance(script.lower(), ground_truth.lower())\n",
    "\n",
    "    \n",
    "    print(\"ORIGINAL Edit Distance: \\n{0}\".format(goog_edit_distance))\n",
    "    print(\"RE-RANKED Edit Distance: \\n{0}\".format(rr_edit_distance))\n",
    "    print()\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 97 text files in the directory: /Users/deanmwebb/Google Drive/Development/consulting/lexicon/LibriSpeech/dev-clean/**/*.txt\n",
      "10 Transcripts Processed.\n",
      "Average API Accuracy: 0.903240679607\n",
      "Average Custom Model Accuracy: 0.901859249805\n",
      "\n",
      "20 Transcripts Processed.\n",
      "Average API Accuracy: 0.918353468066\n",
      "Average Custom Model Accuracy: 0.936690391278\n",
      "\n",
      "30 Transcripts Processed.\n",
      "Average API Accuracy: 0.929826596805\n",
      "Average Custom Model Accuracy: 0.947840839505\n",
      "\n",
      "40 Transcripts Processed.\n",
      "Average API Accuracy: 0.928767158398\n",
      "Average Custom Model Accuracy: 0.941303845343\n",
      "\n",
      "50 Transcripts Processed.\n",
      "Average API Accuracy: 0.922505902324\n",
      "Average Custom Model Accuracy: 0.941705758542\n",
      "\n",
      "60 Transcripts Processed.\n",
      "Average API Accuracy: 0.926741081298\n",
      "Average Custom Model Accuracy: 0.94743754375\n",
      "\n",
      "70 Transcripts Processed.\n",
      "Average API Accuracy: 0.928304473218\n",
      "Average Custom Model Accuracy: 0.950507872917\n",
      "\n",
      "80 Transcripts Processed.\n",
      "Average API Accuracy: 0.927884314036\n",
      "Average Custom Model Accuracy: 0.951970949538\n",
      "\n",
      "90 Transcripts Processed.\n",
      "Average API Accuracy: 0.931461541754\n",
      "Average Custom Model Accuracy: 0.955575861477\n",
      "\n",
      "100 Transcripts Processed.\n",
      "Average API Accuracy: 0.932289825935\n",
      "Average Custom Model Accuracy: 0.956552296097\n",
      "\n",
      "110 Transcripts Processed.\n",
      "Average API Accuracy: 0.931687082465\n",
      "Average Custom Model Accuracy: 0.954465064124\n",
      "\n",
      "120 Transcripts Processed.\n",
      "Average API Accuracy: 0.92841243576\n",
      "Average Custom Model Accuracy: 0.949485999312\n",
      "\n",
      "130 Transcripts Processed.\n",
      "Average API Accuracy: 0.929766427831\n",
      "Average Custom Model Accuracy: 0.947221616095\n",
      "\n",
      "140 Transcripts Processed.\n",
      "Average API Accuracy: 0.926463354016\n",
      "Average Custom Model Accuracy: 0.944120655654\n",
      "\n",
      "150 Transcripts Processed.\n",
      "Average API Accuracy: 0.928987329366\n",
      "Average Custom Model Accuracy: 0.938826459104\n",
      "\n",
      "160 Transcripts Processed.\n",
      "Average API Accuracy: 0.92622768634\n",
      "Average Custom Model Accuracy: 0.936823919178\n",
      "\n",
      "170 Transcripts Processed.\n",
      "Average API Accuracy: 0.928444796603\n",
      "Average Custom Model Accuracy: 0.938327320237\n",
      "\n",
      "180 Transcripts Processed.\n",
      "Average API Accuracy: 0.928453453572\n",
      "Average Custom Model Accuracy: 0.938808155083\n",
      "\n",
      "190 Transcripts Processed.\n",
      "Average API Accuracy: 0.929513715354\n",
      "Average Custom Model Accuracy: 0.940244068559\n",
      "\n",
      "200 Transcripts Processed.\n",
      "Average API Accuracy: 0.930480181244\n",
      "Average Custom Model Accuracy: 0.941005066938\n",
      "\n",
      "210 Transcripts Processed.\n",
      "Average API Accuracy: 0.924226455431\n",
      "Average Custom Model Accuracy: 0.934748896308\n",
      "\n",
      "220 Transcripts Processed.\n",
      "Average API Accuracy: 0.923679155518\n",
      "Average Custom Model Accuracy: 0.934905078407\n",
      "\n",
      "230 Transcripts Processed.\n",
      "Average API Accuracy: 0.92376882862\n",
      "Average Custom Model Accuracy: 0.934972597878\n",
      "\n",
      "240 Transcripts Processed.\n",
      "Average API Accuracy: 0.921917051067\n",
      "Average Custom Model Accuracy: 0.932329061465\n",
      "\n",
      "250 Transcripts Processed.\n",
      "Average API Accuracy: 0.921991798038\n",
      "Average Custom Model Accuracy: 0.933176779361\n",
      "\n",
      "260 Transcripts Processed.\n",
      "Average API Accuracy: 0.921426162578\n",
      "Average Custom Model Accuracy: 0.932532065323\n",
      "\n",
      "270 Transcripts Processed.\n",
      "Average API Accuracy: 0.920474971027\n",
      "Average Custom Model Accuracy: 0.932503237157\n",
      "\n",
      "280 Transcripts Processed.\n",
      "Average API Accuracy: 0.920528285014\n",
      "Average Custom Model Accuracy: 0.932691930447\n",
      "\n",
      "290 Transcripts Processed.\n",
      "Average API Accuracy: 0.920225760699\n",
      "Average Custom Model Accuracy: 0.932235321356\n",
      "\n",
      "300 Transcripts Processed.\n",
      "Average API Accuracy: 0.920943678942\n",
      "Average Custom Model Accuracy: 0.932780080825\n",
      "\n",
      "310 Transcripts Processed.\n",
      "Average API Accuracy: 0.920661805994\n",
      "Average Custom Model Accuracy: 0.932644050826\n",
      "\n",
      "320 Transcripts Processed.\n",
      "Average API Accuracy: 0.920338369948\n",
      "Average Custom Model Accuracy: 0.932363877515\n",
      "\n",
      "330 Transcripts Processed.\n",
      "Average API Accuracy: 0.921388349615\n",
      "Average Custom Model Accuracy: 0.933508949774\n",
      "\n",
      "340 Transcripts Processed.\n",
      "Average API Accuracy: 0.922062012855\n",
      "Average Custom Model Accuracy: 0.93478877157\n",
      "\n",
      "350 Transcripts Processed.\n",
      "Average API Accuracy: 0.922209889652\n",
      "Average Custom Model Accuracy: 0.93489134889\n",
      "\n",
      "360 Transcripts Processed.\n",
      "Average API Accuracy: 0.92246891763\n",
      "Average Custom Model Accuracy: 0.93554374807\n",
      "\n",
      "370 Transcripts Processed.\n",
      "Average API Accuracy: 0.92257259124\n",
      "Average Custom Model Accuracy: 0.936500324987\n",
      "\n",
      "380 Transcripts Processed.\n",
      "Average API Accuracy: 0.92092702199\n",
      "Average Custom Model Accuracy: 0.93492667951\n",
      "\n",
      "390 Transcripts Processed.\n",
      "Average API Accuracy: 0.92202683573\n",
      "Average Custom Model Accuracy: 0.935823554137\n",
      "\n",
      "400 Transcripts Processed.\n",
      "Average API Accuracy: 0.921689640138\n",
      "Average Custom Model Accuracy: 0.935778178947\n",
      "\n",
      "410 Transcripts Processed.\n",
      "Average API Accuracy: 0.921750247784\n",
      "Average Custom Model Accuracy: 0.935458491426\n",
      "\n",
      "420 Transcripts Processed.\n",
      "Average API Accuracy: 0.922161281703\n",
      "Average Custom Model Accuracy: 0.936154529935\n",
      "\n",
      "430 Transcripts Processed.\n",
      "Average API Accuracy: 0.922949154953\n",
      "Average Custom Model Accuracy: 0.936831885539\n",
      "\n",
      "440 Transcripts Processed.\n",
      "Average API Accuracy: 0.923050758681\n",
      "Average Custom Model Accuracy: 0.937557505729\n",
      "\n",
      "450 Transcripts Processed.\n",
      "Average API Accuracy: 0.923488458299\n",
      "Average Custom Model Accuracy: 0.937584013473\n",
      "\n",
      "460 Transcripts Processed.\n",
      "Average API Accuracy: 0.923615341394\n",
      "Average Custom Model Accuracy: 0.938070691992\n",
      "\n",
      "470 Transcripts Processed.\n",
      "Average API Accuracy: 0.92408538144\n",
      "Average Custom Model Accuracy: 0.93848523152\n",
      "\n",
      "480 Transcripts Processed.\n",
      "Average API Accuracy: 0.923967587937\n",
      "Average Custom Model Accuracy: 0.93863458405\n",
      "\n",
      "490 Transcripts Processed.\n",
      "Average API Accuracy: 0.924104209284\n",
      "Average Custom Model Accuracy: 0.939053572414\n",
      "\n",
      "500 Transcripts Processed.\n",
      "Average API Accuracy: 0.92367107553\n",
      "Average Custom Model Accuracy: 0.938824367726\n",
      "\n",
      "510 Transcripts Processed.\n",
      "Average API Accuracy: 0.921805597943\n",
      "Average Custom Model Accuracy: 0.93724442029\n",
      "\n",
      "520 Transcripts Processed.\n",
      "Average API Accuracy: 0.92102253447\n",
      "Average Custom Model Accuracy: 0.937148501636\n",
      "\n",
      "530 Transcripts Processed.\n",
      "Average API Accuracy: 0.921158829081\n",
      "Average Custom Model Accuracy: 0.937059989347\n",
      "\n",
      "540 Transcripts Processed.\n",
      "Average API Accuracy: 0.92083508977\n",
      "Average Custom Model Accuracy: 0.936362216273\n",
      "\n",
      "550 Transcripts Processed.\n",
      "Average API Accuracy: 0.920832492703\n",
      "Average Custom Model Accuracy: 0.936588690986\n",
      "\n",
      "560 Transcripts Processed.\n",
      "Average API Accuracy: 0.920133649087\n",
      "Average Custom Model Accuracy: 0.936930738918\n",
      "\n",
      "570 Transcripts Processed.\n",
      "Average API Accuracy: 0.919156191727\n",
      "Average Custom Model Accuracy: 0.935684205545\n",
      "\n",
      "580 Transcripts Processed.\n",
      "Average API Accuracy: 0.91868247161\n",
      "Average Custom Model Accuracy: 0.935409101639\n",
      "\n",
      "590 Transcripts Processed.\n",
      "Average API Accuracy: 0.917772498251\n",
      "Average Custom Model Accuracy: 0.934365370485\n",
      "\n",
      "600 Transcripts Processed.\n",
      "Average API Accuracy: 0.917091522671\n",
      "Average Custom Model Accuracy: 0.933759357349\n",
      "\n",
      "610 Transcripts Processed.\n",
      "Average API Accuracy: 0.917500208168\n",
      "Average Custom Model Accuracy: 0.934213779142\n",
      "\n",
      "620 Transcripts Processed.\n",
      "Average API Accuracy: 0.917686693882\n",
      "Average Custom Model Accuracy: 0.934818712124\n",
      "\n",
      "630 Transcripts Processed.\n",
      "Average API Accuracy: 0.917467049856\n",
      "Average Custom Model Accuracy: 0.934629614646\n",
      "\n",
      "640 Transcripts Processed.\n",
      "Average API Accuracy: 0.917456663338\n",
      "Average Custom Model Accuracy: 0.934491992665\n",
      "\n",
      "650 Transcripts Processed.\n",
      "Average API Accuracy: 0.917911472082\n",
      "Average Custom Model Accuracy: 0.935112375751\n",
      "\n",
      "660 Transcripts Processed.\n",
      "Average API Accuracy: 0.918299584354\n",
      "Average Custom Model Accuracy: 0.935376245122\n",
      "\n",
      "670 Transcripts Processed.\n",
      "Average API Accuracy: 0.917384574076\n",
      "Average Custom Model Accuracy: 0.9342850357\n",
      "\n",
      "680 Transcripts Processed.\n",
      "Average API Accuracy: 0.917151446245\n",
      "Average Custom Model Accuracy: 0.934235047041\n",
      "\n",
      "690 Transcripts Processed.\n",
      "Average API Accuracy: 0.917602679915\n",
      "Average Custom Model Accuracy: 0.934437326134\n",
      "\n",
      "700 Transcripts Processed.\n",
      "Average API Accuracy: 0.917820963114\n",
      "Average Custom Model Accuracy: 0.934511287298\n",
      "\n",
      "710 Transcripts Processed.\n",
      "Average API Accuracy: 0.917816423865\n",
      "Average Custom Model Accuracy: 0.934535924093\n",
      "\n",
      "720 Transcripts Processed.\n",
      "Average API Accuracy: 0.917821320348\n",
      "Average Custom Model Accuracy: 0.934593953144\n",
      "\n",
      "730 Transcripts Processed.\n",
      "Average API Accuracy: 0.916924529892\n",
      "Average Custom Model Accuracy: 0.933726706875\n",
      "\n",
      "740 Transcripts Processed.\n",
      "Average API Accuracy: 0.917436122379\n",
      "Average Custom Model Accuracy: 0.933751074212\n",
      "\n",
      "750 Transcripts Processed.\n",
      "Average API Accuracy: 0.917220025499\n",
      "Average Custom Model Accuracy: 0.93310417082\n",
      "\n",
      "760 Transcripts Processed.\n",
      "Average API Accuracy: 0.917388777142\n",
      "Average Custom Model Accuracy: 0.933403210418\n",
      "\n",
      "770 Transcripts Processed.\n",
      "Average API Accuracy: 0.917119628111\n",
      "Average Custom Model Accuracy: 0.932919855993\n",
      "\n",
      "780 Transcripts Processed.\n",
      "Average API Accuracy: 0.917131295891\n",
      "Average Custom Model Accuracy: 0.93317722704\n",
      "\n",
      "790 Transcripts Processed.\n",
      "Average API Accuracy: 0.916037412803\n",
      "Average Custom Model Accuracy: 0.932021975998\n",
      "\n",
      "800 Transcripts Processed.\n",
      "Average API Accuracy: 0.915543375424\n",
      "Average Custom Model Accuracy: 0.931397649052\n",
      "\n",
      "810 Transcripts Processed.\n",
      "Average API Accuracy: 0.915620371007\n",
      "Average Custom Model Accuracy: 0.931399193246\n",
      "\n",
      "820 Transcripts Processed.\n",
      "Average API Accuracy: 0.916030611919\n",
      "Average Custom Model Accuracy: 0.931703551171\n",
      "\n",
      "830 Transcripts Processed.\n",
      "Average API Accuracy: 0.916073798368\n",
      "Average Custom Model Accuracy: 0.931639741796\n",
      "\n",
      "840 Transcripts Processed.\n",
      "Average API Accuracy: 0.916463510765\n",
      "Average Custom Model Accuracy: 0.931840770858\n",
      "\n",
      "850 Transcripts Processed.\n",
      "Average API Accuracy: 0.916800032405\n",
      "Average Custom Model Accuracy: 0.93197923989\n",
      "\n",
      "860 Transcripts Processed.\n",
      "Average API Accuracy: 0.916933343173\n",
      "Average Custom Model Accuracy: 0.932263043487\n",
      "\n",
      "870 Transcripts Processed.\n",
      "Average API Accuracy: 0.917055957951\n",
      "Average Custom Model Accuracy: 0.932511174321\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_DeadlineExceededError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/google/gax/retry.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mto_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_timeout_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mto_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/google/gax/retry.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mupdated_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0ma_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mupdated_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/google/gax/__init__.py\u001b[0m in \u001b[0;36m_done_check\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0m_DeadlineExceededError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_DeadlineExceededError\u001b[0m: Deadline Exceeded",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-680-b22b73ed37cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Detects speech and words in the audio file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0moperation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong_running_recognize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0malternatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malternatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/google/gax/__init__.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \"\"\"\n\u001b[1;32m    594\u001b[0m         \u001b[0;31m# Check exceptional case: raise if no response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHasField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mGaxError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_operation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/google/gax/__init__.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;31m# Start polling, and return the final result from `_done_check`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mretryable_done_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/google/gax/retry.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# expected delay.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0mto_sleep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelay\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_sleep\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0m_MILLIS_PER_SECOND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0mdelay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdelay_mult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_delay_millis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Gather all samples, load into dictionary\n",
    "# Prepare a plain text corpus from which we train a languague model\n",
    "import glob\n",
    "import operator\n",
    "\n",
    "# Gather all text files from directory\n",
    "WORKING_DIRECTORY = os.path.join(os.getcwd(),'LibriSpeech/')\n",
    "\n",
    "# TRAINING_DIRECTORY = os.path.abspath(os.path.join(os.sep,'Volumes',\"My\\ Passport\\ for\\ Mac\",'lexicon','LibriSpeech'))\n",
    "dev_path = \"{}{}{}{}\".format(WORKING_DIRECTORY, 'dev-clean/', '**/', '*.txt')\n",
    "train_path = \"{}{}{}{}{}\".format(WORKING_DIRECTORY, 'books/', 'utf-8/', '**/', '*.txt*')\n",
    "\n",
    "text_paths = sorted(glob.glob(dev_path, recursive=True))\n",
    "print('Found',len(text_paths),'text files in the directory:', dev_path)\n",
    "\n",
    "transcripts = {}\n",
    "for document in text_paths:\n",
    "    with codecs.open(document, 'r', 'utf-8') as filep:\n",
    "        for i,line in enumerate(filep):\n",
    "            transcripts[line.split()[0]] = ' '.join(line.split()[1:])\n",
    "\n",
    "# Save Dictionary in Pickle File\n",
    "\n",
    "\n",
    "## Evaluate all samples found ##\n",
    "cloud_speech_api_accuracy = []\n",
    "custom_lang_model_accuracy = []\n",
    "epsilon = 0.000000001\n",
    "api_weight = 0.85\n",
    "steps = 0\n",
    "# Pull In Audio File\n",
    "for filename, gt_transcript in transcripts.items():\n",
    "    steps += 1\n",
    "    dirs = filename.split('-')\n",
    "    \n",
    "    audio_filepath = dev_file_name_0 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    dirs[0],\n",
    "    dirs[1],\n",
    "    \"{0}.flac\".format(filename))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Load the audio into memory\n",
    "    with io.open(audio_filepath, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "        audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code='en-US',\n",
    "        max_alternatives=10,\n",
    "        profanity_filter=False,\n",
    "        enable_word_time_offsets=True)\n",
    "\n",
    "    # Detects speech and words in the audio file\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "    result = operation.result(timeout=90)\n",
    "    alternatives = result.results[0].alternatives\n",
    "\n",
    "\n",
    "    # Evaluate API Results for Re-Ranking:\n",
    "    rerank_results = {}\n",
    "    for alternative in alternatives:\n",
    "        sent = alternative.transcript\n",
    "        \n",
    "        # Strip punctuation\n",
    "        translate_table = dict((ord(char), None) for char in string.punctuation)        \n",
    "        sent = sent.translate(translate_table) # remove punctuations\n",
    "\n",
    "        words = nltk.tokenize.word_tokenize(sent)\n",
    "        probs = np.ones_like(words, dtype=np.float32)*epsilon\n",
    "\n",
    "        for word in words:\n",
    "            if words.index(word) < len(words)-1: \n",
    "                freq = cfreq_2gram[word].freq(words[words.index(word)+1])\n",
    "                probs[words.index(word)] = freq\n",
    "\n",
    "        lexicon_score = np.sum(probs)\n",
    "\n",
    "        # Re-rank alternatives using a weighted average of the two scores\n",
    "        confidence_score = alternative.confidence*api_weight + lexicon_score*(1-api_weight)\n",
    "        rerank_results[alternative.transcript] = confidence_score\n",
    "\n",
    "\n",
    "    \n",
    "    index, value = max(enumerate(list(rerank_results.values())), key=operator.itemgetter(1))\n",
    "    # Select Corresponding Transcript:\n",
    "    script=''\n",
    "    for trnscript, confidence in rerank_results.items():\n",
    "        if confidence == value:\n",
    "            script = trnscript\n",
    "                \n",
    "    # Compute the Accuracy, based on the Levenshtein Distance (a.k.a. Edit Distance)\n",
    "    gcs_ed = nltk.edit_distance(alternative.transcript.lower(), gt_transcript.lower())\n",
    "    gcs_upper_bound = max(len(alternative.transcript),len(gt_transcript))\n",
    "    gcs_accuracy = (1.0 - gcs_ed/gcs_upper_bound)\n",
    "    \n",
    "    clm_ed = nltk.edit_distance(script.lower(), gt_transcript.lower())\n",
    "    clm_upper_bound = max(len(script),len(gt_transcript))\n",
    "    clm_accuracy = (1.0 - clm_ed/clm_upper_bound)\n",
    "    \n",
    "    cloud_speech_api_accuracy.append(gcs_accuracy)\n",
    "    custom_lang_model_accuracy.append(clm_accuracy)\n",
    "\n",
    "    if steps % 10 == 0:\n",
    "        print(\"{0} Transcripts Processed.\".format(steps))\n",
    "        print('Average API Accuracy:', np.mean(cloud_speech_api_accuracy))\n",
    "        print('Average Custom Model Accuracy:', np.mean(custom_lang_model_accuracy))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41 text files in the directory: /Users/deanmwebb/Google Drive/Development/consulting/lexicon/LibriSpeech/books/utf-8/**/*.txt*\n"
     ]
    }
   ],
   "source": [
    "# Prepare a plain text corpus from which we train a languague model\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Gather all text files from directory\n",
    "WORKING_DIRECTORY = os.path.join(os.getcwd(),'LibriSpeech/')\n",
    "\n",
    "# TRAINING_DIRECTORY = os.path.abspath(os.path.join(os.sep,'Volumes',\"My\\ Passport\\ for\\ Mac\",'lexicon','LibriSpeech'))\n",
    "dev_path = \"{}{}{}{}\".format(WORKING_DIRECTORY, 'dev-clean/', '**/', '*.txt')\n",
    "train_path = \"{}{}{}{}{}\".format(WORKING_DIRECTORY, 'books/', 'utf-8/', '**/', '*.txt*')\n",
    "\n",
    "text_paths = sorted(glob.glob(train_path, recursive=True))\n",
    "print('Found',len(text_paths),'text files in the directory:', train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "# reading the file in unicode format using codecs library    \n",
    "stoplist = set(stopwords.words('english'))\n",
    "# Strip punctuation\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation) \n",
    "        \n",
    "corpus_raw = u\"\"\n",
    "for book_filename in text_paths[:5]:\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        lines = book_file.read()\n",
    "        corpus_raw += lines.translate(translate_table) # remove punctuations \n",
    "\n",
    "               \n",
    "# Tokenize\n",
    "tokenized_words = nltk.tokenize.word_tokenize(corpus_raw)\n",
    "\n",
    "## Clean the tokens ##\n",
    "# Remove stop words\n",
    "tokenized_words = [word for word in tokenized_words if word not in stoplist]\n",
    "\n",
    "# Remove single-character tokens (mostly punctuation)\n",
    "tokenized_words = [word for word in tokenized_words if len(word) > 1]\n",
    "\n",
    "# Remove numbers\n",
    "tokenized_words = [word for word in tokenized_words if not word.isnumeric()]\n",
    "\n",
    "# Lowercase all words (default_stopwords are lowercase too)\n",
    "tokenized_words = [word.lower() for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 24662\n",
      "Number of speeches: 1\n",
      "Average number of sentences in each speech: 0.0\n",
      "Number of lines: 51308\n",
      "Average number of words in each line: 7.46370936306229\n",
      "\n",
      "The sentences 0 to 10:\n",
      "﻿The Project Gutenberg Etext of The Divine Comedy of Dante\r\n",
      "Translanted by Henry Wadsworth Longfellow\r\n",
      "\r\n",
      "\r\n",
      "Copyright laws are changing all over the world be sure to check\r\n",
      "the copyright laws for your country before posting these files\r\n",
      "\r\n",
      "Please take a look at the important information in this header\r\n",
      "We encourage you to keep this file on your own disk keeping an\r\n",
      "electronic path open for the next readers  Do not remove this\r\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in corpus_raw.split()})))\n",
    "speeches = corpus_raw.split('\\n\\n')\n",
    "print('Number of speeches: {}'.format(len(speeches)))\n",
    "sentence_count_speech = [speeches.count('\\n') for speech in speeches]\n",
    "print('Average number of sentences in each speech: {}'.format(np.average(sentence_count_speech)))\n",
    "\n",
    "sentences = [sentence for speech in speeches for sentence in speech.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(corpus_raw.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    vocabs = set(text)\n",
    "    int_to_vocab = dict(enumerate(vocabs, 1))\n",
    "    vocab_to_int = { v: k for k, v in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    return {\n",
    "        '.': '||period||',\n",
    "        ',': '||comma||',\n",
    "        '\"': '||quotation||',\n",
    "        ';': '||semi_colon||',\n",
    "        '!': '||exclamation||',\n",
    "        '?': '||question||',\n",
    "        '(': '||left_parentheses||',\n",
    "        ')': '||right_parentheses||',\n",
    "        '--': '||dash||',\n",
    "        '\\n': '||return||'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "\n",
    "# for book_filename in text_paths:\n",
    "#     with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "#         lines = book_file.read()\n",
    "#         corpus_raw += lines.translate(translate_table) # remove punctuations \n",
    "with open(\"saved_corp.txt\",'w') as corp_file:\n",
    "    corp_file.write(corpus_raw)\n",
    "\n",
    "corp_file = os.path.join(os.getcwd(),\"saved_corp.txt\")\n",
    "helper.preprocess_and_save_data(corp_file, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return (\n",
    "        tf.placeholder(tf.int32, shape=(None, None), name='input'),\n",
    "        tf.placeholder(tf.int32, shape=(None, None)),\n",
    "        tf.placeholder(tf.float32, name='keep_prob'),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([\n",
    "        tf.contrib.rnn.BasicLSTMCell(rnn_size),\n",
    "        tf.contrib.rnn.BasicLSTMCell(rnn_size)])\n",
    "\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    initial_state = tf.identity(initial_state, 'initial_state')\n",
    "\n",
    "    return cell, initial_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.embed_sequence(input_data, vocab_size, embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, 'final_state')\n",
    "\n",
    "    return outputs, final_state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    inputs = get_embed(input_data, vocab_size, rnn_size)\n",
    "    outputs, final_state = build_rnn(cell, inputs)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, None)\n",
    "\n",
    "    return logits, final_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: A Numpy array where each item is a tuple of (batch of input, batch of target).\n",
    "    \"\"\"\n",
    "    n_batches = int(len(int_text) / (batch_size * seq_length))\n",
    "\n",
    "    # Drop the last few characters to make only full batches\n",
    "    xdata = np.array(int_text[: n_batches * batch_size * seq_length])\n",
    "    ydata = np.array(int_text[1: n_batches * batch_size * seq_length+1])\n",
    "\n",
    "    x_batches = np.split(xdata.reshape(batch_size, -1), n_batches, 1)\n",
    "    y_batches = np.split(ydata.reshape(batch_size, -1), n_batches, 1)\n",
    "\n",
    "    return np.array(list(zip(x_batches, y_batches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 2\n",
    "# Batch Size\n",
    "batch_size = 64\n",
    "# RNN Size\n",
    "rnn_size = 256\n",
    "# Sequence Length\n",
    "seq_length = 10\n",
    "# Learning Rate\n",
    "learning_rate = 0.002\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 100\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/678   train_loss = 9.986\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[55,5] = 21721 is not in [0, 21721)\n\t [[Node: EmbedSequence/embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@EmbedSequence/embeddings\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](EmbedSequence/embeddings/read, _recv_input_0)]]\n\nCaused by op 'EmbedSequence/embedding_lookup', defined at:\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-125-33f70c17d6c6>\", line 12, in <module>\n    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size)\n  File \"<ipython-input-122-38fa234e2bbc>\", line 10, in build_nn\n    inputs = get_embed(input_data, vocab_size, rnn_size)\n  File \"<ipython-input-120-115834285065>\", line 9, in get_embed\n    return tf.contrib.layers.embed_sequence(input_data, vocab_size, embed_dim)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/encoders.py\", line 142, in embed_sequence\n    return embedding_ops.embedding_lookup(embeddings, ids)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py\", line 111, in embedding_lookup\n    validate_indices=validate_indices)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1359, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[55,5] = 21721 is not in [0, 21721)\n\t [[Node: EmbedSequence/embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@EmbedSequence/embeddings\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](EmbedSequence/embeddings/read, _recv_input_0)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[55,5] = 21721 is not in [0, 21721)\n\t [[Node: EmbedSequence/embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@EmbedSequence/embeddings\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](EmbedSequence/embeddings/read, _recv_input_0)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-396c263e7e73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0minitial_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 lr: learning_rate}\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# Show every <show_every_n_batches> batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[55,5] = 21721 is not in [0, 21721)\n\t [[Node: EmbedSequence/embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@EmbedSequence/embeddings\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](EmbedSequence/embeddings/read, _recv_input_0)]]\n\nCaused by op 'EmbedSequence/embedding_lookup', defined at:\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-125-33f70c17d6c6>\", line 12, in <module>\n    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size)\n  File \"<ipython-input-122-38fa234e2bbc>\", line 10, in build_nn\n    inputs = get_embed(input_data, vocab_size, rnn_size)\n  File \"<ipython-input-120-115834285065>\", line 9, in get_embed\n    return tf.contrib.layers.embed_sequence(input_data, vocab_size, embed_dim)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/encoders.py\", line 142, in embed_sequence\n    return embedding_ops.embedding_lookup(embeddings, ids)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/ops/embedding_ops.py\", line 111, in embedding_lookup\n    validate_indices=validate_indices)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1359, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[55,5] = 21721 is not in [0, 21721)\n\t [[Node: EmbedSequence/embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@EmbedSequence/embeddings\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](EmbedSequence/embeddings/read, _recv_input_0)]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return map(\n",
    "        loaded_graph.get_tensor_by_name,\n",
    "        ['input:0', 'initial_state:0', 'final_state:0', 'probs:0']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return int_to_vocab[probabilities.argmax()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_length = 20\n",
    "prime_word = 'Better'\n",
    "\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word]\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        script = script.replace(' ' + token.lower(), key)\n",
    "    script = script.replace('\\n ', '\\n')\n",
    "    script = script.replace('( ', '(')\n",
    "        \n",
    "    print(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gather all samples, load into dictionary\n",
    "# Prepare a plain text corpus from which we train a languague model\n",
    "import glob\n",
    "import operator\n",
    "\n",
    "# Gather all text files from directory\n",
    "WORKING_DIRECTORY = os.path.join(os.getcwd(),'LibriSpeech/')\n",
    "\n",
    "# TRAINING_DIRECTORY = os.path.abspath(os.path.join(os.sep,'Volumes',\"My\\ Passport\\ for\\ Mac\",'lexicon','LibriSpeech'))\n",
    "dev_path = \"{}{}{}{}\".format(WORKING_DIRECTORY, 'dev-clean/', '**/', '*.txt')\n",
    "train_path = \"{}{}{}{}{}\".format(WORKING_DIRECTORY, 'books/', 'utf-8/', '**/', '*.txt*')\n",
    "\n",
    "text_paths = sorted(glob.glob(dev_path, recursive=True))\n",
    "print('Found',len(text_paths),'text files in the directory:', dev_path)\n",
    "\n",
    "transcripts = {}\n",
    "for document in text_paths:\n",
    "    with codecs.open(document, 'r', 'utf-8') as filep:\n",
    "        for i,line in enumerate(filep):\n",
    "            transcripts[line.split()[0]] = ' '.join(line.split()[1:])\n",
    "\n",
    "# Save Dictionary in Pickle File\n",
    "\n",
    "\n",
    "## Evaluate all samples found ##\n",
    "cloud_speech_api_accuracy = []\n",
    "custom_lang_model_accuracy = []\n",
    "epsilon = 0.000000001\n",
    "api_weight = 0.85\n",
    "steps = 0\n",
    "# Pull In Audio File\n",
    "for filename, gt_transcript in transcripts.items():\n",
    "    steps += 1\n",
    "    dirs = filename.split('-')\n",
    "    \n",
    "    audio_filepath = dev_file_name_0 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    dirs[0],\n",
    "    dirs[1],\n",
    "    \"{0}.flac\".format(filename))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Load the audio into memory\n",
    "    with io.open(audio_filepath, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "        audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code='en-US',\n",
    "        max_alternatives=10,\n",
    "        profanity_filter=False,\n",
    "        enable_word_time_offsets=True)\n",
    "\n",
    "    # Detects speech and words in the audio file\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "    result = operation.result(timeout=90)\n",
    "    alternatives = result.results[0].alternatives\n",
    "\n",
    "\n",
    "    # Evaluate API Results for Re-Ranking:\n",
    "    rerank_results = {}\n",
    "    for alternative in alternatives:\n",
    "        sent = alternative.transcript\n",
    "        \n",
    "        # Strip punctuation\n",
    "        translate_table = dict((ord(char), None) for char in string.punctuation)        \n",
    "        sent = sent.translate(translate_table) # remove punctuations\n",
    "\n",
    "        words = nltk.tokenize.word_tokenize(sent)\n",
    "        \n",
    "        \n",
    "        # Run Words through Neural Network\n",
    "        gen_length = len(sent)\n",
    "        num_chunks = 3\n",
    "        sent_sections = list(chunks(sent, num_chunks))\n",
    "        potential_scripts = []\n",
    "        alternatives_gen_acc = []\n",
    "        prime_word = ''\n",
    "        \n",
    "        for section in sent_sections[:-1]:\n",
    "            prime_word += section\n",
    "        \n",
    "            loaded_graph = tf.Graph()\n",
    "            with tf.Session(graph=loaded_graph) as sess:\n",
    "                # Load saved model\n",
    "                loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "                loader.restore(sess, load_dir)\n",
    "\n",
    "                # Get Tensors from loaded model\n",
    "                input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "                # Sentences generation setup\n",
    "                gen_sentences = [prime_word]\n",
    "                prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "                # Generate sentences\n",
    "                for n in range(gen_length):\n",
    "                    # Dynamic Input\n",
    "                    dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "                    dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "                    # Get Prediction\n",
    "                    probabilities, prev_state = sess.run(\n",
    "                        [probs, final_state],\n",
    "                        {input_text: dyn_input, initial_state: prev_state})\n",
    "\n",
    "                    pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "                    gen_sentences.append(pred_word)\n",
    "\n",
    "                # Remove tokens\n",
    "                gen_script = ' '.join(gen_sentences)\n",
    "                for key, token in token_dict.items():\n",
    "                    ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "                    gen_script = gen_script.replace(' ' + token.lower(), key)\n",
    "                gen_script = gen_script.replace('\\n ', '\\n')\n",
    "                gen_script = gen_script.replace('( ', '(')\n",
    "\n",
    "                print()\n",
    "                print()\n",
    "                print(gen_script)\n",
    "                \n",
    "                # potential_scripts.append(gen_script)\n",
    "                \n",
    "                # Find Edit distance between word and potential script that was generated\n",
    "                alt_ed = nltk.edit_distance(sent.lower(), gen_script.lower())\n",
    "                alt_upper_bound = max(len(sent),len(gen_script))\n",
    "                alt_accuracy = (1.0 - alt_ed/alt_upper_bound)\n",
    "                alternatives_gen_acc.append(alt_accuracy)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Re-rank alternatives using a weighted average of the two scores\n",
    "        prediction_score = np.mean(alternatives_gen_acc)\n",
    "        rerank_results[alternative.transcript] = confidence_score\n",
    "\n",
    "\n",
    "    \n",
    "    index, value = max(enumerate(list(rerank_results.values())), key=operator.itemgetter(1))\n",
    "    # Select Corresponding Transcript:\n",
    "    script=''\n",
    "    for trnscript, confidence in rerank_results.items():\n",
    "        if confidence == value:\n",
    "            script = trnscript\n",
    "            \n",
    "    \n",
    "    \n",
    "                \n",
    "    # Compute the Accuracy, based on the Levenshtein Distance (a.k.a. Edit Distance)\n",
    "    gcs_ed = nltk.edit_distance(alternative.transcript.lower(), gt_transcript.lower())\n",
    "    gcs_upper_bound = max(len(alternative.transcript),len(gt_transcript))\n",
    "    gcs_accuracy = (1.0 - gcs_ed/gcs_upper_bound)\n",
    "    \n",
    "    clm_ed = nltk.edit_distance(script.lower(), gt_transcript.lower())\n",
    "    clm_upper_bound = max(len(script),len(gt_transcript))\n",
    "    clm_accuracy = (1.0 - clm_ed/clm_upper_bound)\n",
    "    \n",
    "    cloud_speech_api_accuracy.append(gcs_accuracy)\n",
    "    custom_lang_model_accuracy.append(clm_accuracy)\n",
    "\n",
    "    if steps % 10 == 0:\n",
    "        print(\"{0} Transcripts Processed.\".format(steps))\n",
    "        print('Average API Accuracy:', np.mean(cloud_speech_api_accuracy))\n",
    "        print('Average Custom Model Accuracy:', np.mean(custom_lang_model_accuracy))\n",
    "        print()\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:sdc_dev]",
   "language": "python",
   "name": "conda-env-sdc_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
