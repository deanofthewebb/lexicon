{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon - Custom Language Model\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "For this project, I will build a simple custom language model that is able to learn from any text data provided, and return a transcript with confidence values from input posed in speech utterances. I will use Google's cloud-based services to preprocess the input audio data and transcribe into an initial guess. Then I will train a model to improve on Google cloud speech API's response.\n",
    "\n",
    "\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "In order to use Google's cloud-based services, you first need to create an account on the [Google Cloud Platform](https://cloud.google.com//).\n",
    "\n",
    "Then, for each service you want to use, you have to enable use of that service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: google-cloud-speech in /Users/deanmwebb/anaconda/lib/python2.7/site-packages\n",
      "Requirement already up-to-date: google-gax<0.16dev,>=0.15.14 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-cloud-speech)\n",
      "Requirement already up-to-date: google-cloud-core<0.28dev,>=0.27.0 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-cloud-speech)\n",
      "Requirement already up-to-date: googleapis-common-protos[grpc]<2.0dev,>=1.5.2 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-cloud-speech)\n",
      "Requirement already up-to-date: ply==3.8 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: dill<0.3dev,>=0.2.5 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: future<0.17dev,>=0.16.0 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: grpcio<2.0dev,>=1.0.2 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: google-auth<2.0dev,>=1.0.0 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: protobuf<4.0dev,>=3.0.0 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: requests<3.0dev,>=2.13.0 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: six>=1.10.0 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-cloud-core<0.28dev,>=0.27.0->google-cloud-speech)\n",
      "Requirement already up-to-date: futures>=3.0.0; python_version < \"3.2\" in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-cloud-core<0.28dev,>=0.27.0->google-cloud-speech)\n",
      "Requirement already up-to-date: setuptools>=34.0.0 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-cloud-core<0.28dev,>=0.27.0->google-cloud-speech)\n",
      "Requirement already up-to-date: enum34>=1.0.4 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from grpcio<2.0dev,>=1.0.2->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: cachetools>=2.0.0 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-auth<2.0dev,>=1.0.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: rsa>=3.1.4 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-auth<2.0dev,>=1.0.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: pyasn1>=0.1.7 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-auth<2.0dev,>=1.0.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: pyasn1-modules>=0.0.5 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-auth<2.0dev,>=1.0.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: urllib3<1.23,>=1.21.1 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from requests<3.0dev,>=2.13.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: idna<2.7,>=2.5 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from requests<3.0dev,>=2.13.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: chardet<3.1.0,>=3.0.2 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from requests<3.0dev,>=2.13.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: certifi>=2017.4.17 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from requests<3.0dev,>=2.13.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-cloud-speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Google Cloud SDK: https://cloud.google.com/sdk/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Google Cloud SDK!\n",
      "\n",
      "To help improve the quality of this product, we collect anonymized usage data\n",
      "and anonymized stacktraces when crashes are encountered; additional information\n",
      "is available at <https://cloud.google.com/sdk/usage-statistics>. You may choose\n",
      "to opt out of this collection now (by choosing 'N' at the below prompt), or at\n",
      "any time in the future by running the following command:\n",
      "\n",
      "    gcloud config set disable_usage_reporting true\n",
      "\n",
      "\n",
      "Your current Cloud SDK version is: 170.0.1\n",
      "The latest available version is: 171.0.0\n",
      "\n",
      "┌────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│                                                   Components                                                   │\n",
      "├──────────────────┬──────────────────────────────────────────────────────┬──────────────────────────┬───────────┤\n",
      "│      Status      │                         Name                         │            ID            │    Size   │\n",
      "├──────────────────┼──────────────────────────────────────────────────────┼──────────────────────────┼───────────┤\n",
      "│ Update Available │ Cloud SDK Core Libraries                             │ core                     │   6.7 MiB │\n",
      "│ Not Installed    │ App Engine Go Extensions                             │ app-engine-go            │  97.7 MiB │\n",
      "│ Not Installed    │ Cloud Bigtable Command Line Tool                     │ cbt                      │   4.0 MiB │\n",
      "│ Not Installed    │ Cloud Bigtable Emulator                              │ bigtable                 │   3.5 MiB │\n",
      "│ Not Installed    │ Cloud Datalab Command Line Tool                      │ datalab                  │   < 1 MiB │\n",
      "│ Not Installed    │ Cloud Datastore Emulator                             │ cloud-datastore-emulator │  15.4 MiB │\n",
      "│ Not Installed    │ Cloud Datastore Emulator (Legacy)                    │ gcd-emulator             │  38.1 MiB │\n",
      "│ Not Installed    │ Cloud Pub/Sub Emulator                               │ pubsub-emulator          │  33.2 MiB │\n",
      "│ Not Installed    │ Emulator Reverse Proxy                               │ emulator-reverse-proxy   │  14.5 MiB │\n",
      "│ Not Installed    │ Google Container Local Builder                       │ container-builder-local  │   3.7 MiB │\n",
      "│ Not Installed    │ Google Container Registry's Docker credential helper │ docker-credential-gcr    │   2.2 MiB │\n",
      "│ Not Installed    │ gcloud Alpha Commands                                │ alpha                    │   < 1 MiB │\n",
      "│ Not Installed    │ gcloud Beta Commands                                 │ beta                     │   < 1 MiB │\n",
      "│ Not Installed    │ gcloud app Java Extensions                           │ app-engine-java          │ 130.8 MiB │\n",
      "│ Not Installed    │ gcloud app PHP Extensions                            │ app-engine-php           │  21.9 MiB │\n",
      "│ Not Installed    │ gcloud app Python Extensions                         │ app-engine-python        │   6.3 MiB │\n",
      "│ Not Installed    │ kubectl                                              │ kubectl                  │  15.9 MiB │\n",
      "│ Installed        │ BigQuery Command Line Tool                           │ bq                       │   < 1 MiB │\n",
      "│ Installed        │ Cloud Storage Command Line Tool                      │ gsutil                   │   3.0 MiB │\n",
      "└──────────────────┴──────────────────────────────────────────────────────┴──────────────────────────┴───────────┘\n",
      "To install or remove components at your current SDK version [170.0.1], run:\n",
      "  $ gcloud components install COMPONENT_ID\n",
      "  $ gcloud components remove COMPONENT_ID\n",
      "\n",
      "To update your SDK installation to the latest version [171.0.0], run:\n",
      "  $ gcloud components update\n",
      "\n",
      "==> Source [/Users/deanmwebb/Google Drive/Development/consulting/lexicon/google-cloud-sdk/completion.bash.inc] in your profile to enable shell command completion for gcloud.\n",
      "==> Source [/Users/deanmwebb/Google Drive/Development/consulting/lexicon/google-cloud-sdk/path.bash.inc] in your profile to add the Google Cloud SDK command line tools to your $PATH.\n",
      "\n",
      "For more information on how to get started, please visit:\n",
      "  https://cloud.google.com/sdk/docs/quickstarts\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!CLOUDSDK_CORE_DISABLE_PROMPTS=1 ./google-cloud-sdk/install.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate with Google Cloud API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activated service account credentials for: [lexicon-bot@exemplary-oath-179301.iam.gserviceaccount.com]\r\n"
     ]
    }
   ],
   "source": [
    "!source google-cloud-sdk/completion.bash.inc && \\\n",
    "source google-cloud-sdk/path.bash.inc && \\\n",
    "gcloud auth activate-service-account lexicon-bot@exemplary-oath-179301.iam.gserviceaccount.com --key-file=Lexicon-e94eff39fad7.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']='/Users/deanmwebb/Google Drive/Development/consulting/lexicon/Lexicon-e94eff39fad7.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test out Cloud Spech API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for operation to complete...\n",
      "Transcript: at this moment to the whole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\n",
      "Confidence Score: 0.9484339356422424\n",
      "Word: at, start_time (s): 0.1, end_time (s): 0.5, total_time (s): 0.4\n",
      "Word: this, start_time (s): 0.5, end_time (s): 0.6000000000000001, total_time (s): 0.10000000000000009\n",
      "Word: moment, start_time (s): 0.6000000000000001, end_time (s): 0.8, total_time (s): 0.19999999999999996\n",
      "Word: to, start_time (s): 0.8, end_time (s): 1.1, total_time (s): 0.30000000000000004\n",
      "Word: the, start_time (s): 1.1, end_time (s): 1.3, total_time (s): 0.19999999999999996\n",
      "Word: whole, start_time (s): 1.3, end_time (s): 1.5, total_time (s): 0.19999999999999996\n",
      "Word: soul, start_time (s): 1.5, end_time (s): 2.0, total_time (s): 0.5\n",
      "Word: of, start_time (s): 2.0, end_time (s): 2.2, total_time (s): 0.20000000000000018\n",
      "Word: the, start_time (s): 2.2, end_time (s): 2.3, total_time (s): 0.09999999999999964\n",
      "Word: Old, start_time (s): 2.3, end_time (s): 2.5, total_time (s): 0.20000000000000018\n",
      "Word: Man, start_time (s): 2.5, end_time (s): 2.7, total_time (s): 0.20000000000000018\n",
      "Word: scene, start_time (s): 2.7, end_time (s): 3.0, total_time (s): 0.2999999999999998\n",
      "Word: centered, start_time (s): 3.0, end_time (s): 3.4, total_time (s): 0.3999999999999999\n",
      "Word: in, start_time (s): 3.4, end_time (s): 3.5, total_time (s): 0.10000000000000009\n",
      "Word: his, start_time (s): 3.5, end_time (s): 3.7, total_time (s): 0.20000000000000018\n",
      "Word: eyes, start_time (s): 3.7, end_time (s): 4.1, total_time (s): 0.39999999999999947\n",
      "Word: which, start_time (s): 4.1, end_time (s): 4.2, total_time (s): 0.10000000000000053\n",
      "Word: became, start_time (s): 4.2, end_time (s): 4.5, total_time (s): 0.2999999999999998\n",
      "Word: bloodshot, start_time (s): 4.5, end_time (s): 5.3, total_time (s): 0.7999999999999998\n",
      "Word: the, start_time (s): 5.3, end_time (s): 5.6, total_time (s): 0.2999999999999998\n",
      "Word: veins, start_time (s): 5.6, end_time (s): 5.9, total_time (s): 0.3000000000000007\n",
      "Word: of, start_time (s): 5.9, end_time (s): 6.1, total_time (s): 0.1999999999999993\n",
      "Word: the, start_time (s): 6.1, end_time (s): 6.1, total_time (s): 0.0\n",
      "Word: throat, start_time (s): 6.1, end_time (s): 6.4, total_time (s): 0.3000000000000007\n",
      "Word: swelled, start_time (s): 6.4, end_time (s): 6.9, total_time (s): 0.5\n",
      "Word: his, start_time (s): 6.9, end_time (s): 7.2, total_time (s): 0.2999999999999998\n",
      "Word: cheeks, start_time (s): 7.2, end_time (s): 7.6, total_time (s): 0.39999999999999947\n",
      "Word: and, start_time (s): 7.6, end_time (s): 7.6, total_time (s): 0.0\n",
      "Word: temples, start_time (s): 7.6, end_time (s): 8.1, total_time (s): 0.5\n",
      "Word: became, start_time (s): 8.1, end_time (s): 8.4, total_time (s): 0.3000000000000007\n",
      "Word: purple, start_time (s): 8.4, end_time (s): 8.8, total_time (s): 0.40000000000000036\n",
      "Word: as, start_time (s): 8.8, end_time (s): 9.0, total_time (s): 0.1999999999999993\n",
      "Word: though, start_time (s): 9.0, end_time (s): 9.1, total_time (s): 0.09999999999999964\n",
      "Word: he, start_time (s): 9.1, end_time (s): 9.2, total_time (s): 0.09999999999999964\n",
      "Word: was, start_time (s): 9.2, end_time (s): 9.3, total_time (s): 0.10000000000000142\n",
      "Word: struck, start_time (s): 9.3, end_time (s): 9.7, total_time (s): 0.3999999999999986\n",
      "Word: with, start_time (s): 9.7, end_time (s): 9.8, total_time (s): 0.10000000000000142\n",
      "Word: epilepsy, start_time (s): 9.8, end_time (s): 10.2, total_time (s): 0.3999999999999986\n",
      "Word: nothing, start_time (s): 10.2, end_time (s): 10.9, total_time (s): 0.7000000000000011\n",
      "Word: was, start_time (s): 10.9, end_time (s): 11.1, total_time (s): 0.1999999999999993\n",
      "Word: wanting, start_time (s): 11.1, end_time (s): 11.4, total_time (s): 0.3000000000000007\n",
      "Word: to, start_time (s): 11.4, end_time (s): 11.5, total_time (s): 0.09999999999999964\n",
      "Word: complete, start_time (s): 11.5, end_time (s): 11.9, total_time (s): 0.40000000000000036\n",
      "Word: this, start_time (s): 11.9, end_time (s): 12.0, total_time (s): 0.09999999999999964\n",
      "Word: but, start_time (s): 12.0, end_time (s): 12.3, total_time (s): 0.3000000000000007\n",
      "Word: the, start_time (s): 12.3, end_time (s): 12.4, total_time (s): 0.09999999999999964\n",
      "Word: utterance, start_time (s): 12.4, end_time (s): 12.8, total_time (s): 0.40000000000000036\n",
      "Word: of, start_time (s): 12.8, end_time (s): 12.8, total_time (s): 0.0\n",
      "Word: a, start_time (s): 12.8, end_time (s): 13.0, total_time (s): 0.1999999999999993\n",
      "Word: cry, start_time (s): 13.0, end_time (s): 13.2, total_time (s): 0.1999999999999993\n",
      "Transcript: at this moment to the whole soul of the old man seemed centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\n",
      "Confidence Score: 0.9514756202697754\n",
      "Transcript: at this moment to the whole soul of the old man seem centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\n",
      "Confidence Score: 0.9415985941886902\n",
      "Transcript: at this moment to the whole sole of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\n",
      "Confidence Score: 0.9484888315200806\n",
      "Transcript: at this moment to the whole sole of the old man seemed centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\n",
      "Confidence Score: 0.9531012773513794\n",
      "Transcript: at this moment to the whole sole of the old man seem centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\n",
      "Confidence Score: 0.9432242512702942\n",
      "Transcript: at this moment to the whole soul of the old man seems centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\n",
      "Confidence Score: 0.9610214233398438\n",
      "Transcript: at this moment to the whole song of The Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\n",
      "Confidence Score: 0.9349250793457031\n",
      "Transcript: at this moment to the whole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this put the utterance of a cry\n",
      "Confidence Score: 0.9473556280136108\n",
      "Transcript: at this moment to the hole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\n",
      "Confidence Score: 0.9477002620697021\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import speech\n",
    "from google.cloud.speech import enums\n",
    "from google.cloud.speech import types\n",
    "\n",
    "# Instantiates a client\n",
    "client = speech.SpeechClient()\n",
    "\n",
    "# The name of the dev-test audio file to transcribe\n",
    "dev_file_name_0 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0000.flac')\n",
    "gt0 = 'GO DO YOU HEAR'\n",
    "\n",
    "dev_file_name_1 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0001.flac')\n",
    "gt1 = 'BUT IN LESS THAN FIVE MINUTES THE STAIRCASE GROANED BENEATH AN EXTRAORDINARY WEIGHT'\n",
    "\n",
    "# The name of the test audio file to transcribe\n",
    "dev_file_name_2 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0002.flac')\n",
    "gt2 = 'AT THIS MOMENT THE WHOLE SOUL OF THE OLD MAN SEEMED CENTRED IN HIS EYES WHICH BECAME BLOODSHOT THE VEINS OF THE THROAT SWELLED HIS CHEEKS AND TEMPLES BECAME PURPLE AS THOUGH HE WAS STRUCK WITH EPILEPSY NOTHING WAS WANTING TO COMPLETE THIS BUT THE UTTERANCE OF A CRY'\n",
    "\n",
    "dev_file_name_3 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0003.flac')\n",
    "gt3 = 'AND THE CRY ISSUED FROM HIS PORES IF WE MAY THUS SPEAK A CRY FRIGHTFUL IN ITS SILENCE'\n",
    "\n",
    "dev_file_name_4 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    '84',\n",
    "    '121123',\n",
    "    '84-121123-0004.flac')\n",
    "gt4 = \"D'AVRIGNY RUSHED TOWARDS THE OLD MAN AND MADE HIM INHALE A POWERFUL RESTORATIVE\"\n",
    "\n",
    "\n",
    "test_file_name_1 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'RNN-Tutorial-master',\n",
    "    'data',\n",
    "    'raw',\n",
    "    'librivox',\n",
    "    'LibriSpeech',\n",
    "    'test-clean-wav',\n",
    "    '4507-16021-0019.wav')\n",
    "\n",
    "\n",
    "audio_files = {dev_file_name_0:gt0, dev_file_name_1:gt1, dev_file_name_2:gt2, dev_file_name_3:gt3, dev_file_name_4:gt4}\n",
    "\n",
    "\n",
    "# Loads the audio into memory\n",
    "with io.open(dev_file_name_2, 'rb') as audio_file:\n",
    "    content = audio_file.read()\n",
    "    audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "config = types.RecognitionConfig(\n",
    "    encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
    "    sample_rate_hertz=16000,\n",
    "    language_code='en-US',\n",
    "    max_alternatives=10,\n",
    "    profanity_filter=False,\n",
    "    enable_word_time_offsets=True)\n",
    "\n",
    "# Detects speech and words in the audio file\n",
    "operation = client.long_running_recognize(config, audio)\n",
    "\n",
    "print('Waiting for operation to complete...')\n",
    "result = operation.result(timeout=90)\n",
    "\n",
    "alternatives = result.results[0].alternatives\n",
    "for alternative in alternatives:\n",
    "    print('Transcript: {}'.format(alternative.transcript))\n",
    "    print('Confidence Score: {}'.format(alternative.confidence))\n",
    "\n",
    "    for word_info in alternative.words:\n",
    "        word = word_info.word\n",
    "        start_time = word_info.start_time\n",
    "        end_time = word_info.end_time\n",
    "        start = start_time.seconds + start_time.nanos * 1e-9\n",
    "        end = end_time.seconds + end_time.nanos * 1e-9\n",
    "        delta = end - start\n",
    "        \n",
    "        print('Word: {}, start_time (s): {}, end_time (s): {}, total_time (s): {}'.format(\n",
    "            word,\n",
    "            start,\n",
    "            end,\n",
    "            delta))\n",
    "        \n",
    "        #TODO: Do we need to figure out how to assign words to alternatives?\n",
    "            # If same amounts, assign words to index of parsed word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Download the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "\n",
    "librispeech_dataset_folder_path = 'LibriSpeech'\n",
    "tar_gz_path = 'dev-clean.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Librispeech dev-clean.tar.gz') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://www.openslr.org/resources/12/dev-clean.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(librispeech_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Preprocess Dataset - Download Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk #NLP Toolkit\n",
    "nltk.download()\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gensim Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages\n",
      "Requirement already up-to-date: six>=1.5.0 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already up-to-date: smart-open>=1.2.1 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already up-to-date: numpy>=1.11.3 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already up-to-date: scipy>=0.18.1 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already up-to-date: requests in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already up-to-date: bz2file in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already up-to-date: boto>=2.32 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already up-to-date: certifi>=2017.4.17 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already up-to-date: chardet<3.1.0,>=3.0.2 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already up-to-date: urllib3<1.23,>=1.21.1 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already up-to-date: idna<2.7,>=2.5 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41 text files in the directory: /Users/deanmwebb/Google Drive/Development/consulting/lexicon/LibriSpeech/books/utf-8/**/*.txt*\n"
     ]
    }
   ],
   "source": [
    "# Prepare a plain text corpus from which we train a languague model\n",
    "import glob\n",
    "\n",
    "# Gather all text files from directory\n",
    "WORKING_DIRECTORY = os.path.join(os.getcwd(),'LibriSpeech/')\n",
    "\n",
    "# TRAINING_DIRECTORY = os.path.abspath(os.path.join(os.sep,'Volumes',\"My\\ Passport\\ for\\ Mac\",'lexicon','LibriSpeech'))\n",
    "dev_path = \"{}{}{}{}\".format(WORKING_DIRECTORY, 'dev-clean/', '**/', '*.txt')\n",
    "train_path = \"{}{}{}{}{}\".format(WORKING_DIRECTORY, 'books/', 'utf-8/', '**/', '*.txt*')\n",
    "\n",
    "text_paths = sorted(glob.glob(train_path, recursive=True))\n",
    "print('Found',len(text_paths),'text files in the directory:', train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset - Tokenize Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "# reading the file in unicode format using codecs library    \n",
    "stoplist = set(stopwords.words('english'))\n",
    "# Strip punctuation\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation) \n",
    "        \n",
    "corpus_raw = u\"\"\n",
    "for book_filename in text_paths:\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        lines = book_file.read()\n",
    "        corpus_raw += lines.translate(translate_table) # remove punctuations \n",
    "\n",
    "               \n",
    "# Tokenize\n",
    "tokenized_words = nltk.tokenize.word_tokenize(corpus_raw)\n",
    "\n",
    "## Clean the tokens ##\n",
    "# Remove stop words\n",
    "tokenized_words = [word for word in tokenized_words if word not in stoplist]\n",
    "\n",
    "# Remove single-character tokens (mostly punctuation)\n",
    "tokenized_words = [word for word in tokenized_words if len(word) > 1]\n",
    "\n",
    "# Remove numbers\n",
    "tokenized_words = [word for word in tokenized_words if not word.isnumeric()]\n",
    "\n",
    "# Lowercase all words (default_stopwords are lowercase too)\n",
    "tokenized_words = [word.lower() for word in tokenized_words]\n",
    "\n",
    "\n",
    "## DON'T USE THIS ##\n",
    "# Stemming words seems to make matters worse, disabled\n",
    "# stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "# tokenized_words = [stemmer.stem(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset - Extract N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.collocations import *\n",
    "from nltk.probability import FreqDist\n",
    "import nltk\n",
    "\n",
    "# extracting the bi-grams and sorting them according to their frequencies\n",
    "finder = BigramCollocationFinder.from_words(tokenized_words)\n",
    "# finder.apply_freq_filter(3)\n",
    "\n",
    "bigram_model = nltk.bigrams(tokenized_words)\n",
    "bigram_model = sorted(bigram_model, key=lambda item: item[1], reverse=True)  \n",
    "# print(bigram_model)\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "np.save(\"lang_model.npy\",bigram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word|Freq:\n",
      "('project', 'gutenbergtm')|1095\n",
      "('project', 'gutenberg')|1014\n",
      "('greater', 'part')|532\n",
      "('captain', 'nemo')|452\n",
      "('united', 'states')|407\n",
      "('great', 'britain')|385\n",
      "('uncle', 'john')|364\n",
      "('gold', 'silver')|337\n",
      "('let', 'us')|324\n",
      "('new', 'york')|309\n",
      "('gutenbergtm', 'electronic')|306\n",
      "('old', 'man')|303\n",
      "('public', 'domain')|293\n",
      "('every', 'one')|290\n",
      "('one', 'day')|281\n",
      "('young', 'man')|281\n",
      "('gutenberg', 'literary')|279\n",
      "('literary', 'archive')|279\n",
      "('archive', 'foundation')|279\n",
      "('dont', 'know')|275\n",
      "('of', 'course')|274\n",
      "('one', 'another')|273\n",
      "('electronic', 'works')|272\n",
      "('per', 'cent')|263\n",
      "('could', 'see')|260\n",
      "('ned', 'land')|254\n",
      "('good', 'deal')|247\n",
      "('mrs', 'sparsit')|244\n",
      "('two', 'three')|240\n",
      "('mr', 'bounderby')|236\n",
      "('set', 'forth')|225\n",
      "('old', 'woman')|218\n",
      "('years', 'ago')|217\n",
      "('the', 'first')|206\n",
      "('you', 'may')|205\n",
      "('it', 'would')|200\n",
      "('next', 'day')|200\n",
      "('long', 'time')|199\n",
      "('said', 'mrs')|198\n",
      "('of', 'the')|198\n",
      "('said', 'mr')|196\n",
      "('first', 'time')|195\n",
      "('every', 'day')|192\n",
      "('one', 'thing')|189\n",
      "('small', 'print')|189\n",
      "('electronic', 'work')|187\n",
      "('men', 'women')|186\n",
      "('every', 'man')|181\n",
      "('it', 'may')|171\n",
      "('and', 'said')|169\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(bigram_model)\n",
    "\n",
    "# Output top 50 words\n",
    "print(\"Word|Freq:\")\n",
    "for word, frequency in fdist.most_common(50):\n",
    "    print(u'{}|{}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Listing the words that can follow after 'said':\n",
      " dict_keys(['taxes', 'revenue', 'latter', 'deviation', 'injustice', 'told', 'shall', 'ease', 'depths', 'mass', 'difficulty', 'insult', 'favour', 'range', 'danger', 'frequency', 'things', 'than', 'competition', 'already', 'cultivation', 'convenience', 'many', 'would', 'perpendicular', 'writings', 'crown', 'usual', 'fire', 'heartiness', 'valuable', 'variation', 'variety', 'teacher', 'could', 'loss’', 'body', 'vitality', 'honor', 'circulation', 'salaries', 'ruritania', 'taint', 'pressure', 'distinctness', 'lights', 'parsimony', 'portion', 'amount', 'haytime', 'solidarity', 'satisfaction', 'mystery', 'wisdom', 'frequently', 'dangers', 'calamity', 'approximation', 'business', 'rights', 'diligence', 'impersonal', 'parts', 'advantages', 'violence', 'english', 'remoteness', 'part', 'but', 'sun', 'abundance', 'understanding', 'account', 'fear', 'universal', 'higher', 'singleness', 'opportunities', 'the', 'africa', 'profusion', 'never', 'warmth', 'difference', 'abroad', 'dole', 'capacities', 'number', 'expression', 'glory', 'grief', 'proportion', 'thirst', 'rent', 'slumbers', 'like', 'foregoing', 'original', 'goods', 'sometimes', 'renown', 'economy', 'london', 'goodness', 'horn', 'men', 'encountering', 'eloquence', 'sadness', 'saving', 'yet', 'no', 'superabundance', 'energy', 'simplicity', 'desire', 'favours', 'fault', 'grace', 'togetherness', 'pride', 'alterations', 'its', 'care', 'benefit', 'sanctity', 'rum', 'indignation', 'land', 'mind', 'death', 'gladness', 'smaller', 'dignity', 'imprudence', 'mountains', 'producing', 'herein', 'wrong', 'worlds', 'strides', 'acorn', 'events', 'profit', 'environment', 'numbers', 'great', 'authority', 'scarcity', 'expense', 'divinity', 'velocity', 'exportation', 'activity', 'place', 'art', 'without', 'distance', 'ned', 'supply', 'continuing', 'honourable', 'rank', 'intelligence', 'dexterity', 'almost', 'annual', 'dawn', 'surplus', 'practicality', 'glorious', 'must', 'poets', 'actually', 'sensation', 'went', 'quantity', 'force', 'action', 'might', 'rapture', 'tartness', 'thing', 'done', 'height', 'degrees', 'perhaps', 'evil', 'thoughts', 'lasting', 'pieces', 'circumstances', 'ordinary', 'service', 'individual', 'heat', 'influence', 'peril', 'greater', 'clerk', 'prospective', 'vessels', 'cheap', 'windbag', 'money', 'peace', 'left', 'corn', 'change', 'gifts', 'trade', 'indeed', 'confusion', 'less', 'confidence', 'jefferies', 'reduction', 'extensive', 'sums', 'none', 'gehenna', 'rice', 'latitude', 'france', 'view', 'splendour', 'nails', 'semblance', 'knave', 'disorders', 'wealth', 'slaves', 'augmentation', 'first', 'haste', 'sorrow', 'if', 'strain', 'facility', 'sufficient', 'perithous', 'employs', 'difficulties', 'effort', 'loss', 'claim', 'real', 'fixed', 'leader', 'whatever', 'rich', 'general', 'one', 'favours\\x94', 'lesser', 'whole', 'believed', 'return', 'quantities', 'among', 'weal', 'he', 'costage', 'consequence', 'dilatation', 'contemporary', 'america', 'comfort', 'advantage', 'personal', 'hope', 'at', 'wonder', 'content', 'antipathy', 'importation', 'riches', 'anyone', 'stock', 'and', 'to', 'success', 'length', 'opening', 'offence', 'rise', 'beginning', 'admiration', 'fast', 'enthusiasm', 'talents', 'little', 'inferiority', 'made', 'vanquished', 'modern', 'suited', 'balance', 'labourers', 'subconscious', 'past', 'incorporation', 'gain', 'either', 'restoration', 'countries', 'produce', 'waxen', 'common', 'fortitude', 'speed', 'kindness', 'professed', 'power', 'steps', 'chance', 'use', 'depth', 'interest', 'value', 'rapidity', 'want', 'enduring', 'guilds', 'age', 'honour', 'shame', 'flourish', 'pain', 'field', 'annoyance', 'mastery', 'love', 'cheapness', 'demand', 'share', 'therefore', 'woe', 'formerly', 'stocks', 'mans', 'zeal', 'second', 'fame', 'titan', 'security', 'extent', 'grew', 'upon', 'importance', 'fortune', 'clearness', 'worldly', 'fund', 'returns', 'pasture', 'liberty', 'price', 'sin', 'as', 'freedom', 'beauty', 'in', 'expected', 'brewery', 'crop', 'injudicious', 'necessary', 'contained', 'effect', 'seems', 'attractions', 'name', 'ii', 'such', 'silence', 'point', 'tenant', 'sum', 'lawe', 'group', 'sovereign', 'obstacles', 'otherwise', 'malversation', 'strength', 'every', 'evils', 'degree', 'though', 'fiercer', 'transgression', 'agony', 'reprobate', 'wiser', 'gold', 'equal', 'seignorage', 'tax', 'former', 'brightness', 'end', 'trespass', 'capital', 'prince', 'frequent', 'come', 'distress', 'weight', 'ever', 'found', 'melodies', 'pleasure', 'life', 'gift', 'boldness', 'far', 'harm', 'ones', 'relevance', 'inconveniency', 'require', 'it', 'well', 'fury', 'present', 'levying', 'moment', 'pomp', 'still', 'need', 'cost', 'delectation', 'triumph', 'time', 'desolation', 'crime'])\n",
      "\n",
      "Listing 20 most frequent words to come after 'said':\n",
      " [('part', 532), ('quantity', 105), ('number', 50), ('proportion', 43), ('value', 24), ('smaller', 16), ('greater', 16), ('share', 16), ('less', 12), ('profit', 11), ('revenue', 9), ('the', 9), ('importance', 9), ('capital', 9), ('surplus', 8), ('variety', 7), ('distance', 7), ('degree', 7), ('ease', 6), ('stock', 6)]\n"
     ]
    }
   ],
   "source": [
    "cfreq_2gram = nltk.ConditionalFreqDist(bigram_model)\n",
    "# print('Conditional Frequency Conditions:\\n', cfreq_2gram)\n",
    "print()\n",
    "\n",
    "# First access the FreqDist associated with \"one\", then the keys in that FreqDist\n",
    "print(\"Listing the words that can follow after 'greater':\\n\", cfreq_2gram[\"greater\"].keys())\n",
    "print()\n",
    "\n",
    "# Determine Most common in conditional frequency\n",
    "print(\"Listing 20 most frequent words to come after 'greater':\\n\", cfreq_2gram[\"greater\"].most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEMO - Evaluate Sentences Using Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for operation to complete...\n",
      "API Results:  [transcript: \"at this moment to the whole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\"\n",
      "confidence: 0.948432981967926\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"at\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"this\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"moment\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  word: \"to\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  word: \"the\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"whole\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "  }\n",
      "  word: \"soul\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  word: \"of\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  word: \"the\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"Old\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  word: \"Man\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "  }\n",
      "  word: \"scene\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  word: \"centered\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"in\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  word: \"his\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 4\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  word: \"eyes\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 4\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 4\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  word: \"which\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 4\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 4\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"became\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 4\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 5\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  word: \"bloodshot\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 5\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 5\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"the\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 5\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 5\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"veins\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 5\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 6\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  word: \"of\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 6\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 6\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  word: \"the\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 6\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 6\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  word: \"throat\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 6\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 6\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"swelled\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 6\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 7\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  word: \"his\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 7\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 7\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"cheeks\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 7\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 7\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"and\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 7\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 8\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  word: \"temples\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 8\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 8\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  word: \"became\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 8\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 8\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"purple\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 8\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 9\n",
      "  }\n",
      "  word: \"as\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 9\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 9\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  word: \"though\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 9\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 9\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  word: \"he\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 9\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 9\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  word: \"was\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 9\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 9\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  word: \"struck\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 9\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 9\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"with\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 9\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 10\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  word: \"epilepsy\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 10\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 10\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"nothing\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 10\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 11\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  word: \"was\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 11\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 11\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  word: \"wanting\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 11\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 11\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"to\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 11\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 11\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"complete\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 11\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 12\n",
      "  }\n",
      "  word: \"this\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 12\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 12\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  word: \"but\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 12\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 12\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  word: \"the\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 12\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 12\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"utterance\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 12\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 12\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"of\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 12\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 13\n",
      "  }\n",
      "  word: \"a\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 13\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 13\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  word: \"cry\"\n",
      "}\n",
      ", transcript: \"at this moment to the whole soul of the old man seemed centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\"\n",
      "confidence: 0.951474666595459\n",
      ", transcript: \"at this moment to the whole soul of the old man seem centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\"\n",
      "confidence: 0.9415977001190186\n",
      ", transcript: \"at this moment to the whole sole of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\"\n",
      "confidence: 0.9484882950782776\n",
      ", transcript: \"at this moment to the whole sole of the old man seemed centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\"\n",
      "confidence: 0.9531007409095764\n",
      ", transcript: \"at this moment to the whole sole of the old man seem centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\"\n",
      "confidence: 0.9432237148284912\n",
      ", transcript: \"at this moment to the whole soul of the old man seems centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\"\n",
      "confidence: 0.9610205888748169\n",
      ", transcript: \"at this moment to the whole song of The Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\"\n",
      "confidence: 0.9349242448806763\n",
      ", transcript: \"at this moment to the whole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this put the utterance of a cry\"\n",
      "confidence: 0.9473547339439392\n",
      ", transcript: \"at this moment to the hole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry\"\n",
      "confidence: 0.9476993680000305\n",
      "]\n",
      "\n",
      "\n",
      "RE-RANKED Results: \n",
      " {'at this moment to the hole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.9081002712249755, 'at this moment to the whole sole of the old man seemed centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.91804185062646859, 'at this moment to the whole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this put the utterance of a cry': 0.90842670500278477, 'at this moment to the whole soul of the old man seemed centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.91686590313911431, 'at this moment to the whole sole of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.90913219824433322, 'at this moment to the whole sole of the old man seem centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.90858307033777241, 'at this moment to the whole song of The Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.89226038940250874, 'at this moment to the whole soul of the old man seem centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.90740718096494677, 'at this moment to the whole soul of the old man seems centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.92589252740144734, 'at this moment to the whole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry': 0.90944847464561462}\n",
      "\n",
      "\n",
      "ORIGINAL Transcript: \n",
      "'at this moment to the hole soul of the Old Man scene centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry' \n",
      "with a confidence_score of: 0.9476993680000305\n",
      "RE-RANKED Transcript: \n",
      "'at this moment to the whole soul of the old man seems centered in his eyes which became bloodshot the veins of the throat swelled his cheeks and temples became purple as though he was struck with epilepsy nothing was wanting to complete this but the utterance of a cry' \n",
      "with a confidence_score of: 0.9258925274014473\n",
      "GROUND TRUTH TRANSCRIPT: \n",
      "AT THIS MOMENT THE WHOLE SOUL OF THE OLD MAN SEEMED CENTRED IN HIS EYES WHICH BECAME BLOODSHOT THE VEINS OF THE THROAT SWELLED HIS CHEEKS AND TEMPLES BECAME PURPLE AS THOUGH HE WAS STRUCK WITH EPILEPSY NOTHING WAS WANTING TO COMPLETE THIS BUT THE UTTERANCE OF A CRY\n",
      "\n",
      "The original transcript was RE-RANKED. The transcripts do not match!\n",
      "Differences between original and re-ranked:  ['scene', 'hole']\n",
      "\n",
      "\n",
      "The original transcript DOES NOT MATCH ground truth.\n",
      "Differences between original and ground truth:  ['scene', 'centered', 'hole']\n",
      "\n",
      "\n",
      "The RE_RANKED transcript DOES NOT MATCH ground truth.\n",
      "Differences between Reranked and ground truth:  ['centered', 'seems']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL Edit Distance: \n",
      "8\n",
      "RE-RANKED Edit Distance: \n",
      "6\n",
      "\n",
      "\n",
      "Waiting for operation to complete...\n",
      "API Results:  [transcript: \"but in less than five minutes the staircase groaned beneath an extraordinary weight\"\n",
      "confidence: 0.9087771773338318\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"but\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"in\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"less\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"than\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  word: \"five\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  word: \"minutes\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"the\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  word: \"staircase\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"groaned\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"beneath\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"an\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  word: \"extraordinary\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"weight\"\n",
      "}\n",
      ", transcript: \"but in less than 5 minutes the staircase groaned beneath an extraordinary weight\"\n",
      "confidence: 0.8721786737442017\n",
      ", transcript: \"but in less than five minutes the staircase groaned beneath an extraordinary wait\"\n",
      "confidence: 0.9087771773338318\n",
      ", transcript: \"but in less than 5 minutes the staircase groaned beneath an extraordinary wait\"\n",
      "confidence: 0.8926088809967041\n",
      ", transcript: \"but in less than five minutes the staircase ground beneath an extraordinary weight\"\n",
      "confidence: 0.8667746186256409\n",
      ", transcript: \"but in less than 5 minutes the staircase ground beneath an extraordinary weight\"\n",
      "confidence: 0.8506063222885132\n",
      ", transcript: \"but in less than five minutes the staircase ground beneath an extraordinary wait\"\n",
      "confidence: 0.8667746186256409\n",
      ", transcript: \"but in less than five minutes the staircase Grande beneath an extraordinary weight\"\n",
      "confidence: 0.9208987355232239\n",
      ", transcript: \"but in less than 5 minutes the staircase ground beneath an extraordinary wait\"\n",
      "confidence: 0.8506063222885132\n",
      ", transcript: \"but in less than 5 minutes the staircase Grande beneath an extraordinary weight\"\n",
      "confidence: 0.904730498790741\n",
      "]\n",
      "\n",
      "\n",
      "RE-RANKED Results: \n",
      " {'but in less than 5 minutes the staircase Grande beneath an extraordinary weight': 0.86024251016788178, 'but in less than 5 minutes the staircase groaned beneath an extraordinary wait': 0.84872697326354674, 'but in less than five minutes the staircase ground beneath an extraordinary wait': 0.82771951705217361, 'but in less than 5 minutes the staircase groaned beneath an extraordinary weight': 0.82931827637366951, 'but in less than five minutes the staircase groaned beneath an extraordinary wait': 0.86744813546538346, 'but in less than five minutes the staircase Grande beneath an extraordinary weight': 0.8789636157453059, 'but in less than five minutes the staircase groaned beneath an extraordinary weight': 0.86744813546538346, 'but in less than five minutes the staircase ground beneath an extraordinary weight': 0.82771951705217361, 'but in less than 5 minutes the staircase ground beneath an extraordinary wait': 0.80899835480377069, 'but in less than 5 minutes the staircase ground beneath an extraordinary weight': 0.80899835480377069}\n",
      "\n",
      "\n",
      "ORIGINAL Transcript: \n",
      "'but in less than 5 minutes the staircase Grande beneath an extraordinary weight' \n",
      "with a confidence_score of: 0.904730498790741\n",
      "RE-RANKED Transcript: \n",
      "'but in less than five minutes the staircase Grande beneath an extraordinary weight' \n",
      "with a confidence_score of: 0.8789636157453059\n",
      "GROUND TRUTH TRANSCRIPT: \n",
      "BUT IN LESS THAN FIVE MINUTES THE STAIRCASE GROANED BENEATH AN EXTRAORDINARY WEIGHT\n",
      "\n",
      "The original transcript was RE-RANKED. The transcripts do not match!\n",
      "Differences between original and re-ranked:  ['5']\n",
      "\n",
      "\n",
      "The original transcript DOES NOT MATCH ground truth.\n",
      "Differences between original and ground truth:  ['5', 'grande']\n",
      "\n",
      "\n",
      "The RE_RANKED transcript DOES NOT MATCH ground truth.\n",
      "Differences between Reranked and ground truth:  ['grande']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL Edit Distance: \n",
      "7\n",
      "RE-RANKED Edit Distance: \n",
      "3\n",
      "\n",
      "\n",
      "Waiting for operation to complete...\n",
      "API Results:  [transcript: \"go do you hear\"\n",
      "confidence: 0.8830462694168091\n",
      "words {\n",
      "  start_time {\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  word: \"go\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"do\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"you\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"hear\"\n",
      "}\n",
      ", transcript: \"go do you here\"\n",
      "confidence: 0.9464384913444519\n",
      ", transcript: \"so do you hear\"\n",
      "confidence: 0.9483599662780762\n",
      ", transcript: \"no do you hear\"\n",
      "confidence: 0.9307782053947449\n",
      "]\n",
      "\n",
      "\n",
      "RE-RANKED Results: \n",
      " {'no do you hear': 0.88448826898820687, 'go do you here': 0.89926208801334717, 'so do you hear': 0.90119094182737169, 'go do you hear': 0.83915772710461167}\n",
      "\n",
      "\n",
      "ORIGINAL Transcript: \n",
      "'no do you hear' \n",
      "with a confidence_score of: 0.9307782053947449\n",
      "RE-RANKED Transcript: \n",
      "'so do you hear' \n",
      "with a confidence_score of: 0.9011909418273717\n",
      "GROUND TRUTH TRANSCRIPT: \n",
      "GO DO YOU HEAR\n",
      "\n",
      "The original transcript was RE-RANKED. The transcripts do not match!\n",
      "Differences between original and re-ranked:  ['no']\n",
      "\n",
      "\n",
      "The original transcript DOES NOT MATCH ground truth.\n",
      "Differences between original and ground truth:  ['no']\n",
      "\n",
      "\n",
      "The RE_RANKED transcript DOES NOT MATCH ground truth.\n",
      "Differences between Reranked and ground truth:  ['so']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL Edit Distance: \n",
      "1\n",
      "RE-RANKED Edit Distance: \n",
      "1\n",
      "\n",
      "\n",
      "Waiting for operation to complete...\n",
      "API Results:  [transcript: \"and the cry issued from his pores if we made us speak a cry frightful in its silence\"\n",
      "confidence: 0.9140974879264832\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"and\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"the\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"cry\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"issued\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  word: \"from\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"his\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  word: \"pores\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "  }\n",
      "  word: \"if\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  word: \"we\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  word: \"made\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"us\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"speak\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 4\n",
      "  }\n",
      "  word: \"a\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 4\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 4\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"cry\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 4\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 5\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  word: \"frightful\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 5\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 5\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"in\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 5\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 5\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"its\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 5\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 6\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  word: \"silence\"\n",
      "}\n",
      ", transcript: \"and the cry issued from his pores if we made us speak a cry frightful in it silence\"\n",
      "confidence: 0.9043365120887756\n",
      ", transcript: \"and the cry issued from his pores if we made us peek a cry frightful in its silence\"\n",
      "confidence: 0.8717085719108582\n",
      ", transcript: \"and the cry issued from his pores if we made us speak a cry frightful and its silence\"\n",
      "confidence: 0.8714524507522583\n",
      ", transcript: \"and the cry issued from his pores if we made us peek a cry frightful in it silence\"\n",
      "confidence: 0.8600011467933655\n",
      ", transcript: \"and the cry issued from his pores if we made us speak a cry frightful and it silence\"\n",
      "confidence: 0.8597450256347656\n",
      ", transcript: \"and the cry issued from his pores if we made us speak a cry frightful init silence\"\n",
      "confidence: 0.9458029270172119\n",
      ", transcript: \"and the cry issued from his pores if we made us speak a cry frightful in it silenced\"\n",
      "confidence: 0.9043365120887756\n",
      ", transcript: \"and the cry issued from his pores if we made us peek a cry frightful and its silence\"\n",
      "confidence: 0.8271170854568481\n",
      ", transcript: \"and the cry issued from his pores if we made us peek a cry frightful and it silence\"\n",
      "confidence: 0.8154096603393555\n",
      "]\n",
      "\n",
      "\n",
      "RE-RANKED Results: \n",
      " {'and the cry issued from his pores if we made us peek a cry frightful and its silence': 0.78660132335498922, 'and the cry issued from his pores if we made us speak a cry frightful in its silence': 0.86929597854614249, 'and the cry issued from his pores if we made us peek a cry frightful in it silence': 0.81784991631284354, 'and the cry issued from his pores if we made us speak a cry frightful and it silence': 0.81766114430502057, 'and the cry issued from his pores if we made us speak a cry frightful in it silenced': 0.86003179121762507, 'and the cry issued from his pores if we made us peek a cry frightful in its silence': 0.82896323045715692, 'and the cry issued from his pores if we made us speak a cry frightful in it silence': 0.86003179121762507, 'and the cry issued from his pores if we made us speak a cry frightful init silence': 0.89941614568233486, 'and the cry issued from his pores if we made us speak a cry frightful and its silence': 0.82878319816663859, 'and the cry issued from his pores if we made us peek a cry frightful and it silence': 0.7754792694933712}\n",
      "\n",
      "\n",
      "ORIGINAL Transcript: \n",
      "'and the cry issued from his pores if we made us peek a cry frightful and it silence' \n",
      "with a confidence_score of: 0.8154096603393555\n",
      "RE-RANKED Transcript: \n",
      "'and the cry issued from his pores if we made us speak a cry frightful init silence' \n",
      "with a confidence_score of: 0.8994161456823349\n",
      "GROUND TRUTH TRANSCRIPT: \n",
      "AND THE CRY ISSUED FROM HIS PORES IF WE MAY THUS SPEAK A CRY FRIGHTFUL IN ITS SILENCE\n",
      "\n",
      "The original transcript was RE-RANKED. The transcripts do not match!\n",
      "Differences between original and re-ranked:  ['peek', 'it']\n",
      "\n",
      "\n",
      "The original transcript DOES NOT MATCH ground truth.\n",
      "Differences between original and ground truth:  ['made', 'peek', 'it', 'us']\n",
      "\n",
      "\n",
      "The RE_RANKED transcript DOES NOT MATCH ground truth.\n",
      "Differences between Reranked and ground truth:  ['made', 'init', 'us']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL Edit Distance: \n",
      "9\n",
      "RE-RANKED Edit Distance: \n",
      "6\n",
      "\n",
      "\n",
      "Waiting for operation to complete...\n",
      "API Results:  [transcript: \"deveny Rush towards the old man and made him and Halo powerful restorative\"\n",
      "confidence: 0.8434420228004456\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 100000000\n",
      "  }\n",
      "  end_time {\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  word: \"deveny\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    nanos: 700000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "  }\n",
      "  word: \"Rush\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  word: \"towards\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 300000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  word: \"the\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  word: \"old\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 600000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  word: \"man\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 1\n",
      "    nanos: 900000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "  }\n",
      "  word: \"and\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  word: \"made\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  word: \"him\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 400000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  word: \"and\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 500000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 2\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"Halo\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 2\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  word: \"powerful\"\n",
      "}\n",
      "words {\n",
      "  start_time {\n",
      "    seconds: 3\n",
      "    nanos: 200000000\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 3\n",
      "    nanos: 800000000\n",
      "  }\n",
      "  word: \"restorative\"\n",
      "}\n",
      ", transcript: \"deveney Rush towards the old man and made him and Halo powerful restorative\"\n",
      "confidence: 0.8541163802146912\n",
      ", transcript: \"deveny rushed towards the old man and made him and Halo powerful restorative\"\n",
      "confidence: 0.8281424045562744\n",
      ", transcript: \"deveny Rush towards the old man and made him and hail powerful restorative\"\n",
      "confidence: 0.7980682849884033\n",
      ", transcript: \"deveney Rush towards the old man and made him and hail powerful restorative\"\n",
      "confidence: 0.8087425231933594\n",
      ", transcript: \"deveny rushed towards the old man and made him and hail powerful restorative\"\n",
      "confidence: 0.7827685475349426\n",
      ", transcript: \"deveney rushed towards the old man and made him and Halo powerful restorative\"\n",
      "confidence: 0.8448580503463745\n",
      ", transcript: \"deveny Rush towards the old man and made him and Hale powerful restorative\"\n",
      "confidence: 0.8532139658927917\n",
      ", transcript: \"devony Rush towards the old man and made him and Halo powerful restorative\"\n",
      "confidence: 0.8753230571746826\n",
      ", transcript: \"deveney Rush towards the old man and made him and Hale powerful restorative\"\n",
      "confidence: 0.8638883829116821\n",
      "]\n",
      "\n",
      "\n",
      "RE-RANKED Results: \n",
      " {'deveny rushed towards the old man and made him and hail powerful restorative': 0.75041957125067704, 'deveney Rush towards the old man and made him and hail powerful restorative': 0.77479364275932305, 'deveney Rush towards the old man and made him and Hale powerful restorative': 0.8271822094917296, 'deveny Rush towards the old man and made him and Hale powerful restorative': 0.81704151332378383, 'deveny Rush towards the old man and made him and hail powerful restorative': 0.76465311646461476, 'deveney rushed towards the old man and made him and Halo powerful restorative': 0.80940459892153738, 'deveney Rush towards the old man and made him and Halo powerful restorative': 0.81789880692958827, 'deveny Rush towards the old man and made him and Halo powerful restorative': 0.80775816738605488, 'deveny rushed towards the old man and made him and Halo powerful restorative': 0.79352473542094226, 'devony Rush towards the old man and made him and Halo powerful restorative': 0.83804515004158009}\n",
      "\n",
      "\n",
      "ORIGINAL Transcript: \n",
      "'deveney Rush towards the old man and made him and Hale powerful restorative' \n",
      "with a confidence_score of: 0.8638883829116821\n",
      "RE-RANKED Transcript: \n",
      "'devony Rush towards the old man and made him and Halo powerful restorative' \n",
      "with a confidence_score of: 0.8380451500415801\n",
      "GROUND TRUTH TRANSCRIPT: \n",
      "D'AVRIGNY RUSHED TOWARDS THE OLD MAN AND MADE HIM INHALE A POWERFUL RESTORATIVE\n",
      "\n",
      "The original transcript was RE-RANKED. The transcripts do not match!\n",
      "Differences between original and re-ranked:  ['hale', 'deveney']\n",
      "\n",
      "\n",
      "The original transcript DOES NOT MATCH ground truth.\n",
      "Differences between original and ground truth:  ['hale', 'deveney', 'rush']\n",
      "\n",
      "\n",
      "The RE_RANKED transcript DOES NOT MATCH ground truth.\n",
      "Differences between Reranked and ground truth:  ['devony', 'halo', 'rush']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL Edit Distance: \n",
      "13\n",
      "RE-RANKED Edit Distance: \n",
      "13\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each word in the evaluation list:\n",
    "# Select word and determine its frequency distribution\n",
    "# Grab probability of second word in the list\n",
    "# Continue this process until the sentence is scored\n",
    "\n",
    "# Add small epsilon value to avoid division by zero\n",
    "epsilon = 0.0000001\n",
    "\n",
    "# Loads the audio into memory\n",
    "for audio, ground_truth in audio_files.items():\n",
    "    with io.open(audio, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "        audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code='en-US',\n",
    "        max_alternatives=10,\n",
    "        profanity_filter=False,\n",
    "        enable_word_time_offsets=True)\n",
    "\n",
    "    # Detects speech and words in the audio file\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "\n",
    "    print('Waiting for operation to complete...')\n",
    "    result = operation.result(timeout=90)\n",
    "\n",
    "    alternatives = result.results[0].alternatives\n",
    "\n",
    "\n",
    "    print(\"API Results: \", alternatives)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    rerank_results = {}\n",
    "    for alternative in alternatives:\n",
    "        sent = alternative.transcript\n",
    "\n",
    "        words = nltk.tokenize.word_tokenize(sent)\n",
    "        probs = np.ones_like(words, dtype=np.float32)*epsilon\n",
    "        # print(words,'\\n',probs)\n",
    "        for word in words:\n",
    "            if words.index(word) < len(words)-1: \n",
    "                freq = cfreq_2gram[word].freq(words[words.index(word)+1])\n",
    "                probs[words.index(word)] = freq\n",
    "            # print(probs)\n",
    "\n",
    "        lexicon_score = np.sum(probs)\n",
    "        # print(word_score)\n",
    "\n",
    "        # Re-rank alternatives using a weighted average of the two scores\n",
    "        api_weight = 0.95\n",
    "        confidence_score = alternative.confidence*api_weight + lexicon_score*(1-api_weight)\n",
    "        rerank_results[alternative.transcript] = confidence_score\n",
    "\n",
    "    print(\"RE-RANKED Results: \\n\", rerank_results)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    import operator\n",
    "    index, value = max(enumerate(list(rerank_results.values())), key=operator.itemgetter(1))\n",
    "    # Select Corresponding Transcript:\n",
    "    script=''\n",
    "    for trnscript, confidence in rerank_results.items():\n",
    "        if confidence == value:\n",
    "            script = trnscript\n",
    "\n",
    "    # Evaluate the differences between the Original and the Reranked transcript:\n",
    "    print(\"ORIGINAL Transcript: \\n'{0}' \\nwith a confidence_score of: {1}\".format(alternative.transcript, alternative.confidence))\n",
    "    print(\"RE-RANKED Transcript: \\n'{0}' \\nwith a confidence_score of: {1}\".format(script, value))\n",
    "    print(\"GROUND TRUTH TRANSCRIPT: \\n{0}\".format(ground_truth))\n",
    "    print()\n",
    "    ranked_differences = list(set(nltk.tokenize.word_tokenize(alternative.transcript.lower())) -\n",
    "                              set(nltk.tokenize.word_tokenize(script.lower())))\n",
    "    if len(ranked_differences) == 0:  \n",
    "        print(\"No reranking was performed. The transcripts match!\")\n",
    "    else:\n",
    "        print(\"The original transcript was RE-RANKED. The transcripts do not match!\")\n",
    "        print(\"Differences between original and re-ranked: \", ranked_differences)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    # Evaluate Differences between the Original and Ground Truth:\n",
    "    gt_orig_diff = list(set(nltk.tokenize.word_tokenize(alternative.transcript.lower())) -\n",
    "                              set(nltk.tokenize.word_tokenize(ground_truth.lower())))\n",
    "    if len(gt_orig_diff) == 0:  \n",
    "        print(\"The ORIGINAL transcript matches ground truth!\")\n",
    "    else:\n",
    "        print(\"The original transcript DOES NOT MATCH ground truth.\")\n",
    "        print(\"Differences between original and ground truth: \", gt_orig_diff)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    gt_rr_diff = list(set(nltk.tokenize.word_tokenize(script.lower())) -\n",
    "                              set(nltk.tokenize.word_tokenize(ground_truth.lower())))\n",
    "    if len(gt_rr_diff) == 0:  \n",
    "        print(\"The RE-RANKED transcript matches ground truth!\")\n",
    "    else:\n",
    "        print(\"The RE_RANKED transcript DOES NOT MATCH ground truth.\")\n",
    "        print(\"Differences between Reranked and ground truth: \", gt_rr_diff)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # Compute the Levenshtein Distance (a.k.a. Edit Distance)\n",
    "#     import nltk.metrics.distance as lev_dist\n",
    "    \n",
    "    # Google API Edit Distance\n",
    "    goog_edit_distance = nltk.edit_distance(alternative.transcript.lower(), ground_truth.lower())\n",
    "    \n",
    "    # Re-Ranked Edit Distance\n",
    "    rr_edit_distance = nltk.edit_distance(script.lower(), ground_truth.lower())\n",
    "\n",
    "    \n",
    "    print(\"ORIGINAL Edit Distance: \\n{0}\".format(goog_edit_distance))\n",
    "    print(\"RE-RANKED Edit Distance: \\n{0}\".format(rr_edit_distance))\n",
    "    print()\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 97 text files in the directory: /Users/deanmwebb/Google Drive/Development/consulting/lexicon/LibriSpeech/dev-clean/**/*.txt\n",
      "10 Transcripts Processed.\n",
      "Average API Accuracy: 0.903240679607\n",
      "Average Custom Model Accuracy: 0.901859249805\n",
      "\n",
      "20 Transcripts Processed.\n",
      "Average API Accuracy: 0.918353468066\n",
      "Average Custom Model Accuracy: 0.936690391278\n",
      "\n",
      "30 Transcripts Processed.\n",
      "Average API Accuracy: 0.929826596805\n",
      "Average Custom Model Accuracy: 0.947840839505\n",
      "\n",
      "40 Transcripts Processed.\n",
      "Average API Accuracy: 0.928767158398\n",
      "Average Custom Model Accuracy: 0.941303845343\n",
      "\n",
      "50 Transcripts Processed.\n",
      "Average API Accuracy: 0.922505902324\n",
      "Average Custom Model Accuracy: 0.941705758542\n",
      "\n",
      "60 Transcripts Processed.\n",
      "Average API Accuracy: 0.926741081298\n",
      "Average Custom Model Accuracy: 0.94743754375\n",
      "\n",
      "70 Transcripts Processed.\n",
      "Average API Accuracy: 0.928304473218\n",
      "Average Custom Model Accuracy: 0.950507872917\n",
      "\n",
      "80 Transcripts Processed.\n",
      "Average API Accuracy: 0.927884314036\n",
      "Average Custom Model Accuracy: 0.951970949538\n",
      "\n",
      "90 Transcripts Processed.\n",
      "Average API Accuracy: 0.931461541754\n",
      "Average Custom Model Accuracy: 0.955575861477\n",
      "\n",
      "100 Transcripts Processed.\n",
      "Average API Accuracy: 0.932289825935\n",
      "Average Custom Model Accuracy: 0.956552296097\n",
      "\n",
      "110 Transcripts Processed.\n",
      "Average API Accuracy: 0.931687082465\n",
      "Average Custom Model Accuracy: 0.954465064124\n",
      "\n",
      "120 Transcripts Processed.\n",
      "Average API Accuracy: 0.92841243576\n",
      "Average Custom Model Accuracy: 0.949485999312\n",
      "\n",
      "130 Transcripts Processed.\n",
      "Average API Accuracy: 0.929766427831\n",
      "Average Custom Model Accuracy: 0.947221616095\n",
      "\n",
      "140 Transcripts Processed.\n",
      "Average API Accuracy: 0.926463354016\n",
      "Average Custom Model Accuracy: 0.944120655654\n",
      "\n",
      "150 Transcripts Processed.\n",
      "Average API Accuracy: 0.928987329366\n",
      "Average Custom Model Accuracy: 0.938826459104\n",
      "\n",
      "160 Transcripts Processed.\n",
      "Average API Accuracy: 0.92622768634\n",
      "Average Custom Model Accuracy: 0.936823919178\n",
      "\n",
      "170 Transcripts Processed.\n",
      "Average API Accuracy: 0.928444796603\n",
      "Average Custom Model Accuracy: 0.938327320237\n",
      "\n",
      "180 Transcripts Processed.\n",
      "Average API Accuracy: 0.928453453572\n",
      "Average Custom Model Accuracy: 0.938808155083\n",
      "\n",
      "190 Transcripts Processed.\n",
      "Average API Accuracy: 0.929513715354\n",
      "Average Custom Model Accuracy: 0.940244068559\n",
      "\n",
      "200 Transcripts Processed.\n",
      "Average API Accuracy: 0.930480181244\n",
      "Average Custom Model Accuracy: 0.941005066938\n",
      "\n",
      "210 Transcripts Processed.\n",
      "Average API Accuracy: 0.924226455431\n",
      "Average Custom Model Accuracy: 0.934748896308\n",
      "\n",
      "220 Transcripts Processed.\n",
      "Average API Accuracy: 0.923679155518\n",
      "Average Custom Model Accuracy: 0.934905078407\n",
      "\n",
      "230 Transcripts Processed.\n",
      "Average API Accuracy: 0.92376882862\n",
      "Average Custom Model Accuracy: 0.934972597878\n",
      "\n",
      "240 Transcripts Processed.\n",
      "Average API Accuracy: 0.921917051067\n",
      "Average Custom Model Accuracy: 0.932329061465\n",
      "\n",
      "250 Transcripts Processed.\n",
      "Average API Accuracy: 0.921991798038\n",
      "Average Custom Model Accuracy: 0.933176779361\n",
      "\n",
      "260 Transcripts Processed.\n",
      "Average API Accuracy: 0.921426162578\n",
      "Average Custom Model Accuracy: 0.932532065323\n",
      "\n",
      "270 Transcripts Processed.\n",
      "Average API Accuracy: 0.920474971027\n",
      "Average Custom Model Accuracy: 0.932503237157\n",
      "\n",
      "280 Transcripts Processed.\n",
      "Average API Accuracy: 0.920528285014\n",
      "Average Custom Model Accuracy: 0.932691930447\n",
      "\n",
      "290 Transcripts Processed.\n",
      "Average API Accuracy: 0.920225760699\n",
      "Average Custom Model Accuracy: 0.932235321356\n",
      "\n",
      "300 Transcripts Processed.\n",
      "Average API Accuracy: 0.920943678942\n",
      "Average Custom Model Accuracy: 0.932780080825\n",
      "\n",
      "310 Transcripts Processed.\n",
      "Average API Accuracy: 0.920661805994\n",
      "Average Custom Model Accuracy: 0.932644050826\n",
      "\n",
      "320 Transcripts Processed.\n",
      "Average API Accuracy: 0.920338369948\n",
      "Average Custom Model Accuracy: 0.932363877515\n",
      "\n",
      "330 Transcripts Processed.\n",
      "Average API Accuracy: 0.921388349615\n",
      "Average Custom Model Accuracy: 0.933508949774\n",
      "\n",
      "340 Transcripts Processed.\n",
      "Average API Accuracy: 0.922062012855\n",
      "Average Custom Model Accuracy: 0.93478877157\n",
      "\n",
      "350 Transcripts Processed.\n",
      "Average API Accuracy: 0.922209889652\n",
      "Average Custom Model Accuracy: 0.93489134889\n",
      "\n",
      "360 Transcripts Processed.\n",
      "Average API Accuracy: 0.92246891763\n",
      "Average Custom Model Accuracy: 0.93554374807\n",
      "\n",
      "370 Transcripts Processed.\n",
      "Average API Accuracy: 0.92257259124\n",
      "Average Custom Model Accuracy: 0.936500324987\n",
      "\n",
      "380 Transcripts Processed.\n",
      "Average API Accuracy: 0.92092702199\n",
      "Average Custom Model Accuracy: 0.93492667951\n",
      "\n",
      "390 Transcripts Processed.\n",
      "Average API Accuracy: 0.92202683573\n",
      "Average Custom Model Accuracy: 0.935823554137\n",
      "\n",
      "400 Transcripts Processed.\n",
      "Average API Accuracy: 0.921689640138\n",
      "Average Custom Model Accuracy: 0.935778178947\n",
      "\n",
      "410 Transcripts Processed.\n",
      "Average API Accuracy: 0.921750247784\n",
      "Average Custom Model Accuracy: 0.935458491426\n",
      "\n",
      "420 Transcripts Processed.\n",
      "Average API Accuracy: 0.922161281703\n",
      "Average Custom Model Accuracy: 0.936154529935\n",
      "\n",
      "430 Transcripts Processed.\n",
      "Average API Accuracy: 0.922949154953\n",
      "Average Custom Model Accuracy: 0.936831885539\n",
      "\n",
      "440 Transcripts Processed.\n",
      "Average API Accuracy: 0.923050758681\n",
      "Average Custom Model Accuracy: 0.937557505729\n",
      "\n",
      "450 Transcripts Processed.\n",
      "Average API Accuracy: 0.923488458299\n",
      "Average Custom Model Accuracy: 0.937584013473\n",
      "\n",
      "460 Transcripts Processed.\n",
      "Average API Accuracy: 0.923615341394\n",
      "Average Custom Model Accuracy: 0.938070691992\n",
      "\n",
      "470 Transcripts Processed.\n",
      "Average API Accuracy: 0.92408538144\n",
      "Average Custom Model Accuracy: 0.93848523152\n",
      "\n",
      "480 Transcripts Processed.\n",
      "Average API Accuracy: 0.923967587937\n",
      "Average Custom Model Accuracy: 0.93863458405\n",
      "\n",
      "490 Transcripts Processed.\n",
      "Average API Accuracy: 0.924104209284\n",
      "Average Custom Model Accuracy: 0.939053572414\n",
      "\n",
      "500 Transcripts Processed.\n",
      "Average API Accuracy: 0.92367107553\n",
      "Average Custom Model Accuracy: 0.938824367726\n",
      "\n",
      "510 Transcripts Processed.\n",
      "Average API Accuracy: 0.921805597943\n",
      "Average Custom Model Accuracy: 0.93724442029\n",
      "\n",
      "520 Transcripts Processed.\n",
      "Average API Accuracy: 0.92102253447\n",
      "Average Custom Model Accuracy: 0.937148501636\n",
      "\n",
      "530 Transcripts Processed.\n",
      "Average API Accuracy: 0.921158829081\n",
      "Average Custom Model Accuracy: 0.937059989347\n",
      "\n",
      "540 Transcripts Processed.\n",
      "Average API Accuracy: 0.92083508977\n",
      "Average Custom Model Accuracy: 0.936362216273\n",
      "\n",
      "550 Transcripts Processed.\n",
      "Average API Accuracy: 0.920832492703\n",
      "Average Custom Model Accuracy: 0.936588690986\n",
      "\n",
      "560 Transcripts Processed.\n",
      "Average API Accuracy: 0.920133649087\n",
      "Average Custom Model Accuracy: 0.936930738918\n",
      "\n",
      "570 Transcripts Processed.\n",
      "Average API Accuracy: 0.919156191727\n",
      "Average Custom Model Accuracy: 0.935684205545\n",
      "\n",
      "580 Transcripts Processed.\n",
      "Average API Accuracy: 0.91868247161\n",
      "Average Custom Model Accuracy: 0.935409101639\n",
      "\n",
      "590 Transcripts Processed.\n",
      "Average API Accuracy: 0.917772498251\n",
      "Average Custom Model Accuracy: 0.934365370485\n",
      "\n",
      "600 Transcripts Processed.\n",
      "Average API Accuracy: 0.917091522671\n",
      "Average Custom Model Accuracy: 0.933759357349\n",
      "\n",
      "610 Transcripts Processed.\n",
      "Average API Accuracy: 0.917500208168\n",
      "Average Custom Model Accuracy: 0.934213779142\n",
      "\n",
      "620 Transcripts Processed.\n",
      "Average API Accuracy: 0.917686693882\n",
      "Average Custom Model Accuracy: 0.934818712124\n",
      "\n",
      "630 Transcripts Processed.\n",
      "Average API Accuracy: 0.917467049856\n",
      "Average Custom Model Accuracy: 0.934629614646\n",
      "\n",
      "640 Transcripts Processed.\n",
      "Average API Accuracy: 0.917456663338\n",
      "Average Custom Model Accuracy: 0.934491992665\n",
      "\n",
      "650 Transcripts Processed.\n",
      "Average API Accuracy: 0.917911472082\n",
      "Average Custom Model Accuracy: 0.935112375751\n",
      "\n",
      "660 Transcripts Processed.\n",
      "Average API Accuracy: 0.918299584354\n",
      "Average Custom Model Accuracy: 0.935376245122\n",
      "\n",
      "670 Transcripts Processed.\n",
      "Average API Accuracy: 0.917384574076\n",
      "Average Custom Model Accuracy: 0.9342850357\n",
      "\n",
      "680 Transcripts Processed.\n",
      "Average API Accuracy: 0.917151446245\n",
      "Average Custom Model Accuracy: 0.934235047041\n",
      "\n",
      "690 Transcripts Processed.\n",
      "Average API Accuracy: 0.917602679915\n",
      "Average Custom Model Accuracy: 0.934437326134\n",
      "\n",
      "700 Transcripts Processed.\n",
      "Average API Accuracy: 0.917820963114\n",
      "Average Custom Model Accuracy: 0.934511287298\n",
      "\n",
      "710 Transcripts Processed.\n",
      "Average API Accuracy: 0.917816423865\n",
      "Average Custom Model Accuracy: 0.934535924093\n",
      "\n",
      "720 Transcripts Processed.\n",
      "Average API Accuracy: 0.917821320348\n",
      "Average Custom Model Accuracy: 0.934593953144\n",
      "\n",
      "730 Transcripts Processed.\n",
      "Average API Accuracy: 0.916924529892\n",
      "Average Custom Model Accuracy: 0.933726706875\n",
      "\n",
      "740 Transcripts Processed.\n",
      "Average API Accuracy: 0.917436122379\n",
      "Average Custom Model Accuracy: 0.933751074212\n",
      "\n",
      "750 Transcripts Processed.\n",
      "Average API Accuracy: 0.917220025499\n",
      "Average Custom Model Accuracy: 0.93310417082\n",
      "\n",
      "760 Transcripts Processed.\n",
      "Average API Accuracy: 0.917388777142\n",
      "Average Custom Model Accuracy: 0.933403210418\n",
      "\n",
      "770 Transcripts Processed.\n",
      "Average API Accuracy: 0.917119628111\n",
      "Average Custom Model Accuracy: 0.932919855993\n",
      "\n",
      "780 Transcripts Processed.\n",
      "Average API Accuracy: 0.917131295891\n",
      "Average Custom Model Accuracy: 0.93317722704\n",
      "\n",
      "790 Transcripts Processed.\n",
      "Average API Accuracy: 0.916037412803\n",
      "Average Custom Model Accuracy: 0.932021975998\n",
      "\n",
      "800 Transcripts Processed.\n",
      "Average API Accuracy: 0.915543375424\n",
      "Average Custom Model Accuracy: 0.931397649052\n",
      "\n",
      "810 Transcripts Processed.\n",
      "Average API Accuracy: 0.915620371007\n",
      "Average Custom Model Accuracy: 0.931399193246\n",
      "\n",
      "820 Transcripts Processed.\n",
      "Average API Accuracy: 0.916030611919\n",
      "Average Custom Model Accuracy: 0.931703551171\n",
      "\n",
      "830 Transcripts Processed.\n",
      "Average API Accuracy: 0.916073798368\n",
      "Average Custom Model Accuracy: 0.931639741796\n",
      "\n",
      "840 Transcripts Processed.\n",
      "Average API Accuracy: 0.916463510765\n",
      "Average Custom Model Accuracy: 0.931840770858\n",
      "\n",
      "850 Transcripts Processed.\n",
      "Average API Accuracy: 0.916800032405\n",
      "Average Custom Model Accuracy: 0.93197923989\n",
      "\n",
      "860 Transcripts Processed.\n",
      "Average API Accuracy: 0.916933343173\n",
      "Average Custom Model Accuracy: 0.932263043487\n",
      "\n",
      "870 Transcripts Processed.\n",
      "Average API Accuracy: 0.917055957951\n",
      "Average Custom Model Accuracy: 0.932511174321\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_DeadlineExceededError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/google/gax/retry.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mto_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_timeout_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mto_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/google/gax/retry.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mupdated_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0ma_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mupdated_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/google/gax/__init__.py\u001b[0m in \u001b[0;36m_done_check\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0m_DeadlineExceededError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_DeadlineExceededError\u001b[0m: Deadline Exceeded",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-680-b22b73ed37cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Detects speech and words in the audio file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0moperation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong_running_recognize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0malternatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malternatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/google/gax/__init__.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \"\"\"\n\u001b[1;32m    594\u001b[0m         \u001b[0;31m# Check exceptional case: raise if no response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHasField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mGaxError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_operation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/google/gax/__init__.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;31m# Start polling, and return the final result from `_done_check`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mretryable_done_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages/google/gax/retry.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# expected delay.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0mto_sleep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelay\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_sleep\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0m_MILLIS_PER_SECOND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0mdelay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdelay_mult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_delay_millis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Gather all samples, load into dictionary\n",
    "# Prepare a plain text corpus from which we train a languague model\n",
    "import glob\n",
    "import operator\n",
    "\n",
    "# Gather all text files from directory\n",
    "WORKING_DIRECTORY = os.path.join(os.getcwd(),'LibriSpeech/')\n",
    "\n",
    "# TRAINING_DIRECTORY = os.path.abspath(os.path.join(os.sep,'Volumes',\"My\\ Passport\\ for\\ Mac\",'lexicon','LibriSpeech'))\n",
    "dev_path = \"{}{}{}{}\".format(WORKING_DIRECTORY, 'dev-clean/', '**/', '*.txt')\n",
    "train_path = \"{}{}{}{}{}\".format(WORKING_DIRECTORY, 'books/', 'utf-8/', '**/', '*.txt*')\n",
    "\n",
    "text_paths = sorted(glob.glob(dev_path, recursive=True))\n",
    "print('Found',len(text_paths),'text files in the directory:', dev_path)\n",
    "\n",
    "transcripts = {}\n",
    "for document in text_paths:\n",
    "    with codecs.open(document, 'r', 'utf-8') as filep:\n",
    "        for i,line in enumerate(filep):\n",
    "            transcripts[line.split()[0]] = ' '.join(line.split()[1:])\n",
    "\n",
    "# Save Dictionary in Pickle File\n",
    "\n",
    "\n",
    "## Evaluate all samples found ##\n",
    "cloud_speech_api_accuracy = []\n",
    "custom_lang_model_accuracy = []\n",
    "epsilon = 0.000000001\n",
    "api_weight = 0.85\n",
    "steps = 0\n",
    "# Pull In Audio File\n",
    "for filename, gt_transcript in transcripts.items():\n",
    "    steps += 1\n",
    "    dirs = filename.split('-')\n",
    "    \n",
    "    audio_filepath = dev_file_name_0 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'LibriSpeech',\n",
    "    'dev-clean',\n",
    "    dirs[0],\n",
    "    dirs[1],\n",
    "    \"{0}.flac\".format(filename))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Load the audio into memory\n",
    "    with io.open(audio_filepath, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "        audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.FLAC,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code='en-US',\n",
    "        max_alternatives=10,\n",
    "        profanity_filter=False,\n",
    "        enable_word_time_offsets=True)\n",
    "\n",
    "    # Detects speech and words in the audio file\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "    result = operation.result(timeout=90)\n",
    "    alternatives = result.results[0].alternatives\n",
    "\n",
    "\n",
    "    # Evaluate API Results for Re-Ranking:\n",
    "    rerank_results = {}\n",
    "    for alternative in alternatives:\n",
    "        sent = alternative.transcript\n",
    "        \n",
    "        # Strip punctuation\n",
    "        translate_table = dict((ord(char), None) for char in string.punctuation)        \n",
    "        sent = sent.translate(translate_table) # remove punctuations\n",
    "\n",
    "        words = nltk.tokenize.word_tokenize(sent)\n",
    "        probs = np.ones_like(words, dtype=np.float32)*epsilon\n",
    "\n",
    "        for word in words:\n",
    "            if words.index(word) < len(words)-1: \n",
    "                freq = cfreq_2gram[word].freq(words[words.index(word)+1])\n",
    "                probs[words.index(word)] = freq\n",
    "\n",
    "        lexicon_score = np.sum(probs)\n",
    "\n",
    "        # Re-rank alternatives using a weighted average of the two scores\n",
    "        confidence_score = alternative.confidence*api_weight + lexicon_score*(1-api_weight)\n",
    "        rerank_results[alternative.transcript] = confidence_score\n",
    "\n",
    "\n",
    "    \n",
    "    index, value = max(enumerate(list(rerank_results.values())), key=operator.itemgetter(1))\n",
    "    # Select Corresponding Transcript:\n",
    "    script=''\n",
    "    for trnscript, confidence in rerank_results.items():\n",
    "        if confidence == value:\n",
    "            script = trnscript\n",
    "                \n",
    "    # Compute the Accuracy, based on the Levenshtein Distance (a.k.a. Edit Distance)\n",
    "    gcs_ed = nltk.edit_distance(alternative.transcript.lower(), gt_transcript.lower())\n",
    "    gcs_upper_bound = max(len(alternative.transcript),len(gt_transcript))\n",
    "    gcs_accuracy = (1.0 - gcs_ed/gcs_upper_bound)\n",
    "    \n",
    "    clm_ed = nltk.edit_distance(script.lower(), gt_transcript.lower())\n",
    "    clm_upper_bound = max(len(script),len(gt_transcript))\n",
    "    clm_accuracy = (1.0 - clm_ed/clm_upper_bound)\n",
    "    \n",
    "    cloud_speech_api_accuracy.append(gcs_accuracy)\n",
    "    custom_lang_model_accuracy.append(clm_accuracy)\n",
    "\n",
    "    if steps % 10 == 0:\n",
    "        print(\"{0} Transcripts Processed.\".format(steps))\n",
    "        print('Average API Accuracy:', np.mean(cloud_speech_api_accuracy))\n",
    "        print('Average Custom Model Accuracy:', np.mean(custom_lang_model_accuracy))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:sdc_dev]",
   "language": "python",
   "name": "conda-env-sdc_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
