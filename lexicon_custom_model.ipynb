{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon - Custom Language Model\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "For this project, I will build a simple custom language model that is able to learn from any text data provided, and return a transcript with confidence values from input posed in speech utterances. I will use Google's cloud-based services to preprocess the input audio data and transcribe into an initial guess. Then I will train a model to improve on Google cloud speech API's response.\n",
    "\n",
    "\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "In order to use Google's cloud-based services, you first need to create an account on the [Google Cloud Platform](https://cloud.google.com//).\n",
    "\n",
    "Then, for each service you want to use, you have to enable use of that service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: google-cloud-speech in /Users/deanmwebb/anaconda/lib/python2.7/site-packages\n",
      "Requirement already up-to-date: google-gax<0.16dev,>=0.15.14 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-cloud-speech)\n",
      "Requirement already up-to-date: google-cloud-core<0.28dev,>=0.27.0 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-cloud-speech)\n",
      "Requirement already up-to-date: googleapis-common-protos[grpc]<2.0dev,>=1.5.2 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-cloud-speech)\n",
      "Requirement already up-to-date: ply==3.8 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: dill<0.3dev,>=0.2.5 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: future<0.17dev,>=0.16.0 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: grpcio<2.0dev,>=1.0.2 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: google-auth<2.0dev,>=1.0.0 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: protobuf<4.0dev,>=3.0.0 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: requests<3.0dev,>=2.13.0 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: six>=1.10.0 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-cloud-core<0.28dev,>=0.27.0->google-cloud-speech)\n",
      "Requirement already up-to-date: futures>=3.0.0; python_version < \"3.2\" in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-cloud-core<0.28dev,>=0.27.0->google-cloud-speech)\n",
      "Requirement already up-to-date: setuptools>=34.0.0 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-cloud-core<0.28dev,>=0.27.0->google-cloud-speech)\n",
      "Requirement already up-to-date: enum34>=1.0.4 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from grpcio<2.0dev,>=1.0.2->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: cachetools>=2.0.0 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-auth<2.0dev,>=1.0.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: rsa>=3.1.4 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-auth<2.0dev,>=1.0.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: pyasn1>=0.1.7 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-auth<2.0dev,>=1.0.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: pyasn1-modules>=0.0.5 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from google-auth<2.0dev,>=1.0.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: urllib3<1.23,>=1.21.1 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from requests<3.0dev,>=2.13.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: idna<2.7,>=2.5 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from requests<3.0dev,>=2.13.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: chardet<3.1.0,>=3.0.2 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from requests<3.0dev,>=2.13.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n",
      "Requirement already up-to-date: certifi>=2017.4.17 in /Users/deanmwebb/anaconda/lib/python2.7/site-packages (from requests<3.0dev,>=2.13.0->google-gax<0.16dev,>=0.15.14->google-cloud-speech)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-cloud-speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Google Cloud SDK: https://cloud.google.com/sdk/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Google Cloud SDK!\n",
      "\n",
      "To help improve the quality of this product, we collect anonymized usage data\n",
      "and anonymized stacktraces when crashes are encountered; additional information\n",
      "is available at <https://cloud.google.com/sdk/usage-statistics>. You may choose\n",
      "to opt out of this collection now (by choosing 'N' at the below prompt), or at\n",
      "any time in the future by running the following command:\n",
      "\n",
      "    gcloud config set disable_usage_reporting true\n",
      "\n",
      "\n",
      "Your current Cloud SDK version is: 170.0.1\n",
      "The latest available version is: 171.0.0\n",
      "\n",
      "┌────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│                                                   Components                                                   │\n",
      "├──────────────────┬──────────────────────────────────────────────────────┬──────────────────────────┬───────────┤\n",
      "│      Status      │                         Name                         │            ID            │    Size   │\n",
      "├──────────────────┼──────────────────────────────────────────────────────┼──────────────────────────┼───────────┤\n",
      "│ Update Available │ Cloud SDK Core Libraries                             │ core                     │   6.7 MiB │\n",
      "│ Not Installed    │ App Engine Go Extensions                             │ app-engine-go            │  97.7 MiB │\n",
      "│ Not Installed    │ Cloud Bigtable Command Line Tool                     │ cbt                      │   4.0 MiB │\n",
      "│ Not Installed    │ Cloud Bigtable Emulator                              │ bigtable                 │   3.5 MiB │\n",
      "│ Not Installed    │ Cloud Datalab Command Line Tool                      │ datalab                  │   < 1 MiB │\n",
      "│ Not Installed    │ Cloud Datastore Emulator                             │ cloud-datastore-emulator │  15.4 MiB │\n",
      "│ Not Installed    │ Cloud Datastore Emulator (Legacy)                    │ gcd-emulator             │  38.1 MiB │\n",
      "│ Not Installed    │ Cloud Pub/Sub Emulator                               │ pubsub-emulator          │  33.2 MiB │\n",
      "│ Not Installed    │ Emulator Reverse Proxy                               │ emulator-reverse-proxy   │  14.5 MiB │\n",
      "│ Not Installed    │ Google Container Local Builder                       │ container-builder-local  │   3.7 MiB │\n",
      "│ Not Installed    │ Google Container Registry's Docker credential helper │ docker-credential-gcr    │   2.2 MiB │\n",
      "│ Not Installed    │ gcloud Alpha Commands                                │ alpha                    │   < 1 MiB │\n",
      "│ Not Installed    │ gcloud Beta Commands                                 │ beta                     │   < 1 MiB │\n",
      "│ Not Installed    │ gcloud app Java Extensions                           │ app-engine-java          │ 130.8 MiB │\n",
      "│ Not Installed    │ gcloud app PHP Extensions                            │ app-engine-php           │  21.9 MiB │\n",
      "│ Not Installed    │ gcloud app Python Extensions                         │ app-engine-python        │   6.3 MiB │\n",
      "│ Not Installed    │ kubectl                                              │ kubectl                  │  15.9 MiB │\n",
      "│ Installed        │ BigQuery Command Line Tool                           │ bq                       │   < 1 MiB │\n",
      "│ Installed        │ Cloud Storage Command Line Tool                      │ gsutil                   │   3.0 MiB │\n",
      "└──────────────────┴──────────────────────────────────────────────────────┴──────────────────────────┴───────────┘\n",
      "To install or remove components at your current SDK version [170.0.1], run:\n",
      "  $ gcloud components install COMPONENT_ID\n",
      "  $ gcloud components remove COMPONENT_ID\n",
      "\n",
      "To update your SDK installation to the latest version [171.0.0], run:\n",
      "  $ gcloud components update\n",
      "\n",
      "==> Source [/Users/deanmwebb/Google Drive/Development/consulting/lexicon/google-cloud-sdk/completion.bash.inc] in your profile to enable shell command completion for gcloud.\n",
      "==> Source [/Users/deanmwebb/Google Drive/Development/consulting/lexicon/google-cloud-sdk/path.bash.inc] in your profile to add the Google Cloud SDK command line tools to your $PATH.\n",
      "\n",
      "For more information on how to get started, please visit:\n",
      "  https://cloud.google.com/sdk/docs/quickstarts\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!CLOUDSDK_CORE_DISABLE_PROMPTS=1 ./google-cloud-sdk/install.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate with Google Cloud API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activated service account credentials for: [lexicon-bot@exemplary-oath-179301.iam.gserviceaccount.com]\r\n"
     ]
    }
   ],
   "source": [
    "!source google-cloud-sdk/completion.bash.inc && \\\n",
    "source google-cloud-sdk/path.bash.inc && \\\n",
    "gcloud auth activate-service-account lexicon-bot@exemplary-oath-179301.iam.gserviceaccount.com --key-file=Lexicon-e94eff39fad7.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']='/Users/deanmwebb/Google Drive/Development/consulting/lexicon/Lexicon-e94eff39fad7.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test out Cloud Spech API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for operation to complete...\n",
      "Transcript: the boy hears too much of what is talked about here\n",
      "Confidence Score: 0.9152035117149353\n",
      "Word: the, start_time (s): 0.1, end_time (s): 0.5, total_time (s): 0.4\n",
      "Word: boy, start_time (s): 0.5, end_time (s): 0.7000000000000001, total_time (s): 0.20000000000000007\n",
      "Word: hears, start_time (s): 0.7000000000000001, end_time (s): 1.1, total_time (s): 0.4\n",
      "Word: too, start_time (s): 1.1, end_time (s): 1.2, total_time (s): 0.09999999999999987\n",
      "Word: much, start_time (s): 1.2, end_time (s): 1.4, total_time (s): 0.19999999999999996\n",
      "Word: of, start_time (s): 1.4, end_time (s): 1.5, total_time (s): 0.10000000000000009\n",
      "Word: what, start_time (s): 1.5, end_time (s): 1.6, total_time (s): 0.10000000000000009\n",
      "Word: is, start_time (s): 1.6, end_time (s): 1.7000000000000002, total_time (s): 0.10000000000000009\n",
      "Word: talked, start_time (s): 1.7000000000000002, end_time (s): 2.0, total_time (s): 0.2999999999999998\n",
      "Word: about, start_time (s): 2.0, end_time (s): 2.1, total_time (s): 0.10000000000000009\n",
      "Word: here, start_time (s): 2.1, end_time (s): 2.5, total_time (s): 0.3999999999999999\n",
      "Transcript: the boy here's too much of what is talked about here\n",
      "Confidence Score: 0.9053400158882141\n",
      "Transcript: Pep Boy hears too much of what is talked about here\n",
      "Confidence Score: 0.9245736598968506\n",
      "Transcript: purple here's too much of what is talked about here\n",
      "Confidence Score: 0.9280999302864075\n",
      "Transcript: the boys ears too much of what is talked about here\n",
      "Confidence Score: 0.9337901473045349\n",
      "Transcript: purple hears too much of what is talked about here\n",
      "Confidence Score: 0.9368866086006165\n",
      "Transcript: couple hears too much of what is talked about here\n",
      "Confidence Score: 0.9381117820739746\n",
      "Transcript: the boy here's to much of what is talked about here\n",
      "Confidence Score: 0.8731024265289307\n",
      "Transcript: before here's too much of what is talked about here\n",
      "Confidence Score: 0.9293252229690552\n",
      "Transcript: the boy hears to much of what is talked about here\n",
      "Confidence Score: 0.8793312311172485\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import speech\n",
    "from google.cloud.speech import enums\n",
    "from google.cloud.speech import types\n",
    "\n",
    "# Instantiates a client\n",
    "client = speech.SpeechClient()\n",
    "\n",
    "# The name of the dev-test audio file to transcribe\n",
    "dev_file_name_1 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'RNN-Tutorial-master',\n",
    "    'data',\n",
    "    'raw',\n",
    "    'librivox',\n",
    "    'LibriSpeech',\n",
    "    'dev-clean-wav',\n",
    "    '777-126732-0068.wav')\n",
    "\n",
    "# The name of the test audio file to transcribe\n",
    "dev_file_name_2 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'RNN-Tutorial-master',\n",
    "    'data',\n",
    "    'raw',\n",
    "    'librivox',\n",
    "    'LibriSpeech',\n",
    "    'dev-clean-wav',\n",
    "    '3752-4944-0041.wav')\n",
    "\n",
    "test_file_name_1 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'RNN-Tutorial-master',\n",
    "    'data',\n",
    "    'raw',\n",
    "    'librivox',\n",
    "    'LibriSpeech',\n",
    "    'test-clean-wav',\n",
    "    '4507-16021-0019.wav')\n",
    "\n",
    "test_file_name_2 = os.path.join(\n",
    "    os.getcwd(),\n",
    "    'RNN-Tutorial-master',\n",
    "    'data',\n",
    "    'raw',\n",
    "    'librivox',\n",
    "    'LibriSpeech',\n",
    "    'test-clean-wav',\n",
    "    '7176-92135-0009.wav')\n",
    "\n",
    "# Loads the audio into memory\n",
    "with io.open(dev_file_name_1, 'rb') as audio_file:\n",
    "    content = audio_file.read()\n",
    "    audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "config = types.RecognitionConfig(\n",
    "    encoding=enums.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "    sample_rate_hertz=16000,\n",
    "    language_code='en-US',\n",
    "    max_alternatives=10,\n",
    "    profanity_filter=False,\n",
    "    enable_word_time_offsets=True)\n",
    "\n",
    "# Detects speech and words in the audio file\n",
    "operation = client.long_running_recognize(config, audio)\n",
    "\n",
    "print('Waiting for operation to complete...')\n",
    "result = operation.result(timeout=90)\n",
    "\n",
    "alternatives = result.results[0].alternatives\n",
    "for alternative in alternatives:\n",
    "    print('Transcript: {}'.format(alternative.transcript))\n",
    "    print('Confidence Score: {}'.format(alternative.confidence))\n",
    "\n",
    "    for word_info in alternative.words:\n",
    "        word = word_info.word\n",
    "        start_time = word_info.start_time\n",
    "        end_time = word_info.end_time\n",
    "        start = start_time.seconds + start_time.nanos * 1e-9\n",
    "        end = end_time.seconds + end_time.nanos * 1e-9\n",
    "        delta = end - start\n",
    "        \n",
    "        print('Word: {}, start_time (s): {}, end_time (s): {}, total_time (s): {}'.format(\n",
    "            word,\n",
    "            start,\n",
    "            end,\n",
    "            delta))\n",
    "        \n",
    "        #TODO: Do we need to figure out how to assign words to alternatives?\n",
    "            # If same amounts, assign words to index of parsed word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Download the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "\n",
    "librispeech_dataset_folder_path = 'LibriSpeech'\n",
    "tar_gz_path = 'dev-clean.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Librispeech dev-clean.tar.gz') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://www.openslr.org/resources/12/dev-clean.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(librispeech_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Preprocess Dataset - Download Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk #NLP Toolkit\n",
    "nltk.download()\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gensim Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages\n",
      "Requirement already up-to-date: six>=1.5.0 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already up-to-date: smart-open>=1.2.1 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already up-to-date: numpy>=1.11.3 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already up-to-date: scipy>=0.18.1 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already up-to-date: requests in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already up-to-date: bz2file in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already up-to-date: boto>=2.32 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already up-to-date: certifi>=2017.4.17 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already up-to-date: chardet<3.1.0,>=3.0.2 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already up-to-date: urllib3<1.23,>=1.21.1 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already up-to-date: idna<2.7,>=2.5 in /Users/deanmwebb/anaconda/envs/sdc_dev/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41 text files in the directory: /Users/deanmwebb/Google Drive/Development/consulting/lexicon/LibriSpeech/books/utf-8/**/*.txt*\n"
     ]
    }
   ],
   "source": [
    "# Prepare a plain text corpus from which we train a languague model\n",
    "import glob\n",
    "\n",
    "# Gather all text files from directory\n",
    "WORKING_DIRECTORY = os.path.join(os.getcwd(),'LibriSpeech/')\n",
    "\n",
    "# TRAINING_DIRECTORY = os.path.abspath(os.path.join(os.sep,'Volumes',\"My\\ Passport\\ for\\ Mac\",'lexicon','LibriSpeech'))\n",
    "dev_path = \"{}{}{}{}\".format(WORKING_DIRECTORY, 'dev-clean/', '**/', '*.txt')\n",
    "train_path = \"{}{}{}{}{}\".format(WORKING_DIRECTORY, 'books/', 'utf-8/', '**/', '*.txt*')\n",
    "\n",
    "text_paths = sorted(glob.glob(train_path, recursive=True))\n",
    "print('Found',len(text_paths),'text files in the directory:', train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset - Tokenize Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "# reading the file in unicode format using codecs library    \n",
    "stoplist = set(stopwords.words('english'))\n",
    "\n",
    "corpus_raw = u\"\"\n",
    "for book_filename in text_paths:\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw += book_file.read()\n",
    "\n",
    "# Tokenize\n",
    "tokenized_words = nltk.tokenize.word_tokenize(corpus_raw)\n",
    "\n",
    "## Clean the tokens ##\n",
    "# Remove stop words\n",
    "tokenized_words = [word for word in tokenized_words if word not in stoplist]\n",
    "\n",
    "# Remove single-character tokens (mostly punctuation)\n",
    "tokenized_words = [word for word in tokenized_words if len(word) > 1]\n",
    "\n",
    "# Remove numbers\n",
    "tokenized_words = [word for word in tokenized_words if not word.isnumeric()]\n",
    "\n",
    "# Lowercase all words (default_stopwords are lowercase too)\n",
    "tokenized_words = [word.lower() for word in tokenized_words]\n",
    "\n",
    "\n",
    "## DON'T USE THIS ##\n",
    "# Stemming words seems to make matters worse, disabled\n",
    "# stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "# tokenized_words = [stemmer.stem(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset - Extract N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.collocations import *\n",
    "from nltk.probability import FreqDist\n",
    "import nltk\n",
    "\n",
    "# extracting the bi-grams and sorting them according to their frequencies\n",
    "finder = BigramCollocationFinder.from_words(tokenized_words)\n",
    "finder.apply_freq_filter(3)\n",
    "\n",
    "bigram_model = nltk.bigrams(tokenized_words)\n",
    "bigram_model = sorted(bigram_model, key=lambda item: item[1], reverse=True)  \n",
    "# print(bigram_model)\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "np.save(\"lang_model.npy\",bigram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(bigram_model)\n",
    "\n",
    "# Output top 50 words\n",
    "print(\"Word|Freq:\")\n",
    "for word, frequency in fdist.most_common(50):\n",
    "    print(u'{}|{}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfreq_2gram = nltk.ConditionalFreqDist(bigram_model)\n",
    "# print('Conditional Frequency Conditions:\\n', cfreq_2gram)\n",
    "print()\n",
    "\n",
    "# First access the FreqDist associated with \"one\", then the keys in that FreqDist\n",
    "print(\"Listing the words that can follow after 'said':\\n\", cfreq_2gram[\"come\"].keys())\n",
    "print()\n",
    "\n",
    "# Determine Most common in conditional frequency\n",
    "print(\"Listing 20 most frequent words to come after 'said':\\n\", cfreq_2gram[\"come\"].most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate Approach: Use Gensim to Create Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24379\n"
     ]
    }
   ],
   "source": [
    "# Word Encoding\n",
    "import codecs\n",
    "# Regex\n",
    "import glob\n",
    "import re\n",
    "# Natural Language Toolkit\n",
    "import nltk\n",
    "# Word2Vec\n",
    "import gensim.models.word2vec as w2v\n",
    "# Dimensionality Reduction\n",
    "import sklearn.manifold\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "# Stop List\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# reading the file in unicode format using codecs library    \n",
    "stoplist = set(stopwords.words('english'))\n",
    "transcripts = [[[word for word in line.lower().split()[1:] if word not in stoplist]\n",
    "         for i,line in enumerate(codecs.open(document,\"r\",\"utf-8\"))]\n",
    "         for document in text_paths]\n",
    "\n",
    "# Reshape list to be a transcript for each row\n",
    "transcript_list = []\n",
    "for doc in transcripts:\n",
    "    for line in doc:\n",
    "        if len(line) > 0:\n",
    "            transcript_list.append(line)\n",
    "\n",
    "# Convert wordlist to raw corpus\n",
    "corpus_raw = u\"\"\n",
    "for line in transcript_list:\n",
    "    corpus_raw += u\" \".join(line)\n",
    "\n",
    "# Tokenize\n",
    "raw_sentences = nltk.word_tokenize(corpus_raw)\n",
    "# print(raw_sentences)\n",
    "print(len(raw_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Alternate approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2703\n",
      "['thou', 'like', 'arcturus', 'steadfast', 'skies', 'with', 'tardy', 'sense', 'guidest', 'thy', 'kingdom', 'fair', 'bearing', 'alone', 'load', 'liberty']\n",
      "46582\n",
      ".\n",
      "[('of', 'your'), ('on', 'your'), ('the', 'young'), ('and', 'you'), ('that', 'you'), ('for', 'you'), ('.', 'you'), ('tell', 'you'), ('if', 'you'), ('with', 'you'), ('perhaps', 'you'), ('then', 'you'), ('do', 'you'), ('as', 'you'), ('to', 'you'), ('but', 'you'), ('see', 'you'), (\"n't\", 'you'), ('of', 'you'), ('are', 'you'), ('and', 'yet'), ('.', 'yet'), ('.', 'yes'), ('a', 'year'), ('you', 'would'), ('she', 'would'), ('i', 'would'), ('it', 'would'), ('he', 'would'), ('the', 'world'), ('the', 'words'), ('a', 'word'), ('miss', 'woodley'), ('.', 'with'), ('satisfied', 'with'), ('up', 'with'), ('and', 'with'), ('covered', 'with'), ('it', 'with'), ('him', 'with'), ('the', 'window'), ('the', 'wind'), ('it', 'will'), ('we', 'will'), ('you', 'will'), ('i', 'will'), ('his', 'wife'), ('the', 'whole'), ('one', 'who'), ('.', 'who'), ('people', 'who'), ('those', 'who'), ('a', 'white'), ('the', 'white'), ('.', 'while'), ('in', 'which'), ('that', 'which'), ('of', 'which'), ('to', 'which'), ('with', 'which'), ('but', 'when'), ('and', 'when'), ('.', 'when'), ('know', 'what'), ('and', 'what'), ('see', 'what'), ('of', 'what'), ('.', 'what'), ('you', 'were'), ('there', 'were'), ('and', 'were'), ('it', 'were'), ('who', 'were'), ('they', 'were'), ('we', 'were'), ('that', 'were'), ('he', 'went'), ('and', 'went'), ('.', 'well'), ('very', 'well'), ('as', 'well'), ('when', 'we'), ('that', 'we'), ('.', 'we'), ('and', 'we'), ('if', 'we'), ('as', 'we'), ('the', 'way'), ('the', 'water'), ('and', 'was'), ('which', 'was'), ('he', 'was'), ('that', 'was'), ('she', 'was'), ('there', 'was'), ('it', 'was'), ('man', 'was'), ('what', 'was'), ('who', 'was'), ('i', 'was'), ('this', 'was'), ('his', 'voice'), ('his', 'violin'), ('is', 'very'), ('.', 'very'), ('the', 'very'), ('a', 'very'), ('mister', 'verloc'), ('in', 'vain'), ('for', 'us'), ('of', 'us'), ('got', 'up'), ('came', 'up'), ('it', 'up'), ('.', 'under'), ('the', 'two'), ('or', 'two'), ('to', 'try'), ('the', 'train'), ('the', 'top'), ('i', 'took'), ('he', 'took'), ('on', 'to'), ('been', 'to'), ('happened', 'to'), ('is', 'to'), ('seemed', 'to'), ('as', 'to'), ('and', 'to'), ('trying', 'to'), ('went', 'to'), ('came', 'to'), ('her', 'to'), ('had', 'to'), ('wish', 'to'), ('not', 'to'), ('were', 'to'), ('able', 'to'), ('out', 'to'), ('returned', 'to'), ('wanted', 'to'), ('was', 'to'), ('us', 'to'), ('them', 'to'), ('owing', 'to'), ('back', 'to'), ('ready', 'to'), ('go', 'to'), ('me', 'to'), ('it', 'to'), ('him', 'to'), ('how', 'to'), ('desire', 'to'), ('enough', 'to'), ('come', 'to'), ('up', 'to'), ('seems', 'to'), ('began', 'to'), ('seem', 'to'), ('have', 'to'), ('time', 'to'), ('himself', 'to'), ('said', 'to'), ('.', 'to'), ('going', 'to'), ('according', 'to'), ('you', 'to'), ('ought', 'to'), ('accustomed', 'to'), ('appeared', 'to'), ('used', 'to'), ('first', 'time'), ('a', 'time'), ('this', 'time'), ('some', 'time'), ('the', 'time'), ('or', 'three'), ('a', 'thousand'), ('he', 'thought'), ('i', 'thought'), ('as', 'though'), ('of', 'those'), ('in', 'this'), ('on', 'this'), ('to', 'this'), ('.', 'this'), ('of', 'this'), ('at', 'this'), ('the', 'third'), ('i', 'think'), ('you', 'think'), ('to', 'think'), ('of', 'things'), ('as', 'they'), ('.', 'they'), ('if', 'they'), ('that', 'they'), ('but', 'they'), ('where', 'they'), ('when', 'they'), ('and', 'they'), ('than', 'they'), ('of', 'these'), ('.', 'these'), ('and', 'there'), ('for', 'there'), ('.', 'there'), ('that', 'there'), ('.', 'then'), ('it', 'then'), ('and', 'then'), ('of', 'them'), ('with', 'them'), ('put', 'them'), ('to', 'them'), ('from', 'their'), ('to', 'their'), ('of', 'their'), ('.', 'their'), ('in', 'their'), ('for', 'their'), ('with', 'their'), ('on', 'the'), ('them', 'the'), ('reached', 'the'), ('it', 'the'), ('all', 'the'), ('take', 'the'), ('since', 'the'), ('are', 'the'), ('between', 'the'), ('across', 'the'), ('had', 'the'), ('along', 'the'), ('which', 'the'), ('like', 'the'), ('while', 'the'), ('until', 'the'), ('but', 'the'), ('near', 'the'), ('that', 'the'), ('is', 'the'), ('during', 'the'), ('than', 'the'), ('see', 'the'), ('or', 'the'), ('against', 'the'), ('round', 'the'), ('only', 'the'), ('from', 'the'), ('.', 'the'), ('opened', 'the'), ('saw', 'the'), ('before', 'the'), ('into', 'the'), ('be', 'the'), ('now', 'the'), ('entered', 'the'), ('as', 'the'), ('at', 'the'), ('if', 'the'), ('add', 'the'), ('left', 'the'), ('of', 'the'), ('behind', 'the'), ('been', 'the'), ('for', 'the'), ('then', 'the'), ('said', 'the'), ('has', 'the'), ('not', 'the'), ('over', 'the'), ('under', 'the'), ('with', 'the'), ('were', 'the'), ('made', 'the'), (\"'s\", 'the'), ('down', 'the'), ('answered', 'the'), ('upon', 'the'), ('to', 'the'), ('and', 'the'), ('took', 'the'), ('by', 'the'), ('through', 'the'), ('put', 'the'), ('out', 'the'), ('when', 'the'), ('in', 'the'), ('time', 'the'), ('among', 'the'), ('him', 'the'), ('have', 'the'), ('about', 'the'), ('after', 'the'), ('her', 'the'), ('asked', 'the'), ('above', 'the'), ('up', 'the'), ('off', 'the'), ('was', 'the'), ('him', 'that'), ('on', 'that'), ('knew', 'that'), ('all', 'that'), ('aware', 'that'), ('was', 'that'), ('and', 'that'), ('so', 'that'), ('for', 'that'), ('with', 'that'), ('see', 'that'), ('out', 'that'), ('.', 'that'), ('know', 'that'), ('from', 'that'), ('than', 'that'), ('thought', 'that'), ('felt', 'that'), ('but', 'that'), ('of', 'that'), ('me', 'that'), ('think', 'that'), ('say', 'that'), ('in', 'that'), ('at', 'that'), ('better', 'than'), ('more', 'than'), ('to', 'tell'), ('i', 'tell'), ('had', 'taken'), ('to', 'take'), ('the', 'table'), ('i', 'suppose'), ('the', 'sun'), ('.', 'sufficient'), ('of', 'such'), ('to', 'such'), ('a', 'strong'), ('in', 'spite'), ('i', 'speak'), ('to', 'speak'), ('the', 'south'), ('the', 'sound'), ('a', 'sort'), ('as', 'soon'), ('in', 'some'), ('with', 'some'), ('.', 'some'), ('for', 'some'), ('and', 'so'), ('was', 'so'), ('are', 'so'), ('.', 'so'), ('the', 'slightest'), ('wandering', 'singer'), ('one', 'side'), ('the', 'side'), ('to', 'show'), ('they', 'should'), ('we', 'should'), ('i', 'should'), ('when', 'she'), ('where', 'she'), ('.', 'she'), ('and', 'she'), ('which', 'she'), ('what', 'she'), ('that', 'she'), ('if', 'she'), ('then', 'she'), ('as', 'she'), ('but', 'she'), ('you', 'shall'), ('i', 'shall'), ('had', 'seen'), ('have', 'seen'), ('it', 'seemed'), ('i', 'see'), ('to', 'see'), ('and', 'see'), ('could', 'see'), ('you', 'see'), ('the', 'second'), ('the', 'sea'), ('the', 'scene'), ('to', 'say'), ('i', 'say'), ('he', 'saw'), ('i', 'saw'), ('the', 'same'), ('and', 'said'), ('i', 'said'), ('she', 'said'), ('he', 'said'), ('the', 'room'), ('the', 'roof'), ('the', 'road'), ('the', 'right'), ('the', 'rest'), ('he', 'reached'), ('or', 'rather'), ('.', 'randal'), ('don', 'quixote'), ('was', 'quite'), ('.', 'put'), ('mode', 'put'), ('and', 'put'), ('the', 'princess'), ('the', 'present'), ('the', 'power'), ('the', 'plain'), ('the', 'place'), ('.', 'perhaps'), ('the', 'people'), ('the', 'part'), ('his', 'own'), ('her', 'own'), ('my', 'own'), ('their', 'own'), ('the', 'oven'), ('and', 'out'), ('find', 'out'), ('it', 'out'), ('to', 'our'), ('of', 'our'), ('.', 'our'), ('the', 'others'), ('the', 'other'), ('each', 'other'), ('or', 'other'), ('the', 'original'), ('two', 'or'), ('one', 'or'), ('good', 'opinion'), ('.', 'only'), ('the', 'only'), ('not', 'only'), ('.', 'one'), ('and', 'one'), ('no', 'one'), ('any', 'one'), ('in', 'one'), ('the', 'one'), ('of', 'one'), ('by', 'one'), ('at', 'one'), ('with', 'one'), ('was', 'one'), ('at', 'once'), ('.', 'on'), ('and', 'on'), ('went', 'on'), ('was', 'on'), ('put', 'on'), ('out', 'on'), ('an', 'old'), ('the', 'old'), ('.', 'oh'), ('head', 'of'), ('form', 'of'), ('name', 'of'), ('layer', 'of'), ('sight', 'of'), ('kind', 'of'), ('most', 'of'), ('end', 'of'), ('sense', 'of'), ('side', 'of'), ('sort', 'of'), ('one', 'of'), ('spite', 'of'), ('word', 'of'), ('instead', 'of'), ('think', 'of'), ('two', 'of'), ('work', 'of'), ('sound', 'of'), ('part', 'of'), ('full', 'of'), ('nothing', 'of'), ('number', 'of'), ('that', 'of'), ('was', 'of'), ('means', 'of'), ('.', 'of'), ('out', 'of'), ('middle', 'of'), ('state', 'of'), ('some', 'of'), ('speak', 'of'), ('top', 'of'), ('life', 'of'), ('all', 'of'), ('son', 'of'), ('those', 'of'), ('blood', 'of'), ('best', 'of'), ('and', 'of'), ('and', 'now'), ('was', 'now'), ('.', 'now'), ('.', 'nothing'), ('was', 'nothing'), ('was', 'not'), ('should', 'not'), ('.', 'not'), ('did', 'not'), ('but', 'not'), ('would', 'not'), ('can', 'not'), ('have', 'not'), ('do', 'not'), ('is', 'not'), (\"'m\", 'not'), ('could', 'not'), ('were', 'not'), ('and', 'not'), ('are', 'not'), ('had', 'not'), ('mister', 'north'), ('.', 'nor'), ('had', 'no'), ('be', 'no'), ('was', 'no'), ('.', 'no'), ('him', 'no'), ('have', 'no'), ('and', 'no'), ('that', 'no'), ('is', 'no'), ('to', 'night'), ('the', 'night'), ('that', 'night'), ('the', 'next'), ('the', 'new'), ('her', 'new'), ('had', 'never'), ('the', 'name'), ('does', \"n't\"), ('ai', \"n't\"), ('is', \"n't\"), ('ca', \"n't\"), ('wo', \"n't\"), ('do', \"n't\"), ('from', 'my'), ('and', 'my'), ('in', 'my'), ('.', 'my'), ('on', 'my'), ('with', 'my'), ('of', 'my'), ('for', 'my'), ('at', 'my'), ('to', 'my'), ('all', 'my'), ('i', 'must'), ('we', 'must'), ('as', 'much'), ('so', 'much'), ('very', 'much'), ('the', 'mountain'), ('her', 'mother'), ('the', 'mother'), ('the', 'most'), ('to', 'morrow'), ('this', 'morning'), ('the', 'morning'), ('is', 'more'), ('no', 'more'), ('once', 'more'), ('the', 'more'), ('and', 'more'), ('much', 'more'), ('a', 'moment'), ('.', 'mode'), ('.', 'mister'), ('.', 'missus'), ('.', 'miss'), ('miss', 'milner'), ('they', 'might'), ('he', 'might'), ('it', 'might'), ('the', 'middle'), ('of', 'men'), ('the', 'men'), ('to', 'meet'), ('ask', 'me'), ('with', 'me'), ('at', 'me'), ('for', 'me'), ('to', 'me'), ('let', 'me'), ('give', 'me'), ('you', 'may'), ('the', 'matter'), ('how', 'many'), ('every', 'man'), ('the', 'man'), ('a', 'man'), ('old', 'man'), ('young', 'man'), ('of', 'man'), ('to', 'make'), ('be', 'made'), ('had', 'made'), ('and', 'made'), ('i', 'love'), ('of', 'love'), ('and', 'looked'), ('.', 'look'), ('to', 'look'), ('no', 'longer'), ('so', 'long'), ('a', 'long'), ('the', 'long'), ('a', 'little'), ('the', 'little'), ('the', 'light'), ('of', 'life'), ('the', 'life'), ('his', 'life'), ('.', 'let'), ('had', 'left'), ('the', 'left'), ('to', 'leave'), ('at', 'least'), ('the', 'least'), ('the', 'latter'), ('the', 'last'), ('at', 'last'), ('a', 'large'), ('the', 'land'), ('the', 'lady'), ('the', 'ladies'), ('not', 'know'), ('you', 'know'), ('i', 'know'), (\"n't\", 'know'), ('she', 'knew'), ('he', 'knew'), ('the', 'kitchen'), ('the', 'king'), ('to', 'keep'), ('of', 'its'), ('in', 'its'), ('to', 'its'), ('and', 'it'), ('is', 'it'), ('into', 'it'), ('as', 'it'), ('if', 'it'), ('to', 'it'), ('that', 'it'), ('what', 'it'), ('of', 'it'), ('but', 'it'), ('thought', 'it'), ('.', 'it'), ('which', 'it'), ('now', 'it'), ('in', 'it'), ('about', 'it'), ('for', 'it'), ('do', 'it'), ('let', 'it'), ('when', 'it'), ('that', 'is'), ('and', 'is'), ('there', 'is'), ('who', 'is'), ('this', 'is'), ('what', 'is'), ('which', 'is'), ('it', 'is'), ('he', 'is'), ('.', 'is'), ('it', 'into'), ('divided', 'into'), ('.', 'indeed'), ('found', 'in'), ('it', 'in'), ('were', 'in'), ('them', 'in'), ('and', 'in'), ('him', 'in'), ('up', 'in'), ('.', 'in'), ('put', 'in'), ('came', 'in'), ('me', 'in'), ('are', 'in'), ('even', 'in'), ('was', 'in'), ('but', 'in'), ('long', 'in'), ('is', 'in'), ('that', 'in'), ('man', 'in'), ('you', 'in'), ('.', 'illustration'), ('and', 'if'), ('as', 'if'), ('but', 'if'), ('that', 'if'), ('.', 'if'), ('which', 'i'), ('though', 'i'), ('when', 'i'), ('me', 'i'), ('.', 'i'), ('if', 'i'), ('shall', 'i'), ('but', 'i'), ('and', 'i'), ('as', 'i'), ('that', 'i'), ('her', 'husband'), ('a', 'hundred'), ('the', 'human'), ('of', 'human'), ('.', 'how'), ('and', 'how'), ('see', 'how'), ('the', 'house'), (\"'s\", 'house'), ('an', 'hour'), ('the', 'hotel'), ('missus', 'horton'), ('i', 'hope'), ('at', 'home'), ('upon', 'his'), ('to', 'his'), ('with', 'his'), ('by', 'his'), ('and', 'his'), ('up', 'his'), ('all', 'his'), ('in', 'his'), ('on', 'his'), ('at', 'his'), ('was', 'his'), ('from', 'his'), ('of', 'his'), ('for', 'his'), ('that', 'his'), ('as', 'his'), ('over', 'his'), ('.', 'his'), ('like', 'him'), ('for', 'him'), ('to', 'him'), ('with', 'him'), ('the', 'highest'), ('.', 'here'), ('into', 'her'), ('to', 'her'), ('from', 'her'), ('.', 'her'), ('about', 'her'), ('on', 'her'), ('for', 'her'), ('and', 'her'), ('of', 'her'), ('in', 'her'), ('with', 'her'), ('the', 'heart'), ('his', 'head'), ('if', 'he'), ('him', 'he'), ('because', 'he'), ('what', 'he'), ('which', 'he'), ('but', 'he'), ('.', 'he'), ('so', 'he'), ('did', 'he'), ('that', 'he'), ('time', 'he'), ('then', 'he'), ('and', 'he'), ('when', 'he'), ('as', 'he'), ('would', 'have'), ('should', 'have'), ('not', 'have'), ('must', 'have'), ('to', 'have'), ('you', 'have'), ('they', 'have'), ('who', 'have'), ('we', 'have'), ('might', 'have'), ('.', 'have'), ('i', 'have'), ('it', 'has'), ('he', 'has'), ('her', 'hands'), ('his', 'hands'), ('the', 'hand'), ('his', 'hand'), ('her', 'hand'), ('which', 'had'), ('and', 'had'), ('that', 'had'), ('who', 'had'), ('it', 'had'), ('i', 'had'), ('she', 'had'), ('he', 'had'), ('they', 'had'), ('we', 'had'), ('the', 'guide'), ('the', 'ground'), ('the', 'green'), ('the', 'great'), ('a', 'great'), ('the', 'government'), ('he', 'got'), (\"'ve\", 'got'), ('a', 'good'), ('very', 'good'), ('the', 'good'), ('had', 'gone'), ('was', 'going'), ('of', 'god'), ('to', 'go'), ('to', 'give'), ('the', 'girl'), ('to', 'get'), ('and', 'gave'), ('he', 'gave'), ('the', 'garden'), ('.', 'from'), ('away', 'from'), ('his', 'friends'), ('his', 'friend'), ('a', 'friend'), ('i', 'found'), ('be', 'found'), ('.', 'for'), ('him', 'for'), ('out', 'for'), ('but', 'for'), ('as', 'for'), ('and', 'for'), ('on', 'foot'), ('to', 'follow'), ('the', 'flour'), ('the', 'floor'), ('the', 'first'), ('the', 'fire'), ('a', 'fine'), ('to', 'find'), ('the', 'field'), ('a', 'few'), ('she', 'felt'), ('my', 'father'), ('the', 'father'), ('her', 'father'), ('in', 'fact'), ('the', 'fact'), (\"'s\", 'face'), ('his', 'face'), ('his', 'eyes'), (\"'s\", 'eyes'), ('her', 'eyes'), ('an', 'extraordinary'), ('.', 'every'), ('in', 'every'), ('the', 'evening'), ('.', 'even'), ('and', 'even'), ('he', 'entered'), ('in', 'england'), ('the', 'end'), ('the', 'emperor'), ('the', 'east'), ('to', 'each'), ('sat', 'down'), ('no', 'doubt'), ('the', 'door'), ('be', 'done'), ('he', 'does'), ('to', 'do'), ('we', 'do'), ('i', 'do'), ('.', 'do'), ('you', 'do'), ('i', 'did'), ('he', 'did'), ('she', 'did'), ('the', 'death'), ('and', 'death'), ('the', 'dead'), ('two', 'days'), ('a', 'day'), ('the', 'day'), (\"'s\", 'daughter'), ('criss', 'cross'), ('of', 'course'), ('the', 'country'), ('we', 'could'), ('that', 'could'), ('they', 'could'), ('he', 'could'), ('i', 'could'), ('you', 'could'), ('she', 'could'), ('to', 'come'), ('had', 'come'), ('the', 'church'), ('the', 'children'), ('the', 'child'), ('his', 'chair'), ('a', 'certain'), ('to', 'carry'), ('you', 'can'), ('we', 'can'), ('i', 'can'), ('the', 'camp'), ('to', 'call'), ('i', 'ca'), ('but', 'by'), ('and', 'by'), ('.', 'by'), ('the', 'butter'), ('nothing', 'but'), ('.', 'but'), ('his', 'brother'), ('missus', 'bozzle'), ('the', 'boys'), ('the', 'boy'), ('green', 'box'), ('the', 'bottom'), ('his', 'body'), ('the', 'big'), ('the', 'best'), ('.', 'besides'), ('the', 'bell'), ('i', 'believe'), ('to', 'believe'), ('of', 'being'), ('he', 'began'), ('.', 'before'), ('have', 'been'), ('has', 'been'), ('had', 'been'), ('having', 'been'), ('not', 'been'), ('to', 'bed'), ('to', 'become'), ('a', 'beautiful'), ('not', 'be'), ('could', 'be'), ('might', 'be'), ('to', 'be'), ('should', 'be'), ('will', 'be'), ('would', 'be'), ('may', 'be'), ('shall', 'be'), ('must', 'be'), (\"n't\", 'be'), ('can', 'be'), ('the', 'back'), ('died', 'away'), ('the', 'attack'), ('looking', 'at'), ('look', 'at'), ('was', 'at'), ('.', 'at'), ('not', 'at'), ('and', 'at'), ('looked', 'at'), ('she', 'asked'), ('and', 'as'), ('me', 'as'), ('soon', 'as'), ('such', 'as'), ('but', 'as'), ('well', 'as'), ('just', 'as'), ('.', 'as'), ('much', 'as'), ('was', 'as'), ('his', 'arms'), ('we', 'are'), ('they', 'are'), ('there', 'are'), ('you', 'are'), ('of', 'any'), ('in', 'any'), ('one', 'another'), ('mary', 'ann'), ('well', 'and'), ('you', 'and'), ('her', 'and'), ('salt', 'and'), ('now', 'and'), ('again', 'and'), ('back', 'and'), ('one', 'and'), ('.', 'and'), ('on', 'and'), ('food', 'and'), ('side', 'and'), ('life', 'and'), ('him', 'and'), ('me', 'and'), ('here', 'and'), ('down', 'and'), ('up', 'and'), ('water', 'and'), ('house', 'and'), ('good', 'and'), ('it', 'and'), ('time', 'and'), ('fire', 'and'), ('them', 'and'), ('night', 'and'), ('mother', 'and'), ('head', 'and'), ('man', 'and'), ('by', 'and'), ('trees', 'and'), ('dead', 'and'), ('eyes', 'and'), ('in', 'and'), ('the', 'ancient'), ('in', 'an'), ('as', 'an'), ('of', 'an'), ('was', 'an'), ('with', 'an'), ('.', 'an'), ('and', 'an'), ('is', 'an'), ('i', 'am'), ('was', 'all'), ('of', 'all'), ('and', 'all'), ('with', 'all'), ('for', 'all'), ('.', 'all'), ('at', 'all'), ('in', 'all'), ('were', 'all'), ('the', 'air'), ('and', 'again'), ('and', 'after'), ('.', 'after'), ('he', 'added'), ('been', 'able'), ('upon', 'a'), ('got', 'a'), ('are', 'a'), ('for', 'a'), ('at', 'a'), ('on', 'a'), ('was', 'a'), ('be', 'a'), ('in', 'a'), ('many', 'a'), ('.', 'a'), ('such', 'a'), ('make', 'a'), ('than', 'a'), ('from', 'a'), ('into', 'a'), (\"'s\", 'a'), ('without', 'a'), ('but', 'a'), ('by', 'a'), ('made', 'a'), ('that', 'a'), ('after', 'a'), ('have', 'a'), ('as', 'a'), ('half', 'a'), ('is', 'a'), ('and', 'a'), ('saw', 'a'), ('of', 'a'), ('like', 'a'), ('only', 'a'), ('with', 'a'), ('me', 'a'), ('to', 'a'), ('had', 'a'), ('not', 'a'), ('been', 'a'), ('friend', '.'), ('it', '.'), ('boy', '.'), ('not', '.'), ('silence', '.'), ('head', '.'), ('up', '.'), ('now', '.'), ('away', '.'), ('eyes', '.'), ('work', '.'), ('side', '.'), ('say', '.'), ('home', '.'), ('death', '.'), ('her', '.'), ('food', '.'), ('evening', '.'), ('said', '.'), ('thing', '.'), ('this', '.'), ('other', '.'), ('again', '.'), ('man', '.'), ('hand', '.'), ('time', '.'), ('river', '.'), ('do', '.'), ('another', '.'), ('well', '.'), ('that', '.'), ('us', '.'), ('love', '.'), ('sir', '.'), ('one', '.'), ('you', '.'), ('matter', '.'), ('children', '.'), ('him', '.'), ('me', '.'), ('go', '.'), ('did', '.'), ('house', '.'), ('way', '.'), ('out', '.'), ('done', '.'), ('them', '.'), ('alone', '.'), ('life', '.'), ('day', '.'), ('years', '.'), ('so', '.'), ('face', '.'), ('nothing', '.'), ('all', '.'), ('door', '.'), ('yes', '.'), ('myself', '.'), ('himself', '.'), ('too', '.'), ('i', \"'ve\"), ('what', \"'s\"), ('there', \"'s\"), ('father', \"'s\"), ('it', \"'s\"), ('she', \"'s\"), ('he', \"'s\"), ('who', \"'s\"), ('that', \"'s\"), ('man', \"'s\"), ('emperor', \"'s\"), ('you', \"'re\"), ('i', \"'m\"), ('you', \"'ll\"), ('i', \"'ll\")]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "import re\n",
    "import codecs\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "words_all = []\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "# reading the file in unicode format using codecs library    \n",
    "\n",
    "\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "arr = []\n",
    "texts = [[[word for word in line.lower().split()[1:] if word not in stoplist]\n",
    "         #for i,line in enumerate(codecs.open(document,\"r\",\"utf-8\"))]\n",
    "         for i,line in enumerate(codecs.open(document,\"r\",\"utf-8\"))]\n",
    "         for document in text_paths]\n",
    "\n",
    "# Reshape list to be a transcript for each row\n",
    "for doc in texts:\n",
    "    for line in doc:\n",
    "        if len(line) > 0:\n",
    "            arr.append(line)\n",
    "# texts = arr\n",
    "print(len(arr))\n",
    "print(arr[-1])\n",
    "\n",
    "\n",
    "# Convert wordlist to raw corpus\n",
    "corpus_raw = u\"\"\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "for line in arr:\n",
    "    corpus_raw += u\" \".join(line)+u\". \"\n",
    "    # merged_line = u\" \".join(line)\n",
    "    # print('Merged Line:', merged_line)\n",
    "    # print()\n",
    "    # print()\n",
    "    # raw_sentences.append(nltk.tokenize.word_tokenize(merged_line))\n",
    "#     print(\"Corpus is now {0} characters long\".format(len(corpus_raw)))\n",
    "#     print()\n",
    "# print(corpus_raw)\n",
    "\n",
    "# Tokenize\n",
    "# tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "raw_sentences = nltk.tokenize.word_tokenize(corpus_raw)\n",
    "# print(len(corpus_raw))\n",
    "print(len(raw_sentences))\n",
    "print(raw_sentences[-1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# extracting the bi-grams and sorting them according to their frequencies\n",
    "finder = TrigramCollocationFinder.from_words(raw_sentences)\n",
    "finder.apply_freq_filter(5)\n",
    "trigram_model = finder.ngram_fd\n",
    "trigram_model = sorted(finder.ngram_fd, key=lambda item: item[1],reverse=True)  \n",
    "\n",
    "print(bigram_model)\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "np.save(\"lang_model.npy\",bigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'until', 'this', 'their', 'herself', 'didn', 'ain', 'can', 'yours', 'having', 'i', 've', 'or', 'ours', 'than', 'm', 'me', 'how', 'once', 'himself', 'from', 'own', 'while', 'isn', 'most', 'under', 'for', 'after', 'during', 'he', 'so', 'themselves', 'only', 'further', 'being', 'off', 'has', 'but', 'and', 's', 'shan', 'to', 'against', 'below', 'an', 'down', 'hers', 'again', 'won', 'because', 'the', 'out', 'on', 'his', 'of', 'above', 'my', 'myself', 'll', 'are', 'same', 'we', 'aren', 'then', 'that', 'if', 'very', 'did', 'at', 'through', 'no', 'mustn', 'd', 'whom', 'our', 'over', 'doing', 'now', 'its', 'haven', 'them', 'will', 'couldn', 'wasn', 't', 'who', 'into', 'a', 'there', 'don', 'where', 'was', 'few', 'here', 'as', 'when', 'o', 'hasn', 'in', 'not', 'does', 'you', 'her', 'other', 'be', 're', 'do', 'by', 'such', 'which', 'him', 'theirs', 'with', 'up', 'ourselves', 'all', 'about', 'some', 'needn', 'any', 'between', 'they', 'been', 'wouldn', 'shouldn', 'she', 'y', 'these', 'weren', 'had', 'why', 'should', 'ma', 'mightn', 'yourselves', 'yourself', 'hadn', 'what', 'those', 'were', 'just', 'doesn', 'itself', 'your', 'too', 'each', 'it', 'am', 'both', 'more', 'before', 'nor', 'is', 'have'}\n",
      "There are 8,177 sentences total\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "# reading the file in unicode format using codecs library    \n",
    "stoplist = set(stopwords.words('english'))\n",
    "print(stoplist)\n",
    "# transcripts = [[[word for word in line.lower().split() if word not in stoplist]\n",
    "#          for i,line in enumerate(codecs.open(document,\"r\",\"utf-8\"))]\n",
    "#          for document in text_paths]\n",
    "\n",
    "# # Reshape list to be a transcript for each row\n",
    "# transcript_list = []\n",
    "# for document in transcripts:\n",
    "#     for transcript in document:\n",
    "#         if len(transcript) > 0:\n",
    "#             transcript_list.append(transcript)\n",
    "# print(\"Transcript List Length: {0:,}\".format(len(transcript_list)))\n",
    "\n",
    "# Convert wordlist to raw corpus\n",
    "# corpus_raw = u\"\"\n",
    "# for line in transcript_list:\n",
    "#     new_script = u\" \".join(line)\n",
    "#     corpus_raw += new_script\n",
    "# raw_words = nltk.word_tokenize(corpus_raw)\n",
    "\n",
    "\n",
    "# Tokenize sentences    \n",
    "corpus_raw = u\"\"\n",
    "for book_filename in text_paths[:2]:\n",
    "    # print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw += book_file.read()\n",
    "#     print(\"Corpus is now {0} characters long\".format(len(corpus_raw)))\n",
    "#     print()\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "raw_sentences = tokenizer.tokenize(corpus_raw)\n",
    "# print(raw_sentences[:10])\n",
    "\n",
    "\n",
    "#sentence where each word is tokenized\n",
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "print(\"There are {0:,} sentences total\".format(len(sentences)))\n",
    "\n",
    "## Clean the tokens ##\n",
    "# Remove stop words\n",
    "sentences = [[word for word in sentence if word not in stoplist]\n",
    "            for sentence in sentences]\n",
    "\n",
    "# Remove single-character tokens (mostly punctuation)\n",
    "sentences = [[word for word in sentence if len(word) > 1]\n",
    "            for sentence in sentences]\n",
    "# Remove numbers\n",
    "sentences = [[word for word in sentence if not word.isnumeric()]\n",
    "            for sentence in sentences]\n",
    "# Lowercase all words (default_stopwords are lowercase too)\n",
    "sentences = [[word.lower() for word in sentence]\n",
    "            for sentence in sentences]\n",
    "# # Stemming words seems to make matters worse, disabled\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "sentences = [[stemmer.stem(word) for word in sentence]\n",
    "            for sentence in sentences]\n",
    "\n",
    "# print(\"Raw Words Length: {0:,}\".format(len(raw_words)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:sdc_dev]",
   "language": "python",
   "name": "conda-env-sdc_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
